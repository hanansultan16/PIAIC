{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1ZTu-bQIrjG",
        "outputId": "ba7266a0-1576-4fda-b4af-0f78ee392a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone pinecone-notebooks langchain pinecone-client google-generativeai openai tqdm chromadb langchain_community langchain-google-genai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import time\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA2ocKn9IsLh",
        "outputId": "60140430-5806-4df8-a85f-08d3006012a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Pinecone API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"langchain-test-index\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=3072,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "5PH_WCUeI9YD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-google-vertexai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4EaPdNJGko",
        "outputId": "8ffd36bf-1121-4116-f0ad-cc5461c5c091"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOkozL-eJShr",
        "outputId": "8cc88f23-d732-45ae-cdb1-df3519c66530"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/6.9 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "BIjQxi2nJXlI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "key=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "VxVocxrtJrCt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(google_api_key=key,\n",
        "                                   model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "HeqbsuOUJufI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "# Replace 'your-project-id' with your actual Google Cloud Project ID\n",
        "vertexai.init(project=\"gen-lang-client-0765229835\", location=\"us-central1\")\n",
        "\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\")"
      ],
      "metadata": {
        "id": "-rjjl6jYJI09"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "jI-mZ0KjJMUt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf  # Install if you haven't already\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/docker-deep-dive-nigel-poulton 2024.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_6VeINnKbud",
        "outputId": "f06b3b64-d652-480e-9a53-8c5fbe30c993"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h0tW8W0LyBV",
        "outputId": "064804dd-ffd4-4816-f19d-74de62a2a566"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 0}, page_content=''),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 1}, page_content='Docker Deep Dive\\nZero to Docker in a single book!\\nNigel Poulton\\nThis book is for sale athttp://leanpub.com/dockerdeepdive\\nThis version was published on 2024-05-21\\nISBN 9781916585133\\nThis is aLeanpub book. Leanpub empowers authors and publishers with the Lean\\nPublishing process.Lean Publishingis the act of publishing an in-progress ebook using\\nlightweight tools and many iterations to get reader feedback, pivot until you have the\\nright book and build traction once you do.\\n© 2016 - 2024 Nigel Poulton'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 2}, page_content='Huge thanks to my wife and kids for putting up with a geek in the house who genuinely thinks\\nhe’s a bunch of software running inside of a container on top of midrange biological hardware. It\\ncan’t be easy living with me!\\nMassive thanks as well to everyone who watches my Pluralsight videos. I love connecting with you\\nand really appreciate all the feedback I’ve gotten over the years. This was one of the major reasons\\nI decided to write this book! I hope it’ll be an amazing tool to help you drive your careers even\\nfurther forward.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 3}, page_content='Contents\\n0: About the book. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1\\nPart 1: The big picture stuff. . . . . . . . . . . . . . . . . . . . .4\\n1: Containers from 30,000 feet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\\nThe bad old days. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nHello VMware! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nVMwarts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nHello Containers!. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\nLinux containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\nHello Docker! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\nDocker and Windows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\nWhat about WebAssembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\nWhat about Kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2: Docker and container-related standards and projects. . . . . . . . . . . . . . . .10\\nDocker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\nContainer-related standards and projects. . . . . . . . . . . . . . . . . . . . . . . . 12\\n3: Getting Docker. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16\\nDocker Desktop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nInstalling Docker with Multipass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nInstalling Docker on Linux. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4: The big picture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22\\nThe Ops Perspective. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\nThe Dev Perspective. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nPart 2: The technical stuff. . . . . . . . . . . . . . . . . . . . . . .30\\n5: The Docker Engine. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .31'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 4}, page_content='CONTENTS\\nDocker Engine – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\nThe Docker Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\nThe influence of the Open Container Initiative (OCI). . . . . . . . . . . . . . . . . 34\\nrunc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\ncontainerd . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\nStarting a new container (example). . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\nWhat’s the shim all about?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\nHow it’s implemented on Linux. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n6: Working with Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40\\nDocker images – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\nIntro to images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nPulling images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\nImage registries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\nImage naming and tagging. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nImages and layers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nPulling images by digest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\nMulti-architecture images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\nVulnerability scanning with Docker Scout. . . . . . . . . . . . . . . . . . . . . . . . 61\\nDeleting Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\nImages – The commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n7: Working with containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\\nContainers – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\nContainers vs VMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\nImages and Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\nCheck Docker is running. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\nStarting a container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\nHow containers start apps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\nConnecting to a container. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\nInspecting container processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\nThe docker inspect command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\nWriting data to a container. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\nStopping, restarting, and deleting a container. . . . . . . . . . . . . . . . . . . . . . 80\\nKilling a container’s main process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\nDebugging slim images and containers with Docker Debug. . . . . . . . . . . . . 84\\nSelf-healing containers with restart policies. . . . . . . . . . . . . . . . . . . . . . . 89\\nContainers – The commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n8: Containerizing an app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94\\nContainerizing an app – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nContainerize a single-container app. . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\nMoving to production with multi-stage builds. . . . . . . . . . . . . . . . . . . . .106'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 5}, page_content='CONTENTS\\nBuildx, BuildKit, drivers, and Build Cloud. . . . . . . . . . . . . . . . . . . . . . . . 111\\nMulti-architecture builds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .113\\nA few good practices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .116\\nContainerizing an app – The commands. . . . . . . . . . . . . . . . . . . . . . . . .118\\n9: Multi-container apps with Compose. . . . . . . . . . . . . . . . . . . . . . . . . . .120\\nDocker Compose – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .120\\nCompose background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\nInstalling Compose. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\nThe sample app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .122\\nCompose files. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\\nDeploying apps with Compose – The commands. . . . . . . . . . . . . . . . . . . .133\\n10: Docker Swarm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135\\nDocker Swarm – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135\\nSwarm primer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .136\\nBuild a secure swarm cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\nDocker Swarm – The Commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . .158\\n11: Deploying apps with Docker Stacks. . . . . . . . . . . . . . . . . . . . . . . . . .160\\nDeploying apps with Docker Stacks – The TLDR. . . . . . . . . . . . . . . . . . . .160\\nBuild a Swarm lab. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\nThe sample app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .162\\nDeploy the app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168\\nManaging the app . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\nDeploying apps with Docker Stacks – The Commands. . . . . . . . . . . . . . . .176\\n12: Docker and WebAssembly. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .177\\nPre-reqs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .178\\nIntro to Wasm and Wasm containers. . . . . . . . . . . . . . . . . . . . . . . . . . .180\\nWrite a Wasm app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nContainerize a Wasm app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183\\nRun a Wasm container. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\\nClean up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .186\\nChapter summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .186\\n13: Docker Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .188\\nDocker Networking – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . .189\\nDocker networking theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .189\\nSingle-host bridge networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .193\\nExternal access via port mappings. . . . . . . . . . . . . . . . . . . . . . . . . . . . .200\\nDocker Networking – The Commands. . . . . . . . . . . . . . . . . . . . . . . . . . 214\\n14: Docker overlay networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .216'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 6}, page_content='CONTENTS\\nDocker overlay networking – The TLDR. . . . . . . . . . . . . . . . . . . . . . . .216\\nDocker overlay networking history. . . . . . . . . . . . . . . . . . . . . . . . . . . .216\\nBuilding and testing Docker overlay networks. . . . . . . . . . . . . . . . . . . . . 217\\nOverlay networks explained. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\\nDocker overlay networking – The commands. . . . . . . . . . . . . . . . . . . . . .229\\n15: Volumes and persistent data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .231\\nVolumes and persistent data – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . 231\\nContainers without volumes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232\\nContainers with volumes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .233\\nVolumes and persistent data – The Commands. . . . . . . . . . . . . . . . . . . . .240\\n16: Docker security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .242\\nDocker security – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .242\\nKernel Namespaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .243\\nControl Groups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .246\\nCapabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .246\\nMandatory Access Control systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\nseccomp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\nDocker security technologies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .248\\nSwarm security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .248\\nDocker Scout and vulnerability scanning. . . . . . . . . . . . . . . . . . . . . . . .255\\nSigning and verifying images with Docker Content Trust. . . . . . . . . . . . . . .258\\nDocker Secrets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\\nWhat next . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .264\\nTerminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .266\\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .273'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 7}, page_content='0: About the book\\nThis is a book about Docker and containers; no prior knowledge required! In fact, the\\nbook’s motto isZero to Docker in a single book.\\nSo, if you want to work with cloud and cloud-native technologies, this book is dedicated\\nto you.\\nWhy should I read this book or care about Docker?\\nDocker is here, and it’s changed the world. If you want the best jobs working with\\nthe best technologies, you need to know Docker and containers. They’re even central\\nto Kubernetes, and a strong Docker skill set will help you learn Kubernetes. Docker\\nand containers are also well-positioned for emerging cloud technologies such as\\nWebAssembly and AI workloads.\\nWhat if I’m not a developer\\nMost applications, even modern cloud-native microservices, need high-performance\\nproduction-grade infrastructure. If you think traditional developers will take care of\\nthis, think again. To cut a long story short, if you want to thrive in the modern cloud-\\nfirst world, you must know Docker. But don’t stress, this book will give you all the skills\\nyou need.\\nHow I’ve organized the book\\nI’ve divided the book into two main sections:\\n1. The big picture stuff\\n2. The technical stuff\\nThe big picture stuffgets you up to speed with things like what Docker is, why we\\nhave containers, and the fundamental jargon such ascloud-native, microservices,and\\norchestration.\\nThe technical stuffsection covers everything you need to know aboutimages, containers,\\nmulti-container microservices apps,and the increasingly important topic oforchestration.\\nIt even covers WebAssembly, vulnerability scanning with Docker Scout, debugging\\ncontainers, high availability, and more.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 8}, page_content='0: About the book 2\\nChapter breakdown\\n• Chapter 1:Summarises the history and potential future of Docker and containers\\n• Chapter 2:Explains the most important container-related standards and projects\\n• Chapter 3:Shows you a few ways to get Docker\\n• Chapter 4:Walks you through a very simple hands-on container workflow\\n• Chapter 5:Explains the architecture of the Docker Engine\\n• Chapter 6:Dives deep into images and image management\\n• Chapter 7:Dives deep into containers and container management\\n• Chapter 8:Walks you through the process of containerizing an app\\n• Chapter 9:Shows you how to build, deploy, and manage multi-container apps\\nwith Compose\\n• Chapter 10:Walks you through building a secure swarm\\n• Chapter 11:Deploys and manages a multi-container app on a secure swarm\\n• Chapter 12:Walks you through building and containerizing a WebAssembly app\\n• Chapter 13:Dives into Docker networking\\n• Chapter 14:Builds and tests Docker overlay networks\\n• Chapter 15:Introduces you to persistent and non-persistent data in Docker\\n• Chapter 16:Covers all the major Linux and Docker security technologies\\nEditions and updates\\nDocker and the cloud-native ecosystem are evolving fast, and a 2-3-year-old book on\\nDocker isn’t valuable. As a result, I’m committed to updating the book every year.\\nIf that sounds excessive, welcome to the new normal.\\nThe book is available in hardback, paperback, and e-book on all good book publishing\\nplatforms.\\nWhen you purchase the Kindle edition, you’re entitled to all future updates. However,\\nKindle doesn’t always download the latest edition.\\nA potential solution is to go tohttp://amzn.to/2l53jdg and chooseQuick Solutions.\\nThen selectDigital Purchases, search for your Docker Deep Dive Kindle edition\\npurchase, and selectContent and Devices. Your purchase should appear in the list with\\na button that saysUpdate Available. Click that button. Delete your old version on your\\nKindle and download the new one.\\nIf this doesn’t work, your only option is to contact Kindle Support.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 9}, page_content='0: About the book 3\\nFeedback\\nIf you like the book and it helps your career, share the love by recommending it to a\\nfriend and leaving a review on Amazon or Goodreads.\\nIf you spot a typo or want to make a recommendation, email me atddd@nigelpoulton.com\\nThat’s everything. Let’s get rocking with Docker!'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 10}, page_content='Part 1: The big picture stuff'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 11}, page_content='1: Containers from 30,000 feet\\nContainers have taken over the world!\\nIn this chapter, you’ll learn why we have containers, what they do for us, and where we\\ncan use them.\\nThe bad old days\\nApplications are the powerhouse of every modern business. When applications break,\\nbusinesses break.\\nMost applications run on servers, and in the past, we were limited to running one\\napplication per server. As a result, the story went something like this:\\nEvery time a business needed a new application, it had to buy a new server. Unfor-\\ntunately, we weren’t very good at modeling the performance requirements of new\\napplications, and the IT departments had to guess. This often resulted in businesses\\nbuying very expensive servers with a lot more performance capability than the apps\\nneeded. After all, nobody wanted underpowered servers incapable of handling the app,\\nresulting in unhappy customers and lost revenue. As a result, companies ended up with\\nracks and racks of overpowered servers operating as low as 5-10% of their potential\\ncapacity. This was a tragic waste of company capital and environmental resources!\\nHello VMware!\\nAmid all this, VMware, Inc. gave the world a gift — thevirtual machine (VM).\\nAs soon as VMware came along, the world became much better. We finally had a\\ntechnology that allowed us to safely run multiple business applications on a single\\nserver.\\nIt was a game-changer. Businesses could run new apps on the spare capacity of existing\\nservers, spawning a golden age of maximizing the value of existing assets.\\nVMwarts\\nBut, and there’s always abut! As great as VMs are, they’re far from perfect.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 12}, page_content='1: Containers from 30,000 feet 6\\nA feature of the VM model is every VM needing its own dedicated operating system\\n(OS). Unfortunately, this has several drawbacks, including:\\n• Every OS consumes CPU, RAM, and other resources we’d rather use on applica-\\ntions\\n• Every VM and OS needs patching\\n• Every VM and OS needs monitoring\\nVMs are also slow to boot and not very portable.\\nHello Containers!\\nWhile most of us were reaping the benefits of VMs, web scalers like Google had already\\nmoved on from VMs and were using containers.\\nA feature of the container model is that every container shares the OS of the host it’s\\nrunning on. This means a single host can run more containers than VMs. For example,\\na host that can run 10 VMs might be able to run 50 containers, making containers far\\nmore efficient than VMs.\\nContainers are also faster and more portable than VMs.\\nLinux containers\\nModern containers started in the Linux world and are the product of incredible work\\nfrom many people over many years. For example, Google contributed many container-\\nrelated technologies to the Linux kernel. It’s thanks to many contributions like these\\nthat we have containers today.\\nSome of the major technologies behind modern containers include;kernel namespaces,\\ncontrol groups (cgroups), capabilities,and more.\\nHowever, despite all this great work, containers were incredibly complicated, and it\\nwasn’t until Docker came along that they became accessible to the masses.\\nNote: I know that many container-like technologies pre-date Docker and\\nmodern containers. However, none of them changed the world the way\\nDocker has.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 13}, page_content='1: Containers from 30,000 feet 7\\nHello Docker!\\nDocker was the magic that made Linux containers easy and brought them to the masses.\\nWe’ll talk a lot more about Docker in the next chapter.\\nDocker and Windows\\nMicrosoft worked hard to bring Docker and container technologies to the Windows\\nplatform.\\nAt the time of writing, Windows desktop and server platforms support both of the\\nfollowing:\\n• Windows containers\\n• Linux containers\\nWindows containersrun Windows apps and require a host system with a Windows kernel.\\nWindows 10, Windows 11, and all modern versions of Windows Server natively support\\nWindows containers.\\nWindows systems can also run Linux containers via theWSL 2 (Windows Subsystem for\\nLinux) subsystem.\\nThis means Windows 10 and Windows 11 are great platforms for developing and testing\\nWindows and Linux containers.\\nHowever, despite all the work developingWindows containers, almost all containers are\\nLinux containers. This is because Linux containers are smaller and faster, and more\\ntooling exists for Linux.\\nAll of the examples in this edition of the book are Linux containers.\\nWindows containers vs Linux containers\\nIt’s vital to understand that containers share the kernel of the host they’re running on.\\nThis means containerized Windows apps need a host with a Windows kernel, whereas\\ncontainerized Linux apps need a host with a Linux kernel. However, as mentioned, you\\ncan run Linux containers on Windows systems that have the WSL 2 backend installed.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 14}, page_content='1: Containers from 30,000 feet 8\\nWhat about Mac containers?\\nThere is no such thing as Mac containers. However, Macs are great platforms for\\nworking with containers, and I do all of my daily work with containers on a Mac.\\nThe most popular way of working with containers on a Mac isDocker Desktop. It works\\nby running Docker inside a lightweight Linux VM on your Mac. Other tools, such as\\nPodman and Rancher Desktop, are also great for working with containers on a Mac.\\nWhat about WebAssembly\\nWebAssembly (Wasm) is a modern binary instruction set that builds applications that are\\nsmaller, faster, more secure, and more portable than containers. You write your app in\\nyour favorite language and compile it as a Wasm binary, and it’ll run anywhere you have\\na Wasm runtime.\\nHowever, Wasm apps have many limitations, and we’re still developing many of\\nthe standards. As a result, containers remain the dominant model for cloud-native\\napplications.\\nThe container ecosystem is also much richer and more mature than the Wasm ecosys-\\ntem.\\nAs you’ll see in the Wasm chapter, Docker and the container ecosystem are adapting\\nto work with Wasm apps, and you should expect a future where VMs, containers, and\\nWasm apps run side-by-side in most clouds and applications.\\nThis book is up-to-date with the latest Wasm and container developments.\\nWhat about Kubernetes\\nKubernetes is the industry standard platform for deploying and managing containerized\\napps.\\nTerminology: A containerized appis an application running as a container.\\nWe’ll cover this in a lot of detail later.\\nOlder versions of Kubernetes used Docker to start and stop containers. However, newer\\nversions usecontainerd, which is a stripped-down version of Docker optimized for use\\nby Kubernetes and other platforms.\\nThe important thing to know is that all Docker containers work on Kubernetes.\\nCheck out these resources if you need to learn Kubernetes:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 15}, page_content='1: Containers from 30,000 feet 9\\n• Quick Start Kubernetes:This is\\x18100 pages and will get you up-to-speed with\\nKubernetes inone day!\\n• The Kubernetes Book. This is the ultimate book for mastering Kubernetes.\\nI update both books annually to ensure they’re up-to-date with the latest and greatest\\ndevelopments in the cloud native ecosystem, including WebAssembly.\\nChapter Summary\\nWe used to live in a world where every time the business needed a new application,\\nwe had to buy a brand-new server. VMware came along and allowed us to drive more\\nvalue out of new and existing servers. However, following the success of VMware and\\nhypervisors came a newer, more efficient, and portable virtualization technology called\\ncontainers. However, containers were complex and hard to implement until Docker came\\nalong and made them easy. WebAssembly is powering a third wave of cloud computing,\\nbut Docker and the container ecosystem are evolving to work with WebAssembly, and\\nthe book has an entire chapter dedicated to Docker and WebAssembly.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 16}, page_content='2: Docker and container-related\\nstandards and projects\\nThis chapter introduces you to Docker and some of the most important standards and\\nprojects shaping the container ecosystem. The goal is to lay some foundations that we’ll\\nbuild on in later chapters.\\nThis chapter has two main parts:\\n• Docker\\n• Container-related standards and projects\\nDocker\\nDocker is at the heart of the container ecosystem. However, the termDocker can mean\\ntwo things:\\n1. The Docker platform\\n2. Docker, Inc.\\nThe Docker platformis a neatly packaged collection of technologies for creating, manag-\\ning, and orchestrating containers.Docker, Inc.is the company that created the Docker\\nplatform and continues to be the driving force behind developing new features.\\nLet’s dive a bit deeper.\\nDocker, Inc.\\nDocker, Inc. is a technology company based out of Palo Alto and founded by French-\\nborn American developer and entrepreneur Solomon Hykes. Solomon is no longer at\\nthe company.\\nThe company started as aplatform as a service (PaaS)provider calleddotCloud. Behind the\\nscenes, dotCloud delivered their services on top of containers and had an in-house to\\nhelp them deploy and manage those containers. They called this in-house toolDocker.\\nThe wordDocker is a British expression meaningdock work____er____ that refers to a\\nperson who loads and unloads cargo from ships.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 17}, page_content='2: Docker and container-related standards and projects 11\\nIn 2013, dotCloud dropped the struggling PaaS side of the business, rebranded as\\nDocker, Inc., and focussed on bringing Docker and containers to the world.\\nThe Docker technology\\nThe Docker platform is designed to make it as easy as possible tobuild, ship,and run\\ncontainers.\\nAt a high level, there are two major parts to the Docker platform:\\n• The CLI (client)\\n• The engine (server)\\nThe CLI is the familiardocker command-line tool for deploying and managing contain-\\ners. It converts simple commands into API requests and sends them to the engine.\\nThe engine comprises all the server-side components that run and manage containers.\\nFigure 2.1 shows the high-level architecture. The client and engine can be on the same\\nhost or connected over the network.\\nFigure 2.1 Docker client and engine.\\nIn later chapters, you’ll see that the client and engine are complex and comprise a lot of\\nsmall specialized parts. Figure 2.2 gives you an idea of some of the complexity behind\\nthe engine. However, the client hides all this complexity so you don’t have to care. For\\nexample, you type friendlydocker commands into the CLI, the CLI converts them to\\nAPI requests and sends them to the daemon, and the daemon takes care of everything\\nelse.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 18}, page_content='2: Docker and container-related standards and projects 12\\nFigure 2.2 Docker CLI and daemon hiding complexity.\\nLet’s switch focus and briefly look at some standards and governance bodies.\\nContainer-related standards and projects\\nThere are several important standards and governance bodies influencing the develop-\\nment of containers and the container ecosystem. Some of these include:\\n• The OCI\\n• The CNCF\\n• The Moby Project\\nThe Open Container Initiative (OCI)\\nThe Open Container Initiative (OCI)1 is a governance council responsible for low-level\\ncontainer-related standards.\\nIt operates under the umbrella of theLinux Foundation2 and was founded in the early\\ndays of the container ecosystem when some of the people at a company called CoreOS\\ndidn’t like the way Docker was dominating the ecosystem. In response, CoreOS created\\nan open standard calledappc3 that defined specifications for things such as image\\nformat and container runtime. They also created a reference implementation calledrkt\\n(pronounced “rocket”).\\nThe appc standard did things differently from Docker and put the ecosystem in an\\nawkward position with two competingstandards.\\nWhile competition is usually a good thing,competing standardsare generally bad, as they\\ngenerate confusion that slows down user adoption. Fortunately, the main players in the\\n1https://www.opencontainers.org\\n2https://www.linuxfoundation.org/projects\\n3https://github.com/appc/spec/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 19}, page_content='2: Docker and container-related standards and projects 13\\necosystem came together and formed the OCI as a vendor-neutral lightweight council\\nto govern container standards. This allowed us to archive the appc project and place all\\nlow-level container-related specifications under the OCI’s governance.\\nAt the time of writing, the OCI maintains three standards calledspecs:\\n• The image-spec4\\n• The runtime-spec5\\n• The distribution-spec6\\nWe often use arail tracksanalogy when explaining the OCI standards:\\nWhen the size and properties of rail tracks were standardized, it gave entrepreneurs\\nin the rail industry confidence the trains, carriages, signaling systems, platforms, and\\nother rail infrastructure they built would work with the standardized tracks — nobody\\nwanted competing standards for track sizes.\\nThe OCI specifications did the same thing for the container ecosystem and it’s flour-\\nished ever since. Docker has also changed a lot since the formation of the OCI, and all\\nmodern versions of Docker implement all three OCI specs. For example:\\n• The Docker builder (BuildKit) createsOCI compliant-images\\n• Docker uses anOCI-compliant runtimeto createOCI-compliant containers\\n• Docker Hub implements the OCI distribution spec and is anOCI-compliant registry\\nDocker, Inc. and many other companies have people on the technical oversight board\\n(TOB) of the OCI.\\nThe CloudNative Computing Foundation (CNCF)\\nThe Cloud Native Computing Foundation (CNCF)7 is another Linux Foundation\\nproject that is influential in the container ecosystem. It was founded in 2015 with the\\ngoal of“…advancing container technologies… and making cloud native computing ubiquitous”.\\nInstead of creating and maintaining container-related specifications, the CNCFhosts\\nimportant projects such as Kubernetes, containerd, Notary, Prometheus, Cilium, and\\nlots more.\\nWhen we say the CNCFhosts these projects, we mean it provides a space, structure, and\\nsupport for projects to grow and mature. For example, all CNCF projects pass through\\nthe following three phases or stages:\\n4https://github.com/opencontainers/image-spec\\n5https://github.com/opencontainers/runtime-spec\\n6https://github.com/opencontainers/distribution-spec\\n7https://www.cncf.io/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 20}, page_content='2: Docker and container-related standards and projects 14\\n• Sandbox\\n• Incubating\\n• Graduated\\nEach phase increases a project’s maturity level by requiring higher standards of gov-\\nernance, documentation, auditing, contribution tracking, marketing, community\\nengagement, and more. For example, new projects accepted as sandbox projects may\\nhave great ideas and great technology but need help and resources to create strong\\ngovernance, etc. The CNCF helps with all of that.\\nGraduated projects are consideredready for productionand are guaranteed to have strong\\ngovernance and implement good practices.\\nIf you look back to Figure 2.2, you’ll see that Docker uses at least two CNCF technolo-\\ngies — containerd and Notary.\\nThe Moby Project\\nDocker created theMoby projectas a community-led place for developers to build\\nspecialized tools for building container platforms.\\nPlatform builders can pick the specific Moby tools they need to build their container\\nplatform. They can even compose their platforms from a mix of Moby tools, in-house\\ntools, and tools from other projects.\\nDocker, Inc. originally created the Moby project, but it now has members including\\nMicrosoft, Mirantis, and Nvidia.\\nThe Docker platform is built using tools from various projects, including the Moby\\nproject, the CNCF, and the OCI.\\nChapter summary\\nThis chapter introduced you to Docker and some of the major influences in the\\ncontainer ecosystem.\\nDocker, Inc., is a technology company based in Palo Alto that is changing how we do\\nsoftware. They were thefirst moversand instigators of the modern container revolution.\\nThe Docker platform focuses on running and managing application containers. It runs\\non Linux and Windows, can be installed almost anywhere, and offers a variety of free\\nand paid-for products.\\nThe Open Container Initiative (OCI) governs low-level container standards and\\nmaintains specifications for runtimes, image format, and registries.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 21}, page_content='2: Docker and container-related standards and projects 15\\nThe CNCF hosts important cloud-native projects and helps them mature into produc-\\ntion-grade tools.\\nThe Moby project hosts low-level tools developers can use to build container platforms.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 22}, page_content='3: Getting Docker\\nThere are lots of ways to get Docker and work with containers. This chapter will show\\nyou the following ways:\\n• Docker Desktop\\n• Multipass\\n• Server installs on Linux\\nDocker Desktop is the best way to work with Docker and gets you the complete Docker\\nexperience on your laptop with all the latest tools, plugins, and extensions. I use it every\\nday, and I recommend it to everyone.\\nWe’ll also show you how to install Docker on your laptop with Multipass and a super-\\nsimple Linux installation. However, you should only consider these if you can’t use\\nDocker Desktop, as they offer fewer features.\\nDocker Desktop\\nDocker Desktop is a desktop app from Docker, Inc. and is the best way to work with\\ncontainers. You get the Docker Engine, a slick UI, all the latest plugins and features, and\\nan extension system with a marketplace. You even get Docker Compose and a single-\\nnode Kubernetes cluster if you want to learn Kubernetes.\\nIt’s free for personal use and education, but you’ll have to pay a license fee if you use it\\nfor work and your company has over 250 employees or does more than $10M in annual\\nrevenue.\\nDocker Desktop on Windows 10 and Windows 11 Professional and Enterprise editions\\nsupports Windows containersand Linux containers.Docker Desktop on Mac, Linux, and\\nHome editions of Windows only supportLinux containers.All of the examples in the\\nbook and almost all of the containers in the real world are Linux containers.\\nLet’s install Docker Desktop on Windows and MacOS.\\nWindows prereqs\\nDocker Desktop on Windows requires all of the following:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 23}, page_content='3: Getting Docker 17\\n• 64-bit version of Windows 10/11\\n• Hardware virtualization support must be enabled in your system’s BIOS\\n• WSL 2\\nBe very careful changing anything in your system’s BIOS.\\nInstalling Docker Desktop on Windows 10 and 11\\nSearch the internet for _“install Docker Desktop on Windows”. This will take you\\nto the relevant download page, where you can download the installer and follow\\nthe instructions. When prompted, you should install and enable the WSL 2 backend\\n(Windows Subsystem for Linux).\\nOnce the installation is complete, you need to manually start Docker Desktop from the\\nWindows Start menu. It may take a minute to start, but you can watch the start progress\\nvia the animated whale icon on the Windows taskbar at the bottom of the screen.\\nOnce it’s running, you can open a terminal and type some simpledocker commands.\\n$ docker version\\n<Snip>\\nServer: Docker Desktop 4.30.0 (149282)\\nEngine:\\nVersion: 26.1.1\\nAPI version: 1.45 (minimum version 1.24)\\nGo version: go1.21.9\\nBuilt: Tue Apr 30 11:48:28 2024\\nOS/Arch: linux/amd64\\n<Snip>\\nNotice how theServer output showsOS/Arch: linux/amd64. This is because a default\\ninstallation assumes you’ll be working with Linux containers.\\nSome versions of Windows let you switch toWindows containersby right-clicking the\\nDocker whale icon in the Windows notifications tray and selectingSwitch to Windows\\ncontainers…. Doing this keeps existing Linux containers running in the background,\\nbut you won’t be able to see or manage them until you switch back to Linux containers\\nmode.\\nCongratulations. You now have a working installation of Docker on your Windows\\nmachine.\\nMake sure you’re running inLinux containers modeso you can follow along with the\\nexamples later in the book.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 24}, page_content='3: Getting Docker 18\\nInstalling Docker Desktop on Mac\\nDocker Desktop for Mac is like Docker Desktop for Windows — a packaged product\\nwith a slick UI that gets you the full Docker experience on your laptop. You can also\\nenable the built-in single-node Kubernetes cluster.\\nBefore proceeding with the installation, you need to know that Docker Desktop on\\nMac installs the daemon and server-side components inside a lightweight Linux VM\\nthat seamlessly exposes the API to your local Mac environment. This means you can\\nopen a terminal on your Mac and rundocker commands without ever knowing it’s all\\nrunning in a hidden VM. This is also why Mac versions of Docker Desktop only work\\nwith Linux containers — everything’s running inside a Linux VM.\\nFigure 3.1 shows the high-level architecture for Docker Desktop on Mac.\\nFigure 3.1\\nThe simplest way to install Docker Desktop on your Mac is to search the web for“install\\nDocker Desktop on MacOS”, follow the links to the download, and then complete the\\nsimple installer.\\nWhen the installer finishes, you’ll have to start Docker Desktop from the MacOS\\nLaunchpad manually. It may take a minute to start, but you can watch the animated\\nDocker whale icon in the status bar at the top of your screen. Once it’s started, you can\\nclick the whale icon to manage Docker Desktop.\\nOpen a terminal window and run some regular Docker commands. Try the following.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 25}, page_content='3: Getting Docker 19\\n$ docker version\\nClient:\\nCloud integration: v1.0.35+desktop.13\\nVersion: 26.1.1\\nAPI version: 1.45\\nOS/Arch: darwin/arm64\\n<Snip>\\nServer: Docker Desktop 4.30.0 (149282)\\nEngine:\\nVersion: 26.1.1\\nAPI version: 1.45 (minimum version 1.24)\\nOS/Arch: linux/arm64\\ncontainerd:\\nVersion: 1.6.31\\nrunc:\\nVersion: 1.1.12\\ndocker-init:\\nVersion: 0.19.0\\n<Snip>\\nNotice that theOS/Arch: for theServer component shows aslinux/amd64 or lin-\\nux/arm64. This is because the daemon runs inside the Linux VM mentioned earlier. The\\nClient component is a native Mac application and runs directly on the Mac OS Darwin\\nkernel. This is why it shows asdarwin/amd64 or darwin/arm64.\\nYou can now use Docker on your Mac.\\nInstalling Docker with Multipass\\nOnly consider this section if you can’t use Docker Desktop, as Multipass installations\\ndon’t ship with out of the box support fordocker scout, docker debug, ordocker\\ninit.\\nMultipass is a free tool for creating cloud-style Linux VMs on your Linux, Mac, or\\nWindows machine and is incredibly easy to install and use. It’s an easy way to create\\nmulti-node production-like Docker clusters.\\nGo tohttps://multipass.run/install and install the right edition for your hardware\\nand OS.\\nOnce installed, you only need three commands:\\n$ multipass launch\\n$ multipass ls\\n$ multipass shell'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 26}, page_content='3: Getting Docker 20\\nLet’s see how to launch and connect to a new VM with Docker pre-installed.\\nRun the following command to create a new VM callednode1 based on thedocker\\nimage. The docker image has Docker pre-installed and ready to go.\\n$ multipass launch docker --name node1\\nIt’ll take a minute or two to download the image and launch the VM.\\nList VMs to make sure yours launched properly.\\n$ multipass ls\\nName State IPv4 Image\\nnode1 Running 192.168.64.37 Ubuntu 22.04 LTS\\n172.17.0.1\\n172.18.0.1\\nYou’ll use the192 IP address when working with the examples later in the book.\\nConnect to the VM with the following command.\\n$ multipass shell node1\\nOnce connected, you can run the following commands to check your Docker version\\nand list installed CLI plugins.\\n$ docker --version\\nDocker version 26.1.0, build 9714adc\\n$ docker info\\nClient: Docker Engine - Community\\nVersion: 26.1.0\\nContext: default\\nDebug Mode: false\\nPlugins:\\nbuildx: Docker Buildx (Docker Inc.)\\nVersion: v0.14.0\\nPath: /usr/libexec/docker/cli-plugins/docker-buildx\\ncompose: Docker Compose (Docker Inc.)\\nVersion: v2.26.1\\nPath: /usr/libexec/docker/cli-plugins/docker-compose\\n<Snip>\\nThe installation in the example only has thebuildx and compose CLI plugins. You’ll\\nhave to manually install the relevant plugins if you want to follow the Docker Scout,\\nDocker Init, and Docker Debug examples later in the book.\\nYou can typeexit to log out of the VM, andmultipass shell node1 to log back in.\\nYou can also typemultipass delete node1 and thenmultipass purge to delete it.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 27}, page_content='3: Getting Docker 21\\nInstalling Docker on Linux\\nOnly consider this section if you can’t use Docker Desktop. This installation doesn’t\\ninstall thedocker scout, docker debug, ordocker init CLI plugins that we’ll use in\\nsome of the later chapters.\\nThese instructions show you how to install Docker on Ubuntu Linux 22.04 and are just\\nfor guidance purposes. Lots of other installation methods exist, and you should search\\nthe web for the latest instructions.\\n$ sudo snap install docker\\ndocker 24.0.5 from Canonical ✓ installed\\nRun some commands to test the installation. You’ll have to prefix them withsudo.\\n$ sudo docker --version\\nDocker version 24.0.5, build ced0996\\n$ sudo docker info\\n<Snip>\\nServer:\\nContainers: 0\\nRunning: 0\\nPaused: 0\\nStopped: 0\\nImages: 0\\nServer Version: 24.0.5\\n<Snip>\\nChapter Summary\\nYou can run Docker almost anywhere, and installing it’s easier than ever.\\nDocker Desktop gives you a fully functional Docker environment on your Linux, Mac,\\nor Windows machine and is the best way to get a Docker development environment\\non your local machine. It’s easy to install, includes the Docker Engine, has a slick UI,\\nand has a marketplace with lots of extensions to extend its capabilities. It works with\\ndocker scout, docker debug, anddocker init, and it even lets you spin up a single-\\nnode Kubernetes cluster.\\nMultipass is a great way to spin up a local VM running Docker, and there are lots of\\nways to install Docker on Linux servers. These give you access to most of the free\\nDocker features but lack some of the features of Docker Desktop.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 28}, page_content='4: The big picture\\nThis chapter will give you some hands-on experience and a high-level view of images\\nand containers. The goal is to prepare you for more detail in the upcoming chapters.\\nWe’ll break this chapter into two parts:\\n• The Ops perspective\\n• The Dev perspective\\nThe ops perspective focuses on starting, stopping, deleting containers, and executing\\ncommands inside them.\\nThe dev perspective focuses more on the application side of things and runs through\\ntaking application source code, building it into a container image, and running it as a\\ncontainer.\\nI recommend you read both sections and follow the examples, as this will give you the\\ndev and ops perspectives. DevOps anyone?\\nThe Ops Perspective\\nIn this section, you’ll complete all of the following:\\n• Check Docker is working\\n• Download an image\\n• Start a container from the image\\n• Execute a command inside the container\\n• Delete the container\\nA typical Docker installation installs the client and the engine on the same machine and\\nconfigures them to talk to each other.\\nRun adocker version command to ensure both are installed and running.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 29}, page_content='4: The big picture 23\\n$ docker version\\nClient: <<---- Start of client response\\nCloud integration: v1.0.35+desktop.13 -----┐\\nVersion: 26.1.1 |\\nAPI version: 1.45 | Client info block\\nGo version: go1.21.9 |\\nOS/Arch: darwin/arm64 |\\nContext: desktop-linux -----┘\\nServer: Docker Desktop 4.30.0 (149282) <<---- Start of server response\\nEngine: -----┐\\nVersion: 26.1.1 |\\nAPI version: 1.45 (minimum version 1.24) |\\nGo version: go1.21.9 |\\nOS/Arch: linux/arm64 |\\ncontainerd: | Server block\\nVersion: 1.6.31 |\\nrunc: |\\nVersion: 1.1.12 |\\ndocker-init: |\\nVersion: 0.19.0 -----┘\\nIf your response from the clientand server looks like the output in the book, everything\\nis working as expected.\\nIf you’re on Linux and get apermission denied while trying to connect to the\\nDocker daemon... error, try again withsudo in front of the command —sudo docker\\nversion. If it works withsudo, you’ll need to prefixall future docker commands with\\nsudo.\\nDownload an image\\nImages are objects that contain everything an app needs to run. This includes an OS\\nfilesystem, the application, and all dependencies. If you work in operations, they’re\\nsimilar to VM templates. If you’re a developer, they’re similar toclasses.\\nRun adocker images command.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nIf you are working from a clean installation, you’ll have no images, and your output\\nwill be the same as the book. If you’re working with Multipass, you might see an image\\ncalled protainer/protainer-ce.\\nCopying new images onto your Docker host is calledpulling. Pull theubuntu:latest\\nimage.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 30}, page_content='4: The big picture 24\\n$ docker pull ubuntu:latest\\nlatest: Pulling from library/ubuntu\\nb91d8878f844: Download complete\\nDigest: sha256:e9569c25505f33ff72e88b2990887c9dcf230f23259da296eb814fc2b41af999\\nStatus: Downloaded newer image for ubuntu:latest\\ndocker.io/library/ubuntu:latest\\nRun anotherdocker images to confirm yourpull command worked.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nubuntu latest e9569c25505f 10 days ago 106MB\\nWe’ll discuss the details of where the image is stored and what’s inside it in later\\nchapters. For now, all you need to know is that images contain enough of an operating\\nsystem (OS) and all the code and dependencies required to run the application. The\\nUbuntu image you pulled includes a stripped-down version of the Ubuntu Linux\\nfilesystem and a few of the standard Linux utilities that ship with Ubuntu.\\nIf you pull anapplication container,such asnginx:latest, you’ll get an image with a\\nminimal OSand the code to run the NGINX app.\\nStart a container from the image\\nIf you’ve been following along, you’ll have a copy of theubuntu:latest image on your\\nDocker host, and you can use thedocker run command to start a container from it.\\nRun the followingdocker run command to start a new container calledtest from the\\nubuntu:latest image.\\n$ docker run --name test -it ubuntu:latest bash\\nroot@bbd2e5ad1817:/#\\nNotice how your shell prompt has changed. This is because the container is already\\nrunning and your shell is attached to it.\\nLet’s quickly examine thatdocker run command.\\ndocker run tells Docker to start a new container. The--name flag told Docker to call\\nthis containertest. Next, the-it flags told Docker to make the container interactive and\\nto attach your shell to the container’s terminal. After that, the command told Docker to\\nbase the container on theubuntu:latest image. Finally, it told Docker to start a Bash\\nshell as the container’s main app.\\nRun aps command from inside the container to list all running processes.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 31}, page_content='4: The big picture 25\\nroot@bbd2e5ad1817:/# ps -elf\\nF S UID PID PPID NI ADDR SZ WCHAN STIME TTY TIME CMD\\n4 S root 1 0 0 - 4560 - 13:38 pts/0 00:00:00 /bin/bash\\n0 R root 9 1 0 - 8606 - 13:38 pts/0 00:00:00 ps -elf\\nThere are only two processes:\\n• PID 1 is the Bash process that we told the container to run\\n• PID 9 is theps -elf command we ran to list the running processes\\nThe presence of theps -elf process in the output can be a bit misleading as it is a short-\\nlived process that exits as soon as theps command completes. This means the only long-\\nrunning process inside the container is the/bin/bash process.\\nPress Ctrl-PQ to exit the container without terminating it. This will land your shell back\\nat your local terminal, and your shell prompt will revert.\\nRun the following command to verify yourtest container is still running.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\nbbd2e5ad1817 ubuntu:latest \"/bin/bash\" 7 mins Up 7 min test\\nThe output shows yourtest container, and you can see it was created 7 minutes ago and\\nhas been running for 7 minutes.\\nExecute a command inside the container\\nYou can use thedocker attach command to attach your shell to the container’s main\\nprocess.\\nRun the following command to attach your shell to the Bash process inside the con-\\ntainer.\\n$ docker attach test\\nroot@bbd2e5ad1817:/#\\nMake sure your shell prompt changes, indicating you successfully connected to the\\ncontainer.\\nExit the container again by typingCtrl-PQ and verify your shell prompt reverts to your\\nlocal machine.\\nDelete the container\\nRun anotherdocker ps to verify your container is still running.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 32}, page_content='4: The big picture 26\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\nbbd2e5ad1817 ubuntu:latest \"/bin/bash\" 9 mins Up 9 min test\\nStop and kill the container using thedocker stop and docker rm commands.\\n$ docker stop test\\ntest\\nIt can take a few seconds for the container to stop.\\n$ docker rm test\\ntest\\nVerify the container was successfully deleted by running thedocker ps command with\\nthe -a flag to list all containers, even those in the stopped state.\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\nCongratulations, you’ve pulled a Docker image, started a container from it, logged in to\\nit, executed a command inside it, stopped it, and deleted it.\\nThe Dev Perspective\\nContainers are all about applications.\\nYou’ll complete all of the following steps in this section:\\n• Clone an app from a GitHub repo\\n• Inspect the app’sDockerfile\\n• Containerize the app\\n• Run the app as a container\\nRun the following command to make a local clone of the repo. This will copy the\\napplication code to your machine so you can containerize it in a future step. You’ll need\\nthe git CLI for this to work.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 33}, page_content='4: The big picture 27\\n$ git clone https://github.com/nigelpoulton/psweb.git\\nCloning into \\'psweb\\'...\\nremote: Enumerating objects: 63, done.\\nremote: Counting objects: 100% (34/34), done.\\nremote: Compressing objects: 100% (22/22), done.\\nremote: Total 63 (delta 13), reused 25 (delta 9), pack-reused 29\\nReceiving objects: 100% (63/63), 13.29 KiB | 4.43 MiB/s, done.\\nResolving deltas: 100% (21/21), done.\\nChange into thepsweb directory and list its contents.\\n$ cd psweb\\n$ ls -l\\ntotal 32\\n-rw-r--r--@ 1 nigelpoulton staff 324 5 Feb 12:31 Dockerfile\\n-rw-r--r-- 1 nigelpoulton staff 378 5 Feb 12:31 README.md\\n-rw-r--r-- 1 nigelpoulton staff 341 5 Feb 12:31 app.js\\n-rw-r--r--@ 1 nigelpoulton staff 355 5 Feb 12:47 package.json\\ndrwxr-xr-x 3 nigelpoulton staff 96 5 Feb 12:31 views\\nThe app is a simple Node.js web app running some static HTML.\\nInspect the app’s Dockerfile\\nThe Dockerfile is a plain-text document that tells Docker how to build the app and\\ndependencies into an image.\\nList the contents of the application’s Dockerfile.\\n$ cat Dockerfile\\nFROM alpine\\nLABEL maintainer=\"nigelpoulton@hotmail.com\"\\nRUN apk add --update nodejs npm curl\\nCOPY . /src\\nWORKDIR /src\\nRUN npm install\\nEXPOSE 8080\\nENTRYPOINT [\"node\", \"./app.js\"]\\nYou’ll learn more about Dockerfiles later in the book. Right now, all you need to know is\\nthat each line represents an instruction Docker executes to build the app into an image.\\nIf you’ve been following along, you’ve pulled some application code from a remote Git\\nrepo and looked at the application’s Dockerfile.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 34}, page_content='4: The big picture 28\\nContainerize the app\\nRun the followingdocker build command to create a new image based on the instruc-\\ntions in the Dockerfile. It will create a new Docker image calledtest:latest.\\nBe sure to run the command from within thepsweb directory.\\n$ docker build -t test:latest .\\n[+] Building 36.2s (11/11) FINISHED\\n=> [internal] load .dockerignore 0.0s\\n=> => transferring context: 2B 0.0s\\n=> [internal] load build definition from Dockerfile 0.0s\\n<Snip>\\n=> => naming to docker.io/library/test:latest 0.0s\\n=> => unpacking to docker.io/library/test:latest 0.7s\\nOnce the build is complete, check that you have an image calledtest:latest.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\ntest latest 0435f2738cf6 21 seconds ago 160MB\\nCongratulations, you’vecontainerized the app. That’s jargon for building it into a\\ncontainer image that contains the app and dependencies.\\nRun the app as a container\\nRun the following command to start a container calledweb1 from the image. If you’re\\non a Windows machine, you’ll need to replace the backslashes with backticks or run the\\ncommand on a single line without the backslashes.\\n$ docker run -d \\\\\\n--name web1 \\\\\\n--publish 8080:8080 \\\\\\ntest:latest\\nOpen a web browser and navigate to the DNS name or IP address of your Docker host\\non port8080. If you’re following along on Docker Desktop, connect tolocalhost:8080\\nor 127.0.0.1:8080. If you’re following along on Multipass, connect to your Multipass\\nVM’s 192 address on port8080. Run anip a | grep 192 command from within the\\nMultipass VM, or run amultipass ls from your local machine to find the address.\\nYou will see the following web page.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 35}, page_content='4: The big picture 29\\nFigure 4.1\\nCongratulations. You’ve copied some application code from a remote Git repo, built it\\ninto a Docker image, and run it as a container. We call thiscontainerizing an app.\\nClean up\\nRun the following commands to terminate the container and delete the image.\\n$ docker rm web1 -f\\nweb1\\n$ docker rmi test:latest\\nUntagged: test:latest\\nDeleted: sha256:0435f27...cac8e2b\\nChapter Summary\\nIn the Ops section of the chapter, you downloaded a Docker image, launched a container\\nfrom it, logged into the container, executed a command inside of it, and then stopped\\nand deleted the container.\\nIn the Dev section, you containerized a simple application by pulling source code from\\nGitHub and building it into an image using instructions in a Dockerfile. You then ran\\nthe app as a container.\\nThe things you’ve learned in this chapter will help you in the upcoming chapters.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 36}, page_content='Part 2: The technical stuff'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 37}, page_content='5: The Docker Engine\\nIn this chapter, we’ll look under the hood of the Docker Engine.\\nThis chapter has a strongoperations focus, and you can use Docker without knowing\\neverything you’re about to learn. However, to truly master something, you need to\\nunderstand what’s going on under the hood. So, if you want tomaster Docker, you\\nshould read this chapter.\\nI’ve divided the chapter into the following sections:\\n• Docker Engine – The TLDR\\n• The Docker Engine\\n• The influence of the Open Container Initiative (OCI)\\n• runc\\n• containerd\\n• Starting a new container (example)\\n• What’s the shim all about\\n• How it’s implemented on Linux\\nLet’s learn about the Docker Engine.\\nDocker Engine – The TLDR\\nDocker Engineis jargon for the server-side components of Docker that run and manage\\ncontainers. If you’ve ever worked with VMware, the Docker Engine is similar to ESXi.\\nThe Docker Engine is modular and built from many small specialized components\\npulled from projects such as the OCI, the CNCF, and the Moby project.\\nIn many ways, the Docker Engine is like a car engine:\\n• A car engine is made from many specialized parts that work together to make a car\\ndrive — intake manifolds, throttle bodies, cylinders, pistons, spark plugs, exhaust\\nmanifolds, and more.\\n• The Docker Engine is made from many specialized tools that work together to\\ncreate and run containers — the API, image builder, high-level runtime, low-level\\nruntime, shims, etc.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 38}, page_content='5: The Docker Engine 32\\nFigure 5.1 shows the components of the Docker Engine that create and run containers.\\nOther components exist, but this simplified diagram focuses on the components that\\nstart and run containers.\\nFigure 5.1\\nThroughout the book, we’ll refer torunc and containerd with lowercase “r” and “c”, which\\nis how they’re both written in the official project documentation. This means sentences\\nstarting with eitherrunc or containerd will not begin with a capital letter.\\nThe Docker Engine\\nWhen Docker was first released, the Docker Engine had two major components:\\n• The Docker daemon (sometimes referred to as just “the daemon”)\\n• LXC\\nThe daemon was a monolithic binary containing all the code for the API, image builders,\\ncontainer execution, volumes, networking, and more.\\nLXC did the hard work of interfacing with the Linux kernel and constructing the\\nrequired namespaces and cgroups to build and start containers.\\nReplacing LXC\\nRelying on LXC posed several problems for the Docker project.\\nFirst, LXC is Linux-specific, and Docker had aspirations of being multi-platform.\\nSecond, Docker was evolving fast, and there was no way of ensuring LXC evolved in the\\nways Docker needed.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 39}, page_content='5: The Docker Engine 33\\nTo improve the experience and help the project evolve more quickly, Docker replaced\\nLXC with its own tool,libcontainer. The goal of libcontainer was to be a platform-\\nagnostic tool that gave Docker access to the fundamental container building blocks in\\nthe host kernel.\\nLibcontainer replaced LXC in Docker a very long time ago.\\nBreaking up the monolithic Docker daemon\\nAs previously mentioned, the Docker Engine was originally a monolith with almost all\\nfunctionality coded into thedaemon. However, as time passed, this became more and\\nmore problematic for the following reasons:\\n1. It got slower\\n2. It wasn’t what the ecosystem wanted\\n3. It’s hard to innovate on monolithic software\\nThe project recognized these challenges and began a long-running program to break\\napart and refactor the Engine so that every feature became its own small specialized tool.\\nPlatform builders could then re-use these tools to build other platforms.\\nThis work of breaking apart the Docker daemon is an ongoing process, and all of the\\ncode for building images and executing containers has been removed and refactored\\ninto small, specialized tools. Notable examples include removing the high-level and\\nlow-level runtime functionality and re-implementing them in separate tools called\\ncontainerd and runc, both of which are used by many different projects including\\nDocker, Kubernetes, Firecracker, and Fargate. More recently (starting with Docker\\nDesktop 4.27.0), Docker has removed image management from the daemon and now\\nuses containerd’s image management capabilities.\\nFigure 5.2 shows another view of the main components of the Docker Engine that are\\nused to run containers and lists the primary responsibilities of each component.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 40}, page_content='5: The Docker Engine 34\\nFigure 5.2 - Engine components and responsibilities\\nOther engine components exist.\\nThe influence of the Open Container Initiative (OCI)\\nAround the same time that Docker, Inc. was refactoring the Engine, theOCI8 was in the\\nprocess of defining two container-related standards:\\n1. Image Specification(image-spec)9\\n2. Runtime Specification(runtime-spec)10\\nBoth specifications were released as version 1.0 in July 2017 and are still vital today.\\nThey’ve even added a third specification called the Distribution Specification (distribu-\\ntion-spec) governing how images are distributed via registries.\\nAt the time of writing, the runtime-spec is at version 1.1.0, the image-spec is at version\\n1.0.2, and the distribution-spec is at 1.0.1. However, plans are underway for version 1.1\\nof both the image-spec and distribution-spec. We mention this to highlight the slow-\\nand-steady nature of these low-level specifications that are heavily relied upon by so\\nmany other projects — stability is the name of the game for low-level OCI specs.\\n8https://www.opencontainers.org/\\n9https://github.com/opencontainers/image-spec\\n10https://github.com/opencontainers/runtime-spec'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 41}, page_content='5: The Docker Engine 35\\nDocker, Inc. was a founding member of the OCI and was heavily involved in defining\\nthe specifications. It continues to be involved by contributing code and helping guide\\nthe specifications.\\nAll versions of Docker since 2016 have implemented the OCI specifications. For\\nexample, Docker usesrunc, the reference implementation of the OCI runtime-spec, to\\ncreate OCI-compliant containers (runtime-spec). It also uses BuildKit to build OCI-\\ncompliant images (image-spec), and Docker Hub is an OCI-compliant registry (registry-\\nspec).\\nrunc\\nAs previously mentioned,runc11 (pronounced “run see” and always written with a\\nlowercase “r”) is the reference implementation of the OCI runtime-spec. Docker, Inc.\\nwas heavily involved in defining the spec and contributed the initial code for runc.\\nrunc is a lightweight CLI wrapper for libcontainer that you can download and use\\nto manage OCI-compliant containers. However, it’s a very low-level tool and lacks\\nalmost all of the features and add-ons you get with the Docker Engine. Fortunately, as\\npreviously shown in Figure 5.2, Docker uses runc as its low-level runtime. This means\\nyou get OCI-compliant containersand the feature-rich Docker user experience.\\nOn the jargon front, we sometimes say that runc operates at theOCI layer, and we often\\nrefer to it as alow-level runtime.\\nDocker and Kubernetes both use runc as their default low-level runtime, and both pair it\\nwith the containerd high-level runtime:\\n• containerd operates as the high-level runtimemanaging lifecycle events\\n• runc operates as the low-level runtimeexecuting lifecycle events by interfacing\\nwith the kernel to do the work of actually building containers and deleting them\\nYou can see the latest releases here:\\n• https://github.com/opencontainers/runc/releases\\ncontainerd\\ncontainerd (pronounced “container dee” and always written with a lowercase “c”) is\\nanother tool that Docker created while stripping functionality out of the daemon.\\n11https://github.com/opencontainers/runc'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 42}, page_content='5: The Docker Engine 36\\nWe refer to containerd as ahigh-level runtimeas itmanages lifecycle events such as\\nstarting, stopping, and deleting containers. However, it needs a low-level runtime to\\nperform the actual work. Most of the time, containerd is paired with runc as its low-\\nlevel runtime. However, as you saw in Figure 5.3, it usesshims that make it possible to\\nreplace runc with other low-level runtimes. We’ll go into more detail in the WebAssem-\\nbly chapter when you’ll see how to use Docker to run WebAssembly apps.\\nThe original plan was for containerd to be a small specialized tool for managing\\ncontainer lifecycle events. However, it has since grown to include the ability to manage\\nimages, networks, and volumes.\\nOne reason for adding more functionality is for projects such as Kubernetes that want\\ncontainerd to be able to push and pull images. Fortunately, this extra functionality is\\nmodular, meaning projects like Kubernetes can include containerd but only take the\\npieces they need.\\ncontainerd was originally developed by Docker, Inc. and donated to the Cloud Native\\nComputing Foundation (CNCF). At the time of writing, containerd is a graduated\\nCNCF project, meaning it’s stable and production-ready. You can see the latest releases\\nhere:\\n• https://github.com/containerd/containerd/releases\\nStarting a new container (example)\\nNow that you have a view of the big picture, let’s see how to use Docker to create a new\\ncontainer.\\nThe most common way of starting containers is using the Docker CLI. Feel free to run\\nthe following command to start a new container calledctr1 based on thenginx image.\\n$ docker run -d --name ctr1 nginx\\nRun adocker ps command to see if the container is running.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\n9cfb0c9aacb2 nginx \"/docker-entrypoint.…\" 9 seconds ago Up 9 seconds 80/tcp ctr1\\nWhen you run commands like this, the Docker client converts them into API requests\\nand sends them to the API exposed by the daemon.\\nThe daemon can expose the API on a local socket or over the network. On Linux, the\\nlocal socket is/var/run/docker.sock and on Windows it’s\\\\pipe\\\\docker_engine.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 43}, page_content='5: The Docker Engine 37\\nThe daemon receives the request, interprets it as a request to create a new container,\\nand passes it to containerd. Remember that the daemon no longer contains any code\\nto create containers.\\nThe daemon communicates with containerd via a CRUD-style API overgRPC12.\\nDespite its name, even containerd cannot create containers. It converts the required\\nDocker image into anOCI bundleand tellsrunc to use this to create a new container.\\nrunc interfaces with the OS kernel to pull together all the constructs necessary to create\\na container (namespaces, cgroups, etc.). The container is started as a child process of\\nrunc, and as soon as the container starts, runc exits.\\nFigure 5.3 summarizes the process.\\nFigure 5.3\\nDecoupling the container creation and management from the Docker daemon and\\nimplementing it in containerd and runc makes it possible to stop, restart, and even\\nupdate the daemon without impacting running containers. We sometimes call this\\ndaemonless containers.\\n12https://grpc.io/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 44}, page_content='5: The Docker Engine 38\\nIf you started the NGINX container earlier, you should delete it using the following\\ncommand.\\n$ docker rm ctr1 -f\\nWhat’s the shim all about?\\nSome of the diagrams in the chapter have shown ashim component.\\nShims are a popular software engineering pattern, and the Docker Engine uses them in\\nbetween containerd and the OCI layer, bringing the following benefits:\\n• Daemonless containers\\n• Improves efficiency\\n• Makes the OCI layer pluggable\\nWe’ve already mentioned daemonless containers.\\nOn the efficiency front, containerd forks a shim and a runc process for every new\\ncontainer. However, each runc process exits as soon as the container starts running,\\nleaving the shim process as the container’s parent process. The shim is lightweight\\nand sits between containerd and the container. It reports on the container’s status and\\nperforms low-level tasks such as keeping the container’s STDIN and STDOUT streams\\nopen.\\nShims also make it possible to replace runc with other low-level runtimes.\\nHow it’s implemented on Linux\\nOn a Linux system, Docker implements the components we’ve discussed as the follow-\\ning separate binaries:\\n• /usr/bin/dockerd (the Docker daemon)\\n• /usr/bin/containerd\\n• /usr/bin/containerd-shim-runc-v2\\n• /usr/bin/runc\\nYou can see all of these on a Linux-based Docker host by running aps command. Some\\nof the processes will only be present when the system has running containers, and you\\ncan’t see them if you’re using Docker Desktop on a Mac because the Docker Engine is\\nrunning inside a VM.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 45}, page_content='5: The Docker Engine 39\\nDo we still need the daemon\\nAt the time of writing, Docker has stripped most of the functionality out of the daemon,\\nleaving the daemon to focus on serving the API.\\nChapter summary\\nThe Docker Engine is a platform that makes it easy to build, ship, and run containers. It\\nimplements the OCI standards and is a modular app comprising many small, specialized\\ncomponents.\\nThe Docker daemonimplements the Docker API, but most other functionality has been\\nstripped out and implemented as standalone composable tools such as containerd and\\nrunc.\\ncontainerd performs image management tasks and oversees container lifecycle manage-\\nment, such as starting, stopping, and deleting containers. Docker, Inc. originally wrote it\\nand then contributed to the CNCF. It’s classed as a high-level runtime and used by many\\nother projects, including Kubernetes, Firecracker, and Fargate.\\ncontainerd relies on a low-level runtime calledrunc to interface with the host kernel and\\nbuild containers. runc is the reference implementation of the OCI runtime-spec and\\nexpects to start containers from OCI-compliant bundles. containerd talks to runc and\\nensures Docker images are presented to runc as OCI-compliant bundles.\\nrunc is based on code from libcontainer, you can run it as a standalone CLI tool to\\ncreate containers, and it’s used almost everywhere that containerd is used.\\nShims make it possible to use containerd with other low-level runtimes.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 46}, page_content='6: Working with Images\\nIn this chapter, we’ll dive deep into Docker images. You’ll learn what images are, how to\\nwork with them, and how they work under the hood. You’ll learn how to build your own\\nin Chapter 8: Containerizing an application.\\nI’ve arranged the chapter as follows:\\n• Docker images – The TLDR\\n• Intro to images\\n• Pulling images\\n• Image registries\\n• Image naming and tagging\\n• Images and layers\\n• Pulling images by digest\\n• Multi-architecture images\\n• Vulnerability scanning with Docker Scout\\n• Deleting images\\nDocker images – The TLDR\\nBefore getting started, all of the following terms mean the same thing, and we’ll use\\nthem interchangeably:Image, Docker image, container image, andOCI image.\\nAn image is a read-only package containing everything you need to run an application.\\nThis means they include application code, dependencies, a minimal set of OS constructs,\\nand metadata. You can start multiple containers from a single image.\\nIf you’re familiar with VMware, images are a bit like VM templates — a VM template is\\nlike a stopped VM, whereas an image is like a stopped container. If you’re a developer,\\nimages are similar toclasses — you can create one or more objects from a class, whereas\\nyou can create one or more containers from an image.\\nThe easiest way to get an image is topull one from aregistry. Docker Hub13 is the most\\ncommon registry, andpulling an image downloads it to your local machine where\\n13https://hub.docker.com'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 47}, page_content='6: Working with Images 41\\nDocker can use it to start one or more containers. Other registries exist, and Docker\\nworks with them all.\\nImages are made by stacking independentlayers and representing them as a single\\nunified object. One layer might have the OS components, another layer might have\\napplication dependencies, and another layer might have the application. Docker stacks\\nthese layers and makes them look like a unified system.\\nImages are usually small. For example, the official NGINX image is around 60MB, and\\nthe official Redis image is around 40MB. However, Windows images can be huge.\\nThat’s the elevator pitch. Let’s dig a little deeper.\\nIntro to images\\nWe’ve already said that images are like stopped containers. You can even stop a con-\\ntainer and create a new image from it. With this in mind, images arebuild-time con-\\nstructs, whereas containers arerun-time constructs. Figure 6.1 shows thebuild and run\\nnature of each and that you can start multiple containers from a single image.\\nFigure 6.1\\nThe docker run command is the most common way to start a container from an image.\\nOnce the container is running, the image and the container are bound, and you cannot\\ndelete the image until you stop and delete the container. If multiple containers use the\\nsame image, you can only delete the image after you’ve deleted all the containers using\\nit.\\nContainers are designed to run a single application or microservice. As such, they\\nshould only contain application code and dependencies. You should not include non-\\nessentials such as build tools or troubleshooting tools.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 48}, page_content='6: Working with Images 42\\nFor example, the official Alpine Linux image is currently about 3MB. This is because it\\ndoesn’t ship with six different shells, three different package managers, and a bunch of\\ntools you “might” need once every ten years. In fact, it’s increasingly common for images\\nto ship without a shell or a package manager — if the application doesn’t need it at run-\\ntime, the image doesn’t include it. We call theseslim images.\\nAnother thing that keeps images small is the lack of an OS kernel. This is because con-\\ntainers use the kernel of the host they’re running on. The only OS-related components\\nin most images are filesystem objects, and you’ll sometimes hear people say images\\ncontain just enough OS.\\nUnfortunately, Windows images can be huge. For example, some Windows-based\\nimages can be gigabytes in size and take a long time to push and pull.\\nPulling images\\nA clean Docker installation has an emptylocal repository.\\nLocal repositoryis jargon for an area on your local machine where Docker stores images\\nfor more convenient access. We sometimes call it theimage cache, and on Linux it’s\\nusually located in/var/lib/docker/<storage-driver>. However, it will be inside the\\nDocker VM if you’re using Docker Desktop.\\nRun the following command to inspect the contents of your local repository. This\\nexample has three images relating to three Docker Desktop extensions I’m running.\\nYours will be different and may be empty.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\ndocker/disk-usage-extension 0.2.9 f4c95478a537 26 hours ago 3.64MB\\ndocker/logs-explorer-extension 0.2.6 417dd9a8f96d 26 hours ago 17.9MB\\nportainer/portainer-docker-extension 2.19.4 908d04d20e86 2 months ago 364MB\\nThe process of getting images is calledpulling.\\nRun the following commands to pull theredis image and verify it exists in your local\\nrepository.\\nNote: If you are following along on Linux and haven’t added your user\\naccount to the localdocker Unix group, you may need to addsudo to the\\nbeginning of all the following commands.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 49}, page_content=\"6: Working with Images 43\\n$ docker pull redis\\nUsing default tag: latest <<---- Assume the 'latest' tag\\nlatest: Pulling from library/redis <<---- Assume you want to pull from Docker Hub\\n08df40659127: Download complete <<---- Pulling layer\\n4f4fb700ef54: Already exists <<---- Pulling layer (local copy must exist)\\n4fe7fa4aab04: Download complete <<---- Pulling layer\\n57dea0f129a5: Download complete <<---- Pulling layer\\nf546e941f15b: Download complete <<---- Pulling layer\\nf7f7da262cdb: Download complete <<---- Pulling layer\\nf45ab649e450: Download complete <<---- Pulling layer\\n983f900bbc88: Download complete <<---- Pulling layer\\nDigest: sha256:76d5908f5e19fcdd73daf956a38826f790336ee4707d9028f32b24ad9ac72c08\\nStatus: Downloaded newer image for redis:latest\\ndocker.io/library/redis:latest <<---- docker.io = Docker Hub\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nredis latest 11c3e418c296 2 weeks ago 223MB\\n<Snip>\\nThe image now exists in your local repository. However, I’ve annotated a few interesting\\nlines from thedocker pull output. We’ll cover them in more detail later in the chapter\\nbut they’re worth a quick mention now.\\nDocker is opinionated and made two assumptions when pulling the image:\\n1. It assumed you wanted to pull the image tagged aslatest\\n2. It assumed you wanted to pull the image from Docker Hub\\nYou can override both, but Docker will use these as defaults if you don’t override them.\\nThe Redis image has eight layers. However, Docker only pulled seven layers because it\\nalready had a local copy of one of them. This is because my system runs thePortainer\\nDocker Desktop extension, which is based on an image that shares a common layer\\nwith the Redis image. You’ll learn about this very soon, but images can share layers, and\\nDocker is clever enough only to pull the layers it doesn’t already have.\\nWe’ll talk more about these points later in the chapter.\\nImage registries\\nWe store images in centralized places calledregistries. The job of a registry is to securely\\nstore images and make them easy to access from different environments.\\nFigure 6.2 shows the central nature of registries in the build > ship > run pipeline.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 50}, page_content='6: Working with Images 44\\nFigure 6.2\\nMost modern registries implement the OCI distribution-spec, and we sometimes call\\nthem OCI registries. Most registries also implement the Docker Registry v2 API, meaning\\nyou can use the Docker CLI and other API tools to query them and work with them in\\nstandard ways. Some offer advanced features such as image scanning and integration\\nwith build pipelines.\\nThe most common registry is Docker Hub, but others exist, including 3rd-party\\ninternet-based registries and secure on-premises registries. However, as previously\\nmentioned, Docker is opinionated and will default to Docker Hub unless you tell it\\nthe name of a different registry. We’ll use Docker Hub for the rest of the book, but the\\nprinciples apply to other registries.\\nImage registries contain one or moreimage repositories, and image repositories contain\\none or more images. Figure 6.3 shows an image registry with three repositories, each\\nwith one or more images.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 51}, page_content='6: Working with Images 45\\nFigure 6.3 - Registry architecture\\nOfficial repositories\\nDocker Hub has the concept ofofficial repositoriesthat are home to images vetted and\\ncurated by Docker and the application vendor. This means theyshould contain up-to-\\ndate high-quality code that is secure, well-documented, and follows good practices.\\nMost of the popular applications and operating systems haveofficial repositorieson\\nDocker Hub, and they’re easy to identify because they live at the top level of the Docker\\nHub namespace and have a greenDocker Official Imagebadge. The following list shows\\na few official repositories and their URLs that exist at the top level of the Docker Hub\\nnamespace:\\n• nginx: https://hub.docker.com/_/nginx/\\n• busybox: https://hub.docker.com/_/busybox/\\n• redis: https://hub.docker.com/_/redis/\\n• mongo: https://hub.docker.com/_/mongo/\\nFigure 6.4 shows the official Alpine and NGINX repositories on Docker Hub. Both have\\nthe greenDocker Official Imagebadge and have over a billion pulls each. Also, notice how\\nboth are available for a wide range of CPU architectures.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 52}, page_content='6: Working with Images 46\\nFigure 6.4 - Official repos on Docker Hub\\nUnofficial repositories\\nThe following list shows two of my personal repositories in the wild west ofunofficial\\nrepositories that you should not trust.\\n• nigelpoulton/gsd — https://hub.docker.com/r/nigelpoulton/gsd/\\n• nigelpoulton/k8sbook — https://hub.docker.com/r/nigelpoulton/k8sbook/\\nNotice how they exist below thenigelpoulton second-level namespace. This is one of\\nseveral indications they are not official repositories.\\nWhile there are lots of great images in unofficial repositories, you should always start\\nwith the assumption that anything from an unofficial repository isunsafe. This is based\\non the good practice of never trusting software from the internet. In fact, you should\\nalso exercise caution when downloading and using Docker Official Images.\\nImage naming and tagging\\nMost of the time, you’ll work with images based on their names, and you can learn\\na lot about an image from its name. Figure 6.5 shows a fully qualified image name,\\nincluding the registry name, user/organization name, repository name, and tag. Docker\\nautomatically populates the registry and tag values if you don’t specify them.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 53}, page_content=\"6: Working with Images 47\\nFigure 6.5 - Fully qualified image name\\nAddressing images from official repositories is easy. All you need to supply is the\\nrepository name and image name separated by a colon. Sometimes we call the image\\nname thetag. The format for adocker pull command pulling an image from an official\\nrepository is:\\n$ docker pull <repository>:<tag>\\nThe example from earlier pulled the Redis image with the following command. It pulled\\nthe image tagged aslatest from the top-levelredis repository.\\n$ docker pull redis:latest\\nThe following examples show how to pull a few different official images.\\n$ docker pull mongo:7.0.5\\n//Pulls the image tagged as '7.0.5' from the official 'mongo' repository.\\n$ docker pull busybox:glibc\\n//Pulls the image tagged as 'glibc' from the official 'busybox' repository.\\n$ docker pull alpine\\n//Pulls the image tagged as 'latest' from the official 'alpine' repository.\\nA couple of things are worth noting.\\n• As previously mentioned, if youdon’t specify an image tag after the repository\\nname, Docker assumes you want the image tagged aslatest. The command will\\nfail if the repository has no image tagged aslatest.\\n• Images tagged aslatest are not guaranteed to be the most up-to-date in the\\nrepository.\\nPulling images fromunofficial repositoriesis almost the same as pulling from official\\nrepositories — you just need to add a Docker Hub username or organization name\\nbefore the repository name. The following example shows how to pull thev2 image\\nfrom thetu-demo repository owned by a not-to-be-trusted person whose Docker Hub\\nID isnigelpoulton.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 54}, page_content='6: Working with Images 48\\n$ docker pull nigelpoulton/tu-demo:v2\\nTo pull an image from a different registry, you just add the registry’s DNS name before\\nthe repository name. For example, the following command pulls thelatest image from\\nBrandon Mitchell’sregclient/regsync repo on GitHub Container Registry (ghcr.io).\\n$ docker pull ghcr.io/regclient/regsync:latest\\nlatest: Pulling from regclient/regsync\\n6f14f2b64ccf: Download complete\\n7746d6728537: Download complete\\n685af2c79c31: Download complete\\n4c377311167a: Download complete\\n662e9541e042: Download complete\\nDigest: sha256:149a95d47d6beed2a1404d7c3b00dddfa583a94836587ba8e3b4fe59853c1ece\\nStatus: Downloaded newer image for ghcr.io/regclient/regsync:latest\\nghcr.io/regclient/regsync:latest\\nNotice how the pull looks the same as it did with Docker Hub. This is because GHCR\\nsupports the OCI registry-spec and implements the Docker Registry v2 API.\\nImages with multiple tags\\nYou can give a single image as many tags as you want.\\nAt first glance, the following output might look like it’s listing three images. However,\\non closer inspection it’s just two — theb4210d0aa52f image is tagged aslatest and v1.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/tu-demo latest b4210d0aa52f 2 days ago 115MB\\nnigelpoulton/tu-demo v1 b4210d0aa52f 2 days ago 115MB\\nnigelpoulton/tu-demo v2 6ba12825d092 12 minutes ago 115MB\\nThis is a great example of thelatest tag not relating to the newest image in the repo.\\nIn this example, thelatest tag refers to the same image as thev1 tag, which is actually\\nolder than thev2 image.\\nImages and layers\\nAs already mentioned, images are a collection of loosely connected read-only layers\\nwhere each layer comprises one or more files.\\nFigure 6.6 shows an image with four layers. Docker takes care of stacking them and\\nrepresenting them as a single unified image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 55}, page_content='6: Working with Images 49\\nFigure 6.6 - Image and stacked layers\\nYou’re about to look at all of the following ways to inspect layer information:\\n• Pull operations\\n• The docker inspect command\\n• The docker history command\\nRun the following command to pull thenode:latest image and observe it pulling the\\nindividual layers. Some newer versions may have more or less layers, but the principle is\\nthe same.\\n$ docker pull node:latest\\nlatest: Pulling from library/ubuntu\\n952132ac251a: Pull complete\\n82659f8f1b76: Pull complete\\nc19118ca682d: Pull complete\\n8296858250fe: Pull complete\\n24e0251a0e2c: Pull complete\\nDigest: sha256:f4691c96e6bbaa99d...28ae95a60369c506dd6e6f6ab\\nStatus: Downloaded newer image for node:latest\\ndocker.io/node:latest\\nEach line ending withPull completerepresents a layer that Docker pulled. This image has\\nfive layers and is shown in Figure 6.7 with layer IDs.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 56}, page_content='6: Working with Images 50\\nFigure 6.7 - Image layers and IDs\\nAnother way to see image layers is to inspect the image with thedocker inspect\\ncommand. The following example inspects the samenode:latest image pulled in the\\nprevious step.\\n$ docker inspect node:latest\\n[\\n{\\n\"Id\": \"sha256:bd3d4369ae.......fa2645f5699037d7d8c6b415a10\",\\n\"RepoTags\": [\\n\"node:latest\"\\n<Snip>\\n\"RootFS\": {\\n\"Type\": \"layers\",\\n\"Layers\": [\\n\"sha256:c8a75145fc...894129005e461a43875a094b93412\",\\n\"sha256:c6f2b330b6...7214ed6aac305dd03f70b95cdc610\",\\n\"sha256:055757a193...3a9565d78962c7f368d5ac5984998\",\\n\"sha256:4837348061...12695f548406ea77feb5074e195e3\",\\n\"sha256:0cad5e07ba...4bae4cfc66b376265e16c32a0aae9\"\\n]\\n}\\n}\\n]\\nThe trimmed output shows the five layers. However, it shows their SHA256 hashes,\\nwhich are different from the short IDs shown in thedocker pull output.\\nThe docker inspect command is great for getting detailed image information.\\nYou can also use thedocker history command to inspect an image and see its layer\\ndata. However, this command shows the build history of an image and isnot a strict list'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 57}, page_content='6: Working with Images 51\\nof layers in the final image. For example, some Dockerfile instructions (ENV, EXPOSE, CMD,\\nand ENTRYPOINT) only add metadata and don’t create layers.\\nBase layers\\nAll Docker images start with abase layer, and every time you add new content, Docker\\nadds a new layer.\\nConsider the following oversimplified example of building a simple Python application.\\nYour corporate policy mandates all applications be built on top of the official Ubuntu\\n24:04 image. This means the official Ubuntu 24:04 image will be the base layer for this\\napp. Installing Python will add a second layer, and your application source code will add\\na third. The final image will have three layers, as shown in Figure 6.8. Remember, this is\\nan oversimplified example for demonstration purposes.\\nFigure 6.8\\nIt’s important to understand that animage is the combination of all layers stacked in the\\norder they were built. Figure 6.9 shows an image with two layers. Each layer has three\\nfiles, meaning the image has six files.\\nIt also shows that thelayers are stored as independent objects, and theimage is just\\nmetadata identifying the required layers and explaining how to stack them.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 58}, page_content='6: Working with Images 52\\nFigure 6.9\\nIn the slightly more complex example of the three-layer image in Figure 6.10, the overall\\nimage only presents six files in the unified view. This is becauseFile 7in the top layer\\nis an updated version ofFile 5directly below (inline). In this situation, the file in the\\nhigher layer obscures the file directly below it. This means you can update the file in an\\nimage by adding new layers.\\nFigure 6.10 - Stacking layers\\nUnder the hood, Docker usesstorage driversto stack layers and present them as a\\nunified filesystem and image. Almost all Docker setups use theoverlay2 driver, butzfs,\\nbtrfs, andvfs are alternative options. However, whichever storage driver you use, the\\ndeveloper and user experience are always the same.\\nFigure 6.11 shows how the three-layer image from Figure 6.10 will appear on the system\\n— all three layers stacked and merged into a single unified view.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 59}, page_content='6: Working with Images 53\\nFigure 6.11 - Unified view of multi-layer image\\nSharing image layers\\nAs previously mentioned, images can share layers, leading to efficiencies in space and\\nperformance.\\nOne of the earlierdocker pull commands generated anAlready existsmessage for one\\nof the layers it pulled. This occurred because one of my Docker Desktop extensions had\\nalready pulled an image that used the exact same layer. As a result, Docker skipped that\\nlayer as it already had a local copy.\\nHere’s the code from earlier, and Figure 6.12 shows two images sharing the same layer.\\n$ docker pull redis:latest\\nlatest: Pulling from library/redis\\n25d3892798f8: Download complete\\ne5d458cf0bea: Download complete\\n4f4fb700ef54: Already exists <<---- This line\\n<Snip>'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 60}, page_content='6: Working with Images 54\\nFigure 6.12 - Two images sharing a layer\\nLayers are also shared on the registry side. This means you can store lots of similar\\nimages in a registry, and the registry will save space by never storing more than a single\\ncopy of any layer.\\nPulling images by digest\\nSo far, you’ve seen how to pull and work with images using names (tags). While this\\nis the most common method, it has a problem — tags are mutable. This means it’s\\npossible to tag an image incorrectly or give a new image the same tag as an older one. An\\nextremely common example is thelatest tag. For example, pulling thealpine:latest\\ntag a year ago will not pull the same image as pulling the same tag today.\\nConsider a quick example outlining one potential implication of trusting mutable tags.\\nImagine you have an image calledgolftrack:1.5 and you get a warning that it has a\\ncritical vulnerability. You build a new image containing the fix and push the new image\\nto the same repository with thesame tag.\\nTake a moment to consider what just happened and the implications.\\nYou have an image calledgolftrack:1.5 that’s being used by lots of containers in your\\nproduction environment, and it has a critical bug. You create a new version containing\\nthe fix. So far, so good, but then you make the mistake. You push the new image to\\nthe same repository with thesame tag as the vulnerable image. This overwrites the\\noriginal image and leaves you without a great way of knowing which of your production'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 61}, page_content='6: Working with Images 55\\ncontainers are using the vulnerable image and which are using the fixed image — both\\nimages have the same tag!\\nThis is whereimage digestscome to the rescue.\\nDocker uses acontent addressable storagemodel where every image gets a cryptographic\\ncontent hashthat we usually call thedigest. As these are hashes of an image’s contents,\\nit’s impossible for two different images to have the same digest. It’s also impossible to\\nchange an image without creating a new unique digest. Fortunately, Docker lets you\\nwork with image digests instead of just names.\\nIf you’ve already pulled an image by name, you can see its digest by running adocker\\nimages command with the--digests flag as shown.\\n$ docker images --digests alpine\\nREPOSITORY TAG DIGEST IMAGE ID CREATED SIZE\\nalpine latest sha256:c5b1261d...8e1ad6b c5b1261d6d3e 2 weeks ago 11.8MB\\nIf you want to find an image’s digestbefore pullingit, you can use thedocker buildx\\nimagetools command. The following example retrieves the image digest for the\\nnigelpoulton/k8sbook/latest image on Docker Hub.\\n$ docker buildx imagetools inspect nigelpoulton/k8sbook:latest\\nName: docker.io/nigelpoulton/k8sbook:latest\\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json\\nDigest: sha256:13dd59a0c74e9a147800039b1ff4d61201375c008b96a29c5bd17244bce2e14b\\n<Snip>\\nYou can now use the digest to pull the image. I’ve trimmed the command and the output\\nfor readability.\\n$ docker pull nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b\\ndocker.io/nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b: Pulling from nigelpoulton/k8sbook\\n59f1664fb787: Download complete\\na052f1888b3e: Download complete\\n94a9f4dfa0e5: Download complete\\nbb7e600677fa: Download complete\\nedfb0c26f1fb: Download complete\\n5b1423465504: Download complete\\n2f232a362cd9: Download complete\\nDigest: sha256:13dd59a0...bce2e14b\\nStatus: Downloaded newer image for nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b\\ndocker.io/nigelpoulton/k8sbook:latest@sha256:13dd59a0...bce2e14b\\nIt’s also possible to directly query the registry API for image data, including digest. The\\nfollowing curl command queries Docker Hub for the digest of the same image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 62}, page_content='6: Working with Images 56\\n$ curl \"https://hub.docker.com/v2/repositories/nigelpoulton/k8sbook/tags/?name=latest\" \\\\\\n|jq \\'.results[].digest\\'\\n\"sha256:13dd59a0c74e9a147800039b1ff4d61201375c008b96a29c5bd17244bce2e14b\"\\nImage hashes and layer hashes\\nYou already know that images are just a loose collection of independent layers. This\\nmeans animage is just a manifest file with some metadata and a list of layers. The\\nactual application and all its dependencies live in thelayers. However, layers are fully\\nindependent and have no concept of being part of an image.\\nWith this in mind, images and layers have their own digests as follows:\\n• Images digests are a crypto hash of the image manifest file\\n• Layer digests are a crypto hash of the layer’s contents\\nThis means all changes to layers or image manifests result in new hashes, giving us an\\neasy and reliable way to know if changes have been made.\\nContent hashes vs distribution hashes\\nDocker compares hashes before and after every push and pull to ensure no tampering\\nhas occurred. However, it also compresses images during push and pull operations to\\nsave network bandwidth and storage space on the registry. As a result of this compres-\\nsion, the before and after hashes won’t match.\\nTo get around this, each layer gets two hashes:\\n• Content hash (uncompressed)\\n• Distribution hash (compressed)\\nEvery time Docker pushes or pulls a layer from a registry, it includes the layer’sdistribu-\\ntion hashand uses this to verify no tapering occurred. This is one reason why the hashes\\nin different CLI and registry outputs don’t always match — sometimes you’re looking at\\nthe content hash, and other times you’re looking at the distribution hash.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 63}, page_content='6: Working with Images 57\\nMulti-architecture images\\nOne of the best things about Docker is its simplicity. However, as technologies grow,\\nthey inevitably get more complex. This happened for Docker when it started supporting\\ndifferent platforms and architectures, such as Windows and Linux on variations of\\nARM, x64, PowerPC, s390x and more. Suddenly, there were multiple versions of\\nthe same image for all the different architectures, and developers and users had to\\nput in significant extra work to get the right version. This broke the smooth Docker\\nexperience.\\nMulti-architecture images to the rescue!\\nFortunately, Docker and the registry API adapted and became clever enough to hide\\nimages for multiple architectures behind a single tag. This means you can do adocker\\npull alpine on any architecture and get the correct version of the image. For example,\\nif you’re on an AMD64 machine, you’ll get the AMD64 image.\\nTo make this happen, the Registry API supports two important constructs:\\n• Manifest lists\\n• Manifests\\nThe manifest listis exactly what it sounds like — a list of architectures supported by an\\nimage tag. Each supported architecture then has its ownmanifest that lists the layers\\nused to build it.\\nRun the following command to see the different architectures supported behind the\\nalpine:latest tag.\\n$ docker buildx imagetools inspect alpine\\nName: docker.io/library/alpine:latest\\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json\\nDigest: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\\nManifests:\\nName: docker.io/library/alpine:latest@sha256:6457d53f...628977d0\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/amd64\\nName: docker.io/library/alpine:latest@sha256:b229a851...d144c1d8\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/arm/v6\\nName: docker.io/library/alpine:latest@sha256:ec299a7b...33b4c6fe\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/arm/v7'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 64}, page_content='6: Working with Images 58\\nName: docker.io/library/alpine:latest@sha256:a0264d60...93467a46\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/arm64/v8\\nName: docker.io/library/alpine:latest@sha256:15c46ced...ab073171\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/386\\nName: docker.io/library/alpine:latest@sha256:b12b826d...ba52a3a2\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/ppc64le\\nIf you look closely, you’ll see a singlemanifest listpointing to sixmanifests.\\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json is the\\nmanifest list.\\nEach MediaType: application/vnd.docker.distribution.manifest.v2+json line\\nrefers to a manifest for each specific architecture.\\nFigure 6.13 shows how manifest lists and manifests are related. On the left, you can see\\na manifest list with entries for the different architectures supported by the image. The\\narrows show that each entry in the manifest list points to a manifest defining the image\\nconfig and the list of layers making up the image for that architecture.\\nFigure 6.13 - Manifest lists and manifests'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 65}, page_content='6: Working with Images 59\\nLet’s step through a quick example.\\nAssume you’re using Docker Desktop on an M3 Mac where Docker runs inside a\\nlinux/arm VM. You ask Docker to pull an image, and Docker makes the relevant calls to\\nthe Registry API to request the appropriate manifest list. Assuming it exists, Docker then\\nparses it alinux/arm entry. Iflinux/arm entry exists, Docker retrieves its manifest,\\nparses it for the crypto IDs of its layers, pulls each individual layer, and assembles them\\ninto the image.\\nLet’s see it in action.\\nThe following examples are from Docker Desktop on an ARM-based Mac and Docker\\nDesktop on an AMD-based Windows machine running inWindows containers mode.\\nBoth start a new container based the officialgolang image and execute thego version\\ncommand. The outputs show the version of Go and the host’s platform and CPU\\narchitecture. Notice how both commands are exactly the same, and Docker takes care\\nof pulling the correct image.\\nBoth images are large and may take a while to download. You do not need to complete\\nthese commands yourself.\\nLinux on arm64 example:\\n$ docker run --rm golang go version\\n<Snip>\\ngo version go1.22.0 linux/arm64\\nWindows on x64 example:\\n> docker run --rm golang go version\\n<Snip>\\ngo version go1.20.4 windows/amd64\\nYou’ve already seen how to use thedocker buildx imagetools command to see the\\nmanifest list and manifests for an image. You can get similar information from the\\ndocker manifest command. The following example inspects the manifest list for\\nthe officialgolang image on Docker Hub. You can see it has images for Linux and\\nWindows on a variety of CPU architectures. You can run the same command without\\nthe grep filter to see the full JSON manifest list. Windows users should replace thegrep\\ncommand withSelect-String architecture,os'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 66}, page_content='6: Working with Images 60\\n$ docker manifest inspect golang | grep \\'architecture\\\\|os\\'\\n\"architecture\": \"amd64\",\\n\"os\": \"linux\"\\n\"architecture\": \"arm\",\\n\"os\": \"linux\",\\n\"architecture\": \"arm64\",\\n\"os\": \"linux\",\\n\"architecture\": \"386\",\\n\"os\": \"linux\"\\n\"architecture\": \"mips64le\",\\n\"os\": \"linux\"\\n\"architecture\": \"ppc64le\",\\n\"os\": \"linux\"\\n\"architecture\": \"s390x\",\\n\"os\": \"linux\"\\n\"architecture\": \"amd64\",\\n\"os\": \"windows\",\\n\"os.version\": \"10.0.20348.2227\"\\n\"architecture\": \"amd64\",\\n\"os\": \"windows\",\\n\"os.version\": \"10.0.17763.5329\"\\nPulling the right image for your system is one thing, but what aboutbuilding images for\\nall these different architectures?\\nThe docker buildx command makes it easy to create multi-architecture images. For\\nexample, you can use Docker Desktop onlinux/arm to build images forlinux/amd\\nand possibly other architectures. We’ll perform builds like these in future chapters, but\\ndocker buildx offers two ways to create multi-architecture images:\\n• Emulation\\n• Build Cloud\\nEmulation modeperforms builds for different architectures on your local machine by\\nrunning the build inside a QEMU virtual machine emulating the target architecture. It\\nworks most of the time but is slow and doesn’t have a shared cache.\\nBuild Cloudis a new service from Docker, Inc. that performs builds in the cloud on\\nnative hardware without requiring emulation. It’s very fast, lets you share a common\\nbuild cache with teammates, and is seamlessly integrated into Docker Desktop and any\\nversion of the Docker Engine using a version of buildx supporting the cloud driver.\\nIt also integrates with GitHub actions and other CI solutions. At the time of writing,\\nDocker Build Cloud is a subscription service you have to pay for.\\nWe’ll use both in future chapters, but I ran the following command to build AMD and\\nARM versions of thenigelpoulton/tu-demo image using Docker Build Cloud.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 67}, page_content='6: Working with Images 61\\n$ docker buildx build \\\\\\n--builder=cloud-nigelpoulton-ddd-cloud \\\\\\n--platform=linux/amd64,linux/arm64 \\\\\\n-t nigelpoulton/tu-demo:latest --push .\\nVulnerability scanning with Docker Scout\\nLots of tools and plugins exist that scan images for known vulnerabilities.\\nWe’ll look at Docker Scout, as it’s built into almost every level of Docker, including the\\nCLI, Docker Desktop, Docker Hub, and thescout.docker.com portal. It’s a very slick\\nservice, but it requires a paid subscription. Other similar products and services exist, but\\nmost require paid subscriptions.\\nRecent versions of Docker Desktop have the Scout CLI plugin pre-installed and ready\\nto go. If you’re running a different version of Docker, you may be able to install the CLI\\nplugin from theGitHub repo14.\\nYou can use thedocker scout quickview command to get a quick vulnerability\\noverview of an image. The following command analyses thenigelpoulton/tu-\\ndemo:latest image. If a local copy doesn’t exist, it pulls it from Docker Hub and\\nperforms the analysis locally.\\n$ docker scout quickview nigelpoulton/tu-demo:latest\\n✓ SBOM of image already cached, 66 packages indexed\\nTarget │ nigelpoulton/tu-demo:latest │ 0C 1H 1M 0L\\ndigest │ b4210d0aa52f │\\nBase image │ python:3-alpine │ 0C 1H 1M 0L\\nUpdated base image │ python:3.11-alpine │ 0C 1H 1M 0L\\n│ │\\nThe output shows zero critical vulnerabilities (0C), one high (1H), one medium (1M),\\nand zero low (0L).\\nYou can use thedocker scout cves command to get more detailed information,\\nincluding remediation advice.\\n14https://github.com/docker/scout-cli'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 68}, page_content='6: Working with Images 62\\n$ docker scout cves nigelpoulton/tu-demo:latest\\n✓ SBOM of image already cached, 66 packages indexed\\n\\uffff Detected 1 vulnerable package with 2 vulnerabilities\\n## Overview\\n│ Analyzed Image\\n────────────────────┼────────────────────────────────\\nTarget │ nigelpoulton/tu-demo:latest\\ndigest │ b4210d0aa52f\\nplatform │ linux/arm64/v8\\nvulnerabilities │ 0C 1H 1M 0L\\nsize │ 26 MB\\npackages │ 66\\n## Packages and Vulnerabilities\\n0C 1H 1M 0L expat 2.5.0-r2\\npkg:apk/alpine/expat@2.5.0-r2?os_name=alpine&os_version=3.19\\n\\uffff HIGH CVE-2023-52425\\nhttps://scout.docker.com/v/CVE-2023-52425\\nAffected range : <2.6.0-r0\\nFixed version : 2.6.0-r0\\n<Snip>\\nI’ve snipped the output so it only shows the critical and high vulnerabilities, but several\\nthings are clear:\\n• It has detected one vulnerable package containing two vulnerabilities\\n• The affected package is calledexpat and the vulnerable version we’re running is\\n2.5.0-r2\\n• It lists the vulnerability asCVE-2023-52425\\n• It includes a link to a Scout report containing more info\\n• It suggests we update to version2.6.0-r0 which contains the fix\\nFigure 6.14 shows how this looks in Docker Desktop, and you get similar integrations\\nand views in Docker Hub.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 69}, page_content='6: Working with Images 63\\nFigure 6.14 - Docker Scout integration with Docker Desktop\\nThe scout.docker.com portal provides an overview dashboard, allows you to configure\\npolicies, and lets you set up integrations with Docker Hub and other registries to\\nremotely scan and monitor multiple repositories.\\nDeleting Images\\nYou can delete images using thedocker rmi command. rmi is short forremove image.\\nDeleting images removes them from your local repository and they’ll no longer show up\\nin yourdocker images commands. The operation also deletes all directories on your\\nlocal filesystem containing layer data. However, Docker won’t delete layers shared by\\nmultiple images until you delete all images that reference them.\\nYou can delete images by name, short ID, or SHA. You can also delete multiple images\\nwith the same command.\\nThe following command deletes three images — one by name, one by short ID, and one\\nby SHA. I’ve trimmed the output for easier reading.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 70}, page_content='6: Working with Images 64\\n$ docker rmi redis:latest af111729d35a sha256:c5b1261d...f8e1ad6b\\nUntagged: redis:latest\\nDeleted: sha256:76d5908f5e19fcdd73daf956a38826f790336ee4707d9028f32b24ad9ac72c08\\nUntagged: nigelpoulton/tu-demo:v2\\nDeleted: sha256:af111729d35a09fd24c25607ec045184bb8d76e37714dfc2d9e55d13b3ebbc67\\nUntagged: alpine:latest\\nDeleted: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\\nDocker will prevent the delete operation if the image is being used by a container or\\nreferenced by more than one tag. However, you can force the operation with the-f\\nflag, but you should do so with caution, as forcing Docker to delete an image in use by\\na container will untag the image and leave it on the system as adangling image.\\nA handy way todelete all imagesis to pass a list of all local image IDs to thedocker rmi\\ncommand. You should use this command with caution, and if you’re following along on\\nWindows, it will only work in a PowerShell terminal.\\n$ docker rmi $(docker images -q) -f\\nTo understand how this works, download a couple of images and then rundocker\\nimages -q.\\n$ docker pull alpine\\n<Snip>\\n$ docker pull ubuntu\\n<Snip>\\n$ docker images -q\\n44dd6f223004\\n3f5ef9003cef\\nSee how thedocker images -q returns a list of local image IDs. Passing this list to\\ndocker rmi will delete all images on the system as shown next.\\n$ docker rmi $(docker images -q) -f\\nUntagged: alpine:latest\\nUntagged: alpine@sha256:02bb6f428431fb...a33cb1af4444c9b11\\nDeleted: sha256:44dd6f2230041...09399391535c0c0183b\\nDeleted: sha256:94dd7d531fa56...97252ba88da87169c3f\\nUntagged: ubuntu:latest\\nUntagged: ubuntu@sha256:dfd64a3b4296d8...9ee20739e8eb54fbf\\nDeleted: sha256:3f5ef9003cefb...79cb530c29298550b92\\nDeleted: sha256:b49483f6a0e69...f3075564c10349774c3\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nLet’s remind ourselves of some of the commands we’ve used.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 71}, page_content='6: Working with Images 65\\nImages – The commands\\n• docker pull is the command to download images from remote registries. It\\ndefaults to Docker Hub but works with other registries. The following command\\nwill pull the image tagged aslatest from thealpine repository on Docker Hub:\\ndocker pull alpine:latest.\\n• docker images lists all of the images in your Docker host’s local repository (image\\ncache). You can add the--digests flag to see the SHA256 hashes.\\n• docker inspect gives you a wealth of image-related metadata in a nicely format-\\nted view.\\n• docker manifest inspect lets you inspect the manifest list of images stored\\nin registries. The following command will show the manifest list for theregctl\\nimage on GitHub Container Registry (GHCR):docker manifest inspect\\nghcr.io/regclient/regctl.\\n• docker buildx is a Docker CLI plugin that works with Docker’s latest build\\nengine features. You saw how to use theimagetools sub-command to query\\nmanifest-related data from images.\\n• docker scout is a Docker CLI plugin that integrates with the Docker Scout\\nbackend to perform image vulnerability scanning. It scans images, provides\\nreports on vulnerabilities, and even suggests remediation actions.\\n• docker rmi is the command to delete images. It deletes all layer data stored in the\\nlocal filesystem, and you cannot delete images associated with containers in the\\nrunning (Up) or stopped (Exited) states.\\nChapter summary\\nThis chapter taught you the important theory and fundamentals of images.\\nYou learned that images contain everything needed to run an application as a container.\\nThis includes just enough OS, source code, dependencies, and metadata.\\nYou can start one or more containers from a single image.\\nUnder the hood, Docker constructs images by stacking one or more read-only layers\\nand presenting them as a unified object. Every image has a manifest that lists the layers\\nthat make up the image and how to stack them.\\nYou learned that image names are also called tags, they’re mutable, and they don’t always\\npull the same image. For example, pulling thealpine:latest tag today will not pull\\nthe same image as it will a year from now. Fortunately, every image gets an immutable\\ndigest that you can use to guarantee you always pull the same image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 72}, page_content='6: Working with Images 66\\nDocker Hub has the notion of curatedofficial imagesthat should be safer to use than\\nunofficial images. However, you should always exercise caution when downloading\\nsoftware from the Internet, even official images from Docker Hub.\\nImages can share layers for efficiency, and Docker makes it easy to build and pull images\\nfor lots of different CPU architectures, such as ARM and AMD.\\nDocker Scout scans images for known vulnerabilities and provides remediation\\nrecommendations. It requires a Docker subscription and is integrated into thedocker\\nCLI, Docker Hub, and Docker Desktop.\\nIn the next chapter, we’ll take a similar tour of containers — the run-time sibling of\\nimages.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 73}, page_content='7: Working with containers\\nDocker implements the Open Container Initiative (OCI) specifications. This means\\nsome of the things you’ll learn in this chapter will apply to other container runtimes and\\nplatforms that implement the OCI specifications.\\nI’ve divided the chapter into the following sections:\\n• Container – The TLDR\\n• Containers vs VMs\\n• Images and containers\\n• Check Docker is running\\n• Starting a container\\n• How containers start apps\\n• Connecting to a container\\n• Inspecting container processes\\n• The docker inspect command\\n• Writing data to a container\\n• Stopping, restarting, and deleting a container\\n• Killing a container’s main process\\n• Debugging slim images and containers with Docker Debug\\n• Self-healing containers with restart policies\\n• The commands\\nContainers – The TLDR\\nContainers are run-time instances of images, and you can start one or more containers\\nfrom a single image.\\nFigure 7.1 shows multiple containers started from a single image. The shared image is\\nread-only, but you can write to the containers.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 74}, page_content='7: Working with containers 68\\nFigure 7.1\\nYou can start, stop, restart, and delete containers just like you can with VMs. However,\\ncontainers are smaller, faster, and more portable than VMs. They’re also designed to\\nbe stateless and ephemeral, whereas VMs are designed to be long-running and can be\\nmigrated with their state and data.\\nContainers are also designed to beimmutable. This means you shouldn’t change them\\nafter you’ve deployed them — if a container fails, you replace it with a new one instead\\nof connecting to it and making the fix in the live instance.\\nContainers should only run a single process and we use them to build microservices\\napps. For example, an application with four features (microservices), such as a web\\nserver, auth, catalog, and store, will have four containers — one running the web server,\\none running the auth service, one running the catalog, and another running the store.\\nContainers vs VMs\\nContainers and VMs are both virtualization technologies for running applications. They\\nboth work on your laptop, bare metal servers, in the cloud, and more. However, the\\nways theyvirtualize are very different:\\n• VMs virtualize hardware\\n• Containers virtualize operating systems\\nIn the VM model, you power on a server and a hypervisor boots. When the hypervisor\\nboots, it claims all hardware resources such as CPU, RAM, storage, and network\\nadapters. To deploy an app, you ask the hypervisor to create a virtual machine. It does\\nthis by carving up the hardware resources into virtual versions, such as virtual CPUs\\nand Virtual RAM, and packaging them into a VM that looks exactly like a physical\\nserver. Once you have the VM, you install an OS and then an app.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 75}, page_content='7: Working with containers 69\\nIn the container model, you power on the same server and an OS boots and claims all\\nhardware resources. You then install a container engine such as Docker. To deploy\\nan app, you ask Docker to create a container. It does this by carving up OS resources\\nsuch as process trees and filesystems into virtual versions and then packaging them as a\\ncontainer that looks exactly like a regular OS. You then tell Docker to run the app in the\\ncontainer.\\nFigure 7.2 shows the two models side by side and attempts to demonstrate the more\\nefficient nature of containers with the same server running 3x more containers than\\nVMs.\\nFigure 7.2\\nIn summary, hypervisors performhardware virtualizationwhere they divide hardware\\nresources into virtual versions and package them as VMs. Container engines perform\\nOS virtualizationwhere they divide OS resources into virtual versions and package them\\nas containers. VMs look and feel exactly like physical servers. Containers look and feel\\nexactly like regular operating systems.\\nThe VM tax\\nOne of the biggest problems with the virtual machine model is that you need to\\ninstall an OS on every VM — every OS consumes CPU, RAM, and storage and takes a\\nrelatively long time to boot.\\nContainers get around all of this by sharing a single OS on the host they’re running on.\\nThis gives containers all of the following benefits over VMs:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 76}, page_content='7: Working with containers 70\\n• Containers are smaller and more portable\\n• You can run more containers on your infrastructure\\n• Containers start faster\\n• Containers reduce the number of operating systems you need to manage (patch,\\nupdate, etc.)\\n• Containers present a smaller attack surface\\nLet’s briefly expand on each point.\\nContainers are smaller than VMs because they only contain application code and a\\nminimal set of OS-related constructs, such as essential filesystem objects. Because of\\nthis, they’re typically only a few megabytes in size. On the other hand, every VM needs a\\nfull OS, meaning they’re usually hundreds or thousands of megabytes.\\nBecause containers don’t contain an OS, you can run a lot more containers than VMs.\\nFor example, deploying 100 applications as VMs will require 100 operating systems,\\neach consuming CPU, memory, and storage, and each needing to be patched and\\nmanaged. However, deploying the same 100 applications as containers requires no\\nadditional operating systems. This drastically reduces your OS management overhead\\nand allows you to allocate more system resources to applications instead of operating\\nsystems.\\nContainers also start faster than VMs because they use the host’s OS which is already\\nbooted. On the other hand, VMs need to go through a full OS bootstrapping process\\nbefore starting the app.\\nOne of the early concerns about containers centered around the shared kernel model\\nwhere all containers on the same host share the host’s kernel. While this offers perfor-\\nmance and portability benefits, it’s less secure than the VM model where every VM has\\nits own dedicated kernel. For example, a rogue container that exploits a vulnerability\\nin the host’s kernel might be able to impact every other container on the same host.\\nFortunately, this is much less of a concern now that container platforms have matured\\nand ship with class-leading tools that can make them more secure than non-container\\nplatforms. For example, most container engines and platforms implementsensible\\ndefaults for security-related technologies such asSELinux, AppArmor, seccomp, capabilities,\\nand more. You can even configure these to make containers more secure than VMs.\\nOther technologies, such as image vulnerability scanning, give you more control over\\nthe security of your software than you ever had before.\\nAt the time of writing, in 2024, containers are the go-to solution for the vast majority of\\nnew applications.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 77}, page_content='7: Working with containers 71\\nPre-reqs\\nYou’ll need a working Docker environment to follow along with the examples, and I\\nrecommend Docker Desktop. Other Docker setups should work, but you may have\\nto manually install the Docker Debug plugin if you want to follow along with those\\nexamples.\\nImages and Containers\\nAs previously mentioned, you can start multiple containers from a single image. The\\nimage is read-only in this relationship, but each container is read-write. As shown\\nin Figure 7.3, Docker accomplishes this by creating a thin read-write layer for each\\ncontainer and placing it on top of the shared image.\\nFigure 7.3 - Container R/W layers\\nIn this example, each container has its own thin R/W layer but shares the same image.\\nThe containers can see and access the files and apps in the imagethrough their own R/W\\nlayer, and if they make any changes, these get written to their R/W layer. When you stop\\na container, Docker keeps the R/W layer and restores it when you restart the container.\\nHowever, when you delete a container, Docker deletes its R/W layer. This way, each\\ncontainer can make and keep its own changes without changing the shared image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 78}, page_content='7: Working with containers 72\\nCheck Docker is running\\nRun adocker version to check Docker is running. It’s a good command because it\\nchecks the CLI and engine components.\\n$ docker version\\nClient:\\nCloud integration: v1.0.35+desktop.13\\nVersion: 26.1.1\\nAPI version: 1.45\\nOS/Arch: darwin/arm64\\n<Snip>\\nServer: Docker Desktop 4.30.0 (149282)\\nEngine:\\nVersion: 26.1.1\\nAPI version: 1.45 (minimum version 1.24)\\nOS/Arch: linux/arm64\\n<Snip>\\nAs long as you get a response from theClient and Server, you’re good to go and can\\nskip to the next section.\\nIf you get an error code in theServer section, this usually means your Docker daemon\\n(server) isn’t running or your user account doesn’t have permission to access it. If\\nyou’re running on Linux, you’ll need to ensure your user account is a member of the\\nlocal docker Unix group. If it isn’t, you can add it by runningusermod -aG docker\\n<username> and restarting your shell. Alternatively, you can prefix alldocker commands\\nwith sudo.\\nYour account needs to be a member of thedocker group so it can access the API which\\nis exposed on a privileged local Unix socket at/var/run/docker.sock. It’s also possible\\nto expose the API over the network.\\nIf your user account is already a member of the localdocker group and you still get an\\nerror from the daemon, there’s a good chance the Docker daemon isn’t running. Run\\none of the following commands to check the status of the daemon.\\nLinux systems not using Systemd.\\n$ service docker status\\ndocker start/running, process 29393\\nLinux systems using Systemd.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 79}, page_content=\"7: Working with containers 73\\n$ systemctl is-active docker\\nactive\\nIf the daemon isn’t running, start it with the appropriate command for your system.\\nStarting a container\\nThe docker run command is the simplest and most common way to start a new\\ncontainer.\\nRun the following command to start a new container calledwebserver.\\n$ docker run -d --name webserver -p 5005:8080 nigelpoulton/ddd-book:web0.1\\nUnable to find image 'nigelpoulton/ddd-book:web0.1' locally\\nweb0.1: Pulling from nigelpoulton/ddd-book\\n4f4fb700ef54: Already exists\\ncf2a607f33f7: Download complete\\n0a1f0c111e9a: Download complete\\nc1af4b5db242: Download complete\\nDigest: sha256:3f5b281b914b1e39df8a1fbc189270a5672ff9e98bfac03193b42d1c02c43ef0\\nStatus: Downloaded newer image for nigelpoulton/ddd-book:web0.1\\nb5594b3b8b3fdce544d2ca048e4340d176bce9f5dc430812a20f1852c395e96b\\nLet’s take a closer look at the command and the output.\\ndocker run tells Docker to run a new container\\nThe -d flag tells Docker to run it in the background as adaemon processand detached\\nfrom your local terminal\\nThe name flag tells Docker to name this containerwebserver.\\nThe -p 5005:8080 flag maps port5005 on your local system to port8080 inside the\\ncontainer. This works because the container’s web server is listening on port8080.\\nThe nigelpoulton/ddd-book:web0.1 argument tells Docker which image to use to start\\nthe container.\\nWhen you hitReturn, the Docker client converted the command into an API request\\nand posted it to the Docker API exposed by the Docker daemon. The Docker daemon\\naccepted the command and searched its local image repository for a copy of the\\nnigelpoulton/ddd-book:web0.1 image. It didn’t find one, so it searched Docker Hub.\\nIn the example, it found one on Docker Hub and pulled a local copy.\\nOnce it had a local copy of the image, the daemon made a request to containerd asking\\nfor a new container. containerd then instructed runc to create the container and start\\nthe app. It also performed the port mapping.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 80}, page_content='7: Working with containers 74\\nRun the following commands to verify the image was pulled locally and a new container\\ncalled webserver is running.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book web0.1 3f5b281b914b 12 minutes ago 159MB\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND STATUS PORTS NAMES\\nb5594b3b8b3f nigelpoulton... \"node ./app.js\" Up 2 mins 0.0.0.0:80->8080/tcp webserver\\nYou can also connect a browser to port5005 on your Docker host to test the app. If\\nyou’re using Docker Desktop, point your browser tolocalhost:5005. If you’re not\\nrunning Docker Desktop, you may need to substitutelocalhost with the name or IP\\nof the host Docker is running on.\\nFigure 7.4 - Web app running on container\\nCongratulations. Docker pulled a local copy of the image and started a container\\nrunning the app defined in the image.\\nHow containers start apps\\nIn the previous section, you created a container running a web app. But how did the\\ncontainer know which app to start and how to start it?\\nThere are three ways you can tell Docker how to start an app in a container:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 81}, page_content='7: Working with containers 75\\n1. An Entrypoint instruction in the image\\n2. A Cmd instruction in the image\\n3. A CLI argument\\nYou’ll learn more about these in the next chapter, but theEntrypoint and Cmd instruc-\\ntions are optional image metadata that store the command Docker uses to start the\\ndefault app. Then, whenever you start a container from the image, Docker checks the\\nEntrypoint or Cmd instruction and executes the stored command.\\nEntrypoint instructions cannot be overridden on the CLI, and anything you pass in via\\nthe CLI will be appended to the Entrypoint instruction as an argument.\\nCmd instructions can be overridden by CLI arguments.\\nRun the following command to see if thenigelpoulton/ddd-book:web0.1 image has\\nan Entrypoint instruction. The command searches the image metadata and returns\\nany lines containing the word“Entrypoint” and the three lines immediately following it.\\nWindows users will need to replace thegrep command withSelect-String -Pattern\\n\\'Entrypoint\\' -Context 0,3.\\n$ docker inspect nigelpoulton/ddd-book:web0.1 | grep Entrypoint -A 3\\n<Snip>\\n\"Entrypoint\": [\\n\"node\",\\n\"./app.js\"\\n],\\nThis image has an Entrypoint instruction that translates into the following command\\n— node ./app.js. If you’re not familiar with Node.js, it’s a simple command telling the\\nNode.js runtime to execute the code in theapp.js file.\\nIf an image doesn’t have an Entrypoint instruction, you can search for the presence of a\\nCmd instruction.\\nIf an image doesn’t have either, you’ll need to pass an argument on the CLI.\\nThe format of thedocker run command is:\\ndocker run <arguments> <image> <command>\\nAs mentioned, the<command> is optional; you don’t need it if the image has an En-\\ntrypoint or Cmd instruction. If you specify a <command>, it will override a Cmd\\ninstruction but will be appended to an Entrypoint instruction.\\nThe following command starts a new background container based on theAlpine image\\nand tells it to run thesleep 20 command, causing it to run for 20 seconds and then exit.\\nThe --rm flag cleans up the exited container so you don’t have to delete it manually.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 82}, page_content='7: Working with containers 76\\n$ docker run --rm -d alpine sleep 20\\nIf you run adocker ps command before the 20-second sleep timer expires, you’ll see\\nthe container in the output. If you run it after 20 seconds, the container will be gone.\\nThe --rm argument automatically cleans up the exited container.\\nMost production images will specify an Entrypoint or Cmd instruction.\\nConnecting to a container\\nYou can use thedocker exec command to execute commands in running containers,\\nand it has two modes:\\n• Interactive\\n• Remote execution\\nInteractive exec sessions connect your terminal to a shell process in the container and\\nbehave like remote SSH sessions. Remote execution mode lets you send commands to a\\nrunning container and prints the output to your local terminal.\\nRun the following command to start an interactive exec session by creating a new shell\\nprocess (sh) inside thewebserver container and connecting your terminal to it. The-\\nit flag makes it aninteractive exec session, and thesh argument starts a newsh process\\ninside the container.sh is a minimal shell program installed in the container.\\n$ docker exec -it webserver sh\\n/src #\\nNotice how your shell prompt changed. This proves your terminal is connected to the\\nshell process inside the container.\\nTry executing a few common Linux commands. Some will work, and some won’t. This\\nis because container images are usually optimized to be lightweight and don’t have all of\\nthe normal commands and packages installed. The following example shows a couple of\\ncommands — one succeeds, and the other one fails.\\nThe examples list the contents of your current directory and try to edit theapp.py file\\nwith thevim editor.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 83}, page_content='7: Working with containers 77\\n/src # ls -l\\ntotal 100\\n-rw-r--r-- 1 root root 324 Feb 20 12:35 Dockerfile\\n-rw-r--r-- 1 root root 377 Feb 20 12:35 README.md\\n-rw-r--r-- 1 root root 341 Feb 20 12:35 app.js\\ndrwxr-xr-x 183 root root 4096 Feb 20 12:41 node_modules\\n-rw-r--r-- 1 root root 74342 Feb 20 12:41 package-lock.json\\n-rw-r--r-- 1 root root 404 Feb 20 12:38 package.json\\ndrwxr-xr-x 2 root root 4096 Feb 20 12:35 views\\n<Snip>\\n/src # vim app.js\\nsh: vim: not found\\nThe vim command fails because it isn’t installed in the container.\\nInspecting container processes\\nMost containers only run a single process. This is the container’s main app process and\\nis always PID 1.\\nRun aps command to see the processes running in your container. You’ll need to be\\nconnected to the exec session from the previous section for these commands to work.\\n/src # ps\\nPID USER TIME COMMAND\\n1 root 0:00 node ./app.js\\n13 root 0:00 sh\\n22 root 0:00 ps\\nThe output shows three processes:\\n• PID 1 is the main application process running the Node.js web app\\n• PID 13 is the shell process your interactive exec session is connected to\\n• PID 22 is theps command you just ran\\nThe ps process terminated as soon as it displayed the output, and thesh process will\\nterminate when you exit theexec session. This means the only long-running process is\\nPID 1 running the Node app.\\nIf you kill the container’s main process (PID 1), you’ll also kill the container. This is\\nbecause containers only run while their main process is running — when that process\\nis no longer running, there’s no reason for the container to run. We’ll demonstrate this\\nlater.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 84}, page_content='7: Working with containers 78\\nType exit to quit the exec session and return to your local terminal.\\nRun anotherdocker exec command without specifying the-it flags. This will\\nremotely execute the command without creating an interactive session. The format of\\nthe command isdocker exec <container> <command>, and it will only work if the\\ncommand you’re executing is installed in the container.\\n$ docker exec webserver ps\\nPID USER TIME COMMAND\\n1 root 0:00 node ./app.js\\n42 root 0:00 ps\\nThis time, only two processes are running because you terminated thesh process when\\nyou typedexit to quit the previous interactive exec session.\\nThe docker inspect command\\nYou’ll absolutely love thedocker inspect command as it’s a treasure trove of detailed\\ninformation about images and containers.\\nThe following command retrieves full details of the runningwebserver container, and\\nI’ve snipped the output to highlight a few interesting things. However, I recommend you\\nrun the command on your own system and study the full output, as you’ll learn a lot.\\n$ docker inspect webserver\\n<Snip>\\n\"State\": {\\n\"Status\": \"running\"\\n},\\n\"Name\": \"/webserver\",\\n\"PortBindings\": {\\n\"8080/tcp\": [\\n{\\n\"HostIp\": \"\",\\n\"HostPort\": \"5005\"\\n}\\n]\\n},\\n\"RestartPolicy\": {\\n\"Name\": \"no\",\\n\"MaximumRetryCount\": 0\\n\"Image\": \"nigelpoulton/ddd-book:web0.1\",\\n\"WorkingDir\": \"/src\",\\n\"Entrypoint\": [\\n\"node\",\\n\"./app.js\"'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 85}, page_content='7: Working with containers 79\\n],\\n}\\n<Snip>\\nThe snipped output shows the container isrunning, is calledwebserver, is binding port\\n8080 in the container to5005 on the host, has no restart policy, and is based on the\\nnigelpoulton/ddd-book:web0.1 image. TheEntrypoint block lists the command that\\nautomatically runs every time the container starts.\\nWe’ll cover this in more detail later, but this container inherited its Entrypoint instruc-\\ntion from the image you started it from. You can verify this by running the following\\ndocker inspect command against the image. I’ve snipped the output to highlight the\\nrelevant section.\\n$ docker inspect nigelpoulton/ddd-book:web0.1\\n<Snip>\\n\"Config\": {\\n\"WorkingDir\": \"/src\",\\n\"Entrypoint\": [\\n\"node\",\\n\"./app.js\"\\n],\\n<Snip>\\nI recommend you take time to investigate the output ofdocker inspect commands.\\nYou’ll learn a lot.\\nWriting data to a container\\nIn this section, you’ll exec onto thewebserver container and edit the web server\\nconfiguration to display a new message on the home page. In the next section, you’ll\\nstop and restart the container and verify your changes aren’t lost.\\nWARNING: This section is for demonstration purposes only. In the real\\nworld, you shouldn’t change live containers like this. Any time you need to\\nchange a live container, you should create and test a new container with the\\nrequired changes and then replace the existing container with the new one.\\nOpen a new interactive exec session to thewebserver container with the following\\ncommand.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 86}, page_content='7: Working with containers 80\\n$ docker exec -it webserver sh\\n/src #\\nThe container runs a simple Node.js web app that uses theviews/home.pug file to build\\nthe app’s home page.\\nRun the following command to open thehome.pug file in thevi editor. Windows users\\ncan use Notepad or another editor.\\n% vi views/home.pug\\nIf you know how to usevi, you can go ahead and change the text on line 8 after theh1\\ntag to anything you like and save your changes.\\nCarefully follow these steps if you’re not comfortable withvi:\\n1. Press thei key to putvi into insert mode\\n2. Use the arrow keys to navigate to line 8\\n3. Use yourdelete key to delete the textafter the h1 tag on line 8\\n4. Type a new message of your choice\\n5. Press theescape key to exitinsert modeand return tocommand mode\\n6. type :wq and pressenter save your changes and exit (:wq is short for write and\\nquit)\\nOnce you’ve saved your changes, refresh your browser to see the updates.\\nType exit to quit the exec session and return to your local terminal.\\nCongratulations, you’ve updated the web server config.\\nStopping, restarting, and deleting a container\\nIn this section, you’ll execute the typical container lifecycle events and see how they\\nimpact the changes you’ve made to the container.\\nThe following commands will only work if you’ve quit the interactive exec session.\\nCheck your container is still running.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 87}, page_content='7: Working with containers 81\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND STATUS PORTS NAMES\\nb5594b3b8b3f nigelpoulton... \"node ./app.js\" Up 51 mins 0.0.0.0:80->8080 webserver\\nStop it with thedocker stop command. It will take up to 10 seconds to stop gracefully.\\n$ docker stop webserver\\nwebserver\\nRun anotherdocker ps command.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\nThe container no longer shows in the list of running containers. However, you can see it\\nif you run the same command with the-a flag to showall containers, including stopped\\nones.\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND STATUS NAMES\\nb5594b3b8b3f nigelpou... \"node ./app.js\" Exited (137) About a minute ago webserver\\nAs you can see in the output, it still exists but is in theExited state. Restart it with the\\nfollowing command.\\n$ docker restart webserver\\nwebserver\\nIf you run anotherdocker ps, you’ll see it in theUp state.\\nRefresh your browser to see if Docker has saved your changes to the home page or\\nreverted to the original.\\nThe container has saved your changes!\\nYou can also run the following command to return the exact contents of the file directly\\nfrom within the container’s filesystem.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 88}, page_content=\"7: Working with containers 82\\n$ docker exec webserver cat views/home.pug\\nhtml\\nhead\\ntitle='Docker FTW'\\nlink(rel='stylesheet', href='https://stackpath.bootstrapcdn.com/....\\nbody\\ndiv.container\\ndiv.jumbotron\\nh1 Everybody loves containers! <<---- I changed this line\\n<Snip>\\nSo far, you’ve seen that starting and stopping containers doesn’t lose changes. You also\\nsaw that restarting them is very fast.\\nRun the following command to delete the container. The-f flag forces the operation\\nand doesn’t allow the app the usual 10-second grace period to flush buffers and grace-\\nfully quit. Be careful forcing operations like this, as Docker doesn’t ask you to confirm.\\n$ docker rm webserver -f\\nwebserver\\nRun adocker ps -a to see if there’s any sign of the container.\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\nAll signs of the container are gone and you cannot restart it. You can start a new\\ninstance by executing anotherdocker run command and specifying the same image,\\nbut it won’t have the changes you made.\\nWARNING: As previously mentioned, changing live containers like this is an\\nanti-pattern and you shouldn’t do it. We only showed it here to demonstrate\\nhow containers work and how changes to the container’s filesystem (made to\\nthe container’s own thin R/W layer) persist across restarts.\\nKilling a container’s main process\\nEarlier in the chapter, we learned that containers are designed to run a single process,\\nand killing this process also kills the container.\\nLet’s test if that’s true.\\nRun the following command to start a new interactive container calledddd-ctr based on\\nthe Ubuntu image and tell it to run a Bash shell as its main process.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 89}, page_content='7: Working with containers 83\\n$ docker run --name ddd-ctr -it ubuntu:24.04 bash\\nUnable to find image \\'ubuntu:24.04\\' locally\\n24.04: Pulling from library/ubuntu\\n51ae9e2de052: Download complete\\nDigest: sha256:ff0b5139e774bb0dee9ca8b572b4d69eaec2795deb8dc47c8c829becd67de41e\\nStatus: Downloaded newer image for ubuntu:24.04\\nroot@d3c892ad0eb3:/#\\nThe command pulls the Ubuntu image and attaches your terminal to the container’s\\nBash shell process.\\nRun aps command to list all running processes.\\nroot@d3c892ad0eb3:/# ps\\nPID TTY TIME CMD\\n1 pts/0 00:00:00 bash\\n9 pts/0 00:00:00 ps\\nPID 1 is the container’s main process and is the Bash shell you told the container to run.\\nThe other one is theps command and has already exited. This means the Bash process is\\nthe only process running in the container.\\nIf you typeexit, you’ll terminate the Bash processand kill the container. This is because\\ncontainers only run while their main process executes.\\nTest this by typingexit to return to your local terminal and then running adocker ps\\n-a command to see if the container terminated.\\nroot@d3c892ad0eb3:/# exit\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND STATUS NAMES\\nd3c892ad0eb3 ubuntu:24.04 \"bash\" Exited (0) 3 secs ago ddd-ctr\\nAs expected, the container is in theexited state and not running. However, you can run\\nthe following two commands to restart it and attach your shell to its main process.\\n$ docker restart ddd-ctr\\nddd-ctr\\n$ docker attach ddd-ctr\\nroot@d3c892ad0eb3:/#\\nYour terminal is once again attached to the Bash shell in the container.\\nYou can typeCtrl PQ to exit a container without killing the process you’re attached to.\\nType Ctrl PQ to exit the container and run anotherdocker ps command to verify the\\ncontainer is still running this time.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 90}, page_content='7: Working with containers 84\\nroot@d3c892ad0eb3:/# <Ctrl PQ>\\nread escape sequence\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND STATUS NAMES\\nd3c892ad0eb3 ubuntu:24.04 \"bash\" Up 27 seconds ddd-ctr\\nThe container is still up.\\nNow that you know how to exit containers without killing them, let’s switch focus and\\nsee how to useDocker Debugto debug slim containers and images.\\nDebugging slim images and containers with Docker\\nDebug\\nAt the time of writing, Docker Debug is a new tool and requires a Pro, Team, or Business\\nsubscription.\\nIt’s a widely accepted good practice to deploy small images containing only app code\\nand dependencies. This means no shell or debugging tools and is a big part of making\\nimages and containers small and secure. However, it also makes it difficult to debug\\nthem when things go wrong.\\nThis is whereDocker Debug comes to the rescue by allowing you to get shell access\\nto images and containers that don’t include a shell and seamlessly injecting powerful\\ndebugging tools into them.\\nAt a high level, Docker Debug works by attaching a shell to a container and mounting a\\ntoolbox loaded with debugging tools. Thistoolbox is mounted as a directory called/nix\\nand is available during your debugging session but is never visible to the container. As\\nsoon as you exit the Docker Debug session, the/nix directory is removed. If you’re\\ndebugging a running container, any changes you make are immediately visible to the\\ncontainer and persist across container restarts. For example, updating anindex.html\\nduring a Docker Debug session will immediately update the running web app, and the\\nchanges will persist if the container is stopped and restarted. If you’re debugging an\\nimage or stopped container, the Docker Debug session creates a debug sandbox and\\nadds it to the image as a R/W layer to make it feel like a running container. However,\\nchanges you make while debugging an image or stopped container arenot persisted and\\nare lost as soon as you quit the debug session.\\nIf you’ve been following along, you’ll have a running container calledddd-ctr. If you\\ndon’t, you can start one by runningdocker run --name ddd-ctr -it ubuntu:24.04\\nbash.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 91}, page_content='7: Working with containers 85\\nRun the following commands to attach to the container and see if it has any debugging\\ntools. The followingdocker attach command is similar to thedocker exec commands\\nyou learned earlier but automatically connects to a container’s main process. You don’t\\nneed to run thedocker attach command if you’re already connected to the container.\\n$ docker attach ddd-ctr\\nroot@d3c892ad0eb3:/#\\nroot@d3c892ad0eb3:/# ping nigelpoulton.com\\nbash: ping: command not found\\nroot@d3c892ad0eb3:/# nslookup nigelpoulton.com\\nbash: nslookup: command not found\\nroot@d3c892ad0eb3:/# vim\\nbash: vim: command not found\\nThe commands all failed because none of the tools are installed in this container. This\\nwould make debugging this container difficult without Docker Debug.\\nType Ctrl PQ to gracefully disconnect from the container without killing the Bash\\nprocess.\\nIn the following steps, you’ll use Docker Debug to get a shell session to the container\\nand run commands that aren’t installed in the container. You can even use Docker\\nDebug to get shell access to containers and images that don’t include a shell.\\nYou need to log in to Docker to use Docker Debug, and it only works if you have a Pro,\\nTeam, or Business license.\\n$ docker login\\nAuthenticating with existing credentials...\\nLogin Succeeded\\nRun the following command to check if you have the Docker Debug CLI plugin. All\\nmodern versions of Docker Desktop include this by default. Other Docker installations\\nmay not have it, and you’ll have to install it manually.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 92}, page_content='7: Working with containers 86\\n$ docker info\\nClient:\\nVersion: 26.1.1\\nContext: desktop-linux\\nDebug Mode: false\\nPlugins:\\ndebug: Get a shell into any image or container. (Docker Inc.)\\nVersion: 0.0.29\\nPath: /Users/nigelpoulton/.docker/cli-plugins/docker-debug\\n<Snip>\\nOnce you’re logged in and have the plugin installed, you’re ready to use Docker Debug.\\nThe format of the command isdocker debug <image>|<container>. We’ll open a\\nDocker Debug session to the running container calledddd-ctr.\\n$ docker debug ddd-ctr\\nThis is an attach shell, i.e.:\\n- Any changes to the container filesystem are visible to the container directly.\\n- The /nix directory is invisible to the actual container.\\nVersion: 0.0.29 (BETA)\\nroot@d3c892ad0eb3 / [ddd-ctr]\\ndocker >\\nYou’ve successfully connected to the running container and got a new shell prompt\\n(docker >). You also got some helpful info displaying the short ID and name of the\\ncontainer you’re debugging, as well as a reminder that any changes you make will be\\nvisible to the container.\\nTry running theping, nslookup, andvim commands that failed in the previous section.\\nIf you get stuck in thevim session, just type:q and pressEnter.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 93}, page_content=\"7: Working with containers 87\\ndocker > ping nigelpoulton.com\\nPING nigelpoulton.com (192.124.249.126) 56(84) bytes of data.\\n64 bytes from cloudproxy10126.sucuri.net (192.124.249.126): icmp_seq=1 ttl=63 time=211 ms\\n64 bytes from cloudproxy10126.sucuri.net (192.124.249.126): icmp_seq=2 ttl=63 time=58.3 ms\\n^C\\ndocker > nslookup nigelpoulton.com\\nzsh: command not found: nslookup\\ndocker > vim\\n~ VIM - Vi IMproved\\n~ version 9.0.1441\\n~ by Bram Moolenaar et al.\\n~ Vim is open source and freely distributable\\n<Snip>\\n:q\\nThe ping and vim commands worked, but thenslookup still failed. This is because the\\ndefault Docker Debugtoolbox includes ping and vim but doesn’t includenslookup. Don’t\\nworry, though. You can use Docker Debug’s built-ininstall command to add any\\npackage listed onsearch.nixos.org.\\nRun the following command to install thebind package (which includes thenslookup\\ntool), and then run thenslookup command again.\\ndocker > install bind\\nTip: You can install any package available at: https://search.nixos.org/packages.\\ninstalling 'bind-9.18.19'\\n<Snip>\\ndocker > nslookup nigelpoulton.com\\nServer: 192.168.65.7\\nAddress: 192.168.65.7#53\\nNon-authoritative answer:\\nName: nigelpoulton.com\\nAddress: 192.124.249.126\\nThe command worked, andnslookup is now installed in yourtoolbox and will be\\navailable in future Docker Debug sessions.\\nCongratulations, you’ve used Docker Debug to attach to a running container and run\\ntroubleshooting commands that aren’t part of the container. You’ve also seen how to\\ninstall additional tools to your Docker Debug toolbox. Remember, any changes you\\nmake to running containers are immediately visible to the container and persist after\\nyou close the session.\\nType exit to terminate the debug session and return to your local shell.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 94}, page_content='7: Working with containers 88\\nRun the following command to create a new Docker Debug session that debugs the\\nnigelpoulton/ddd-book:web0.1 image. Docker will automatically pull the image from\\nDocker Hub if you don’t have a local copy.\\n$ docker debug nigelpoulton/ddd-book:web0.1\\nNote: This is a sandbox shell. All changes will not affect the actual image.\\nVersion: 0.0.29 (BETA)\\nroot@3f5b281b914b /src [nigelpoulton/ddd-book:web0.1]\\ndocker >\\nNotice the different message this time. Debugging images creates asandbox shelland\\nchanges won’t affect the actual image. This reminds you that debugging images and\\nstopped containers behaves differently from debugging running containers:\\n• Changes made while debugging a live container are persisted\\n• Changes made while debugging images or stopped containers are deleted when\\nyou quit the debug session\\nRun annslookup command to prove the tool is saved to your toolbox and available for\\nuse without re-installing.\\ndocker > nslookup craigalanson.com\\nServer: 192.168.65.7\\nAddress: 192.168.65.7#53\\nNon-authoritative answer:\\nName: craigalanson.com\\nAddress: 198.185.159.144\\n<Snip>\\nDocker Debug has a built-inentrypoint command that lets you print, lint, and test an\\nimage or container’sEntrypoint or Cmd command. These are the commands Docker\\nexecutes to start the container’s app.\\nRun the followingentrypoint command to reveal the default command this container\\nwill run when it starts.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 95}, page_content='7: Working with containers 89\\ndocker > entrypoint --print\\nnode ./app.js\\nThe entrypoint command is clever enough to look for Entrypointand Cmd instruc-\\ntions.\\nType exit to quit the debug session.\\nIn summary, Docker Debug is a new tool for debuggingslim images and containers. It\\ngets you shell access to containers and images that don’t include a shell, and you can\\nrun troubleshooting tools that aren’t available in the container or image. Any changes\\nyou make torunning containerstake immediate effect and persist across stop and restart\\noperations. However, changes made while debuggingimages and stopped containersare\\nlost when you close the session. In all cases, the tools you install and use are never part\\nof the container or image.\\nSelf-healing containers with restart policies\\nContainer restart policiesare a simple form of self-healing that allows the local Docker\\nEngine to automatically restart failed containers.\\nYou applyrestart policiesper container, and Docker supports the following four policies:\\n• no (default)\\n• on-failure\\n• always\\n• unless-stopped\\nThe following table shows how each policy reacts to different scenarios. AY indicates\\nthe policy will attempt a container restart, whereas anN indicates it won’t.\\nRestart\\nRestart Non-zero Zero docker stop when Daemon\\npolicy exit code exit code command restarts\\nno N N N N\\non-failure Y N N Y\\nalways Y Y N Y\\nunless-stopped Y Y N N\\nNon-zero exit codes indicate a failure occurred. Zero exit codes indicate the container\\nexited normally without an error.\\nWe’ll demo some examples, but you should also do your own testing.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 96}, page_content='7: Working with containers 90\\nLet’s demonstrate thealways policy by starting a new interactive container with the--\\nrestart always flag and telling it to run a shell process. We’ll then typeexit to kill the\\nshell processand the container to see what happens.\\nRun the following command to start an interactive container calledneversaydie with\\nthe always restart policy.\\n$ docker run --name neversaydie -it --restart always alpine sh\\n/#\\nYour terminal will automatically connect to the shell process inside the container.\\nType exit to kill the shell process and return to your local terminal. This will cause the\\ncontainer to exit with a zero exit code, indicating a normal exit without any failures.\\nAccording to the previous table, thealways restart policy should automatically restart\\nthe container.\\nRun adocker ps command to see if this happened.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\n1933623830bb alpine \"sh\" 15 seconds ago Up 2 seconds neversaydie\\nThe container is running as expected. However, you can see it was created 15 seconds\\nago but has only been running for 2 seconds. This is because you forced it to exit\\nwhen you killed the shell process, and then Docker automatically restarted it. It’s also\\nimportant to know that Docker restarted the same container and didn’t create a new\\none. In fact, if you run adocker inspect against it, you’ll see theRestartCount has\\nbeen incremented to 1. Remember to replacegrep with to replacegrep with Select-\\nString -Pattern \\'RestartCount\\' if you’re on Windows using PowerShell.\\n$ docker inspect neversaydie | grep RestartCount\\n\"RestartCount\": 1,\\nAn interesting feature of the--restart always policy is that if you stop a container\\nwith docker stop and then restart the Docker daemon, Docker will restart the con-\\ntainer when the daemon comes up. To be clear:\\n1. You start a new container with the--restart always policy\\n2. You manually stop it with thedocker stop command\\n3. You restart Docker (or an event causes Docker to restart)\\n4. When Docker comes back up, it starts thestopped container\\nIf you don’t want this behavior, you should try theunless-stopped policy.\\nIf you are working with Docker Compose or Docker Stacks, you can apply restart\\npolicies toservices as follows. We’ll cover these in more detail in later chapters.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 97}, page_content='7: Working with containers 91\\nservices:\\nmyservice:\\n<Snip>\\nrestart_policy:\\ncondition: always | unless-stopped | on-failure\\nClean up\\nYou can rundocker images and docker ps -a commands to see the images you pulled\\nand the containers you created as part of this chapter. Your output will be similar to this.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book web0.1 3f5b281b914b 4 days ago 159MB\\nubuntu 24.04 ff0b5139e774 13 days ago 138MB\\nalpine latest c5b1261d6d3e 4 weeks ago 11.8MB\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\nac165419214f alpine \"sh\" 33 secs ago Up 24 seconds neversaydie\\n5bd3741185fa ubuntu:24.04 \"bash\" 3 mins ago Exited (0) ~1min ago ddd-ctr\\nYou can delete individual containers with thedocker rm <container> -f command\\nand images with thedocker rmi command, and you should always delete containers\\nbefore images.\\nYou can also deleteall containersand all imageswith the following two commands. Be\\nwarned though, they don’t prompt you for confirmation.\\n$ docker rm $(docker ps -aq) -f\\nac165419214f\\n5bd3741185fa\\n$ docker rmi $(docker images -q)\\nUntagged: nigelpoulton/ddd-book:web0.1\\nDeleted: sha256:3f5b281b914b1e39df8a1fbc189270a5672ff9e98bfac03193b42d1c02c43ef0\\nUntagged: ubuntu:24.04\\nDeleted: sha256:ff0b5139e774bb0dee9ca8b572b4d69eaec2795deb8dc47c8c829becd67de41e\\nUntagged: alpine:latest\\nDeleted: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\\nBoth commands work by passing a list of all container/image IDs to the delete com-\\nmand.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 98}, page_content='7: Working with containers 92\\nContainers – The commands\\n• docker run is the command to start new containers. You give it the name of an\\nimage and it starts a container from it. This example starts an interactive container\\nfrom the Ubuntu image and tells it to run the Bash shell:docker run -it ubuntu\\nbash.\\n• Ctrl-PQ is how you detach from a container without killing the process you’re\\nattached to. You’ll use it frequently to detach from running containers without\\nkilling them.\\n• docker ps lists all running containers, and you can add the-a flag to also see\\ncontainers in the stopped(Exited) state.\\n• docker exec allows you to run commands inside containers. The following\\ncommand will start a new Bash shell inside a running container and connect your\\nterminal to it:docker exec -it <container-name> bash. For this to work, the\\ncontainer must include the Bash shell. This command runs aps command inside\\na running container without opening an interactive shell session:docker exec\\n<container-name> ps.\\n• docker stop stops a running container and puts it in theExited (137) state. It\\nissues aSIGTERM to the container’s PID 1 process and allows the container 10\\nseconds to gracefully quit. If the process hasn’t cleaned up and stopped within 10\\nseconds, it sends aSIGKILL to force the container to immediately terminate.\\n• docker restart restarts a stopped container.\\n• docker rm deletes a stopped container. You can add the-f flag to delete the\\ncontainer without having to stop it first.\\n• docker inspect shows you detailed configuration and run-time information\\nabout a container.\\n• docker debug attaches a debug shell to a container or image and lets you run\\ncommands that aren’t available inside the container or image. It requires a Pro,\\nTeam, or Business Docker subscription.\\nChapter summary\\nIn this chapter, you learned some of the major differences between VMs and containers,\\nincluding that containers are smaller, faster, and more portable.\\nYou learned how to start, stop, and restart containers with thedocker CLI, and you saw\\nthat changes to a container’s filesystem persist across restarts.\\nYou learned that containers run a single process and cannot run if this process is killed.\\nYou also saw the three ways of telling a container which app to run and how to start it —\\nvia Entrypoint or Cmd instructions in the image metadata or via thedocker run CLI.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 99}, page_content='7: Working with containers 93\\nYou learned about the new Docker Debug tool and how it allows you to get a shell to\\nslim containers and run troubleshooting commands that don’t exist in the container.\\nFinally, you learned how to attach restart policies to containers and how the different\\nrestart policies work.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 100}, page_content='8: Containerizing an app\\nDocker makes it easy to package applications as images and run them as containers.\\nWe call this processcontainerization, and this chapter will walk you through the entire\\nprocess.\\nI’ve divided the chapter as follows:\\n• Containerizing an app – The TLDR\\n• Containerize a single-container app\\n• Moving to production with multi-stage-builds\\n• Buildx, BuildKit, drivers, and Build Cloud\\n• Multi-architecture builds\\n• A few good practices\\nContainerizing an app – The TLDR\\nDocker aims to make it easy tobuild, ship,and run applications. We call thiscontaineriza-\\ntion and the process looks like this:\\n1. Write your applications and create the list of dependencies\\n2. Create aDockerfile that tells Docker how to build and run the app\\n3. Build the app into an image\\n4. Push the image to a registry (optional)\\n5. Run a container from the image\\nYou can see these five steps in Figure 8.1.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 101}, page_content='8: Containerizing an app 95\\nFigure 8.1 - Basic flow of containerizing an app\\nContainerize a single-container app\\nIn this section, you’ll complete the following steps to containerize a simple Node.js app:\\n• Get the application code from GitHub\\n• Create the Dockerfile\\n• Containerize the app\\n• Run the app\\n• Test the app\\n• Look a bit closer\\nI recommend you follow along with Docker Desktop. This is because we’ll be using the\\nnew docker init command, which might not be installed on other versions of Docker.\\nDon’t worry if you don’t have access to a Docker installation with thedocker init\\nplugin, you can manually copy the Dockerfile and follow the rest of the examples.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 102}, page_content=\"8: Containerizing an app 96\\nGet the application code\\nThe application we’ll use is a simple Node.js web app that serves a web page on port\\n8080.\\nIf you still need to clone the repo, run the following command to clone it. You’ll need\\ngit installed, and it will create a new directory calledddd-book.\\n$ git clone https://github.com/nigelpoulton/ddd-book.git\\nCloning into 'ddd-book'...\\nremote: Enumerating objects: 47, done.\\nremote: Counting objects: 100% (47/47), done.\\nremote: Compressing objects: 100% (32/32), done.\\nremote: Total 47 (delta 11), reused 44 (delta 11), pack-reused 0\\nReceiving objects: 100% (47/47), 167.30 KiB | 1.66 MiB/s, done.\\nResolving deltas: 100% (11/11), done.\\nChange into theddd-book/node-app directory and list its contents.\\n$ cd ddd-book/node-app\\n$ ls -l\\ntotal 98\\n-rw-r--r--@ 1 nigelpoulton staff 341 20 Feb 12:35 app.js\\ndrwxr-xr-x 103 nigelpoulton staff 3296 12 Mar 16:18 node_modules\\n-rw-r--r-- 1 nigelpoulton staff 39975 12 Mar 16:18 package-lock.json\\n-rw-r--r--@ 1 nigelpoulton staff 355 8 Mar 10:10 package.json\\ndrwxr-xr-x 3 nigelpoulton staff 96 20 Feb 12:35 views\\nThis directory is yourbuild contextbecause it contains the application source code and\\nthe files listing dependencies.\\nLet’s create the Dockerfile.\\nCreate the Dockerfile\\nIn the past, you had to create Dockerfiles manually. Fortunately, newer versions of\\nDocker support thedocker init command that analyses applications and automatically\\ncreates Dockerfiles that implement good practices.\\nRun the following command to create a Dockerfile for the app. If your Docker installa-\\ntion doesn’t have thedocker init plugin, you’ll have to skip this step.\\nFeel free to accept a newer version of Node.js, but complete all other prompts as shown.\\nYou’ll need to run it from thenode-app directory.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 103}, page_content='8: Containerizing an app 97\\n$ docker init\\nWelcome to the Docker Init CLI!\\n<Snip>\\n? What application platform does your project use? Node\\n? What version of Node do you want to use? 20.8.0 <<---- Newer versions are OK\\n? Which package manager do you want to use? npm\\n? What command do you want to use to start the app? node app.js\\n? What port does your server listen on? 8080\\nCREATED: .dockerignore\\nCREATED: Dockerfile\\nCREATED: compose.yaml\\nCREATED: README.Docker.md\\n\\uffff Your Docker files are ready!\\nThe process created a new Dockerfile and placed it in your current directory. It looks\\nlike this.\\n1. ARG NODE_VERSION=20.8.0\\n2. FROM node:${NODE_VERSION}-alpine\\n3. ENV NODE_ENV production\\n4. WORKDIR /usr/src/app\\n5. RUN --mount=type=bind,source=package.json,target=package.json \\\\\\n--mount=type=bind,source=package-lock.json,target=package-lock.json \\\\\\n--mount=type=cache,target=/root/.npm \\\\\\nnpm ci --omit=dev\\n6. USER node\\n7. COPY . .\\n8. EXPOSE 8080\\n9. CMD node app.js\\nLines 1 and 2 tell Docker to pull thenode:20.8.0-alpine image and use it as the base\\nfor the new image.\\nLine 3 tells Node to run inproduction mode. This is a Node.js optimization that increases\\nperformance while minimizing logging and other common development features.\\nLine 4 sets the working directory for the remaining steps. For example, theRUN and COPY\\ninstructions on lines 5 and 7 will run against theWORKDIR directory, as will thenode\\napp.js command on line 9.\\nLine 5 bind mounts the dependency files and installs them with thenpm ci --omit-dev\\ncommand.\\nLine 6 ensures Node.js runs the app as a non-root user.\\nLine 7 copies the application’s source code from your build context (the first period)\\ninto theWORKDIR directory (the second period) inside the image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 104}, page_content='8: Containerizing an app 98\\nLine 8 documents the application’s network port.\\nLine 9 is the command Docker will execute whenever it starts a container from the\\nimage.\\nYou now have everything Docker needs to build the application into a container image\\n— source code, dependencies, and a Dockerfile.\\nContainerize the app\\nIn this section, you’ll build the application into a container image.\\nIf your Docker installation doesn’t have thedocker init plugin and you didn’t follow\\nthe previous step, you’ll need to rename thesample-Dockerfile to Dockerfile before\\ncontinuing.\\nRun the following command to build a new image calledddd-book:ch8.node. Be sure to\\ninclude the trailing period (.) as this tells Docker to use your current working directory\\nas thebuild context. Remember, thebuild contextis the directory where your app files live.\\n$ docker build -t ddd-book:ch8.node .\\n[+] Building 16.2s (12/12) FINISHED\\n=> [internal] load build definition from Dockerfile 0.0s\\n=> => transferring dockerfile: 1.21kB 0.0s\\n=> => transferring context: 659B 0.0s\\n=> [stage-0 1/4] FROM docker.io/library/node:20.8.0-alpine 3s <<---- Base layer\\n=> [stage-0 2/4] WORKDIR /usr/src/app 0.2s <<---- New layer\\n=> [stage-0 3/4] RUN --mount=type=bind,source=package... 1.1s <<---- New layer\\n=> [stage-0 4/4] COPY . . 0.1s <<---- New layer\\n=> exporting to image 0.2s\\n=> => exporting layers 0.2s\\n=> => writing image sha256:f282569b8bd0f0...016cc1adafc91 0.0s\\n=> => naming to docker.io/library/ddd-book:ch8.node\\nI’ve snipped the output, but you can see four numbered steps that created four image\\nlayers.\\nCheck that the image exists in your Docker host’s local repository.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\nddd-book ch8.node 24dd040fa06b 18 minutes ago 268MB\\nCongratulations, you’ve containerized the app as an OCI image!\\nRun adocker inspect ddd-book:ch8.node command to verify the image and see the\\nsettings from the Dockerfile. You should be able to see the image layers and metadata\\nsuch as theExposed Ports, WorkingDir, andEntrypoint values.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 105}, page_content='8: Containerizing an app 99\\n$ docker inspect ddd-book:ch8.node\\n[\\n{\\n\"Id\": \"sha256:24dd040fa06baf6e40144c5a59f99a749159a932ecebb737751f7f862963527a\",\\n\"RepoTags\": [\\n\"ddd-book:ch8.node\"\\n<Snip>\\n\"ExposedPorts\": {\\n\"8080/tcp\": {}\\n\"WorkingDir\": \"/usr/src/app\",\\n\"Cmd\": [\\n\"/bin/sh\",\\n\"-c\",\\n\"node app.js\"\\n],\\n<Snip>\\n\"Layers\": [\\n\"sha256:5f4d9fc4d98de91820d2a9c81e501c8cc6429bc8758b43fcb2cd50f4cab9a324\",\\n\"sha256:6b20c4e93dbab9786f96268bbe32c208d385f2c4490a278ad3b1e55cc79480e4\",\\n\"sha256:012c308a78ec993a47fdb7c4c6d17b53d8ce2649a463be28ae5c48ab1af2e039\",\\n\"sha256:35a839ac7cc922afd896a0297e692141c77ed6e03eff6a70db13bb23f6cd4f8f\",\\n\"sha256:918caa8070410ccfb2c5b3b4d62ca66742c46bf21fe0bd433738b7796c530e68\",\\n\"sha256:a48b3b3d0c5a693840e7e4abd7971f130b4447573483628bcb996091e1e8e8b8\",\\n\"sha256:ea2d4594dbbef4009441a33dd1dd4c5076d7fe09a171381a6b7583605569dd11\"\\n]\\n<Snip>\\nYou might wonder why the image has seven layers when only four Dockerfile instruc-\\ntions created layers. This is because thenode:20.8.0-alpine base image already had\\nfour layers. Therefore, theFROM instruction pulled a base image with four layers, and\\nthen theWORKDIR, RUN and COPY instructions added three more layers. You can see this\\nin Figure 8.2.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 106}, page_content='8: Containerizing an app 100\\nFigure 8.2 - Dockerfile and image layers\\nPush the image to Docker Hub\\nThis is an optional section, and you’ll need a Docker Hub account to follow along. Go to\\nhub.docker.com and sign up for one now; they’re free.\\nYou’ll complete the following steps:\\n1. Login to Docker Hub\\n2. Re-tag the image\\n3. Push the image\\nAfter creating images, you’ll normally push them to a registry where you can keep them\\nsafe and make them accessible to teammates and clients. Lots of registries exist, but\\nDocker Hub is the most common public registry and is where Docker pushes images\\nby default.\\nLog in to Docker Hub.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 107}, page_content='8: Containerizing an app 101\\n$ docker login\\nLogin with your Docker ID to push and pull images from Docker Hub.\\nUsername: nigelpoulton\\nPassword:\\nWARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\\nConfigure a credential helper to remove this warning.\\nOnce logged in, you need to re-tag the image. This is because Docker uses information\\nfrom the image tag to determine which registry and repository to push it to.\\nIf you run adocker images command, you’ll see an image tagged asddd-book:ch8.node.\\nIf you push this image, Docker will try to push it to a repository calledddd-book on\\nDocker Hub. However, no such repository exists, and the command will fail.\\nRun the following command to re-tag the image to include your Docker ID. The format\\nof the command isdocker tag <current-tag> <new-tag>, and it creates an additional\\ntag for the same image.\\n$ docker tag ddd-book:ch8.node nigelpoulton/ddd-book:ch8.node\\nRun anotherdocker images command to see the image with both tags. Notice how\\neverything is identical except theREPO column. This is because it’s the same image with\\ndifferent names.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book ch8.node 24dd040fa06b 38 minutes ago 268MB\\nddd-book ch8.node 24dd040fa06b 38 minutes ago 268MB\\nPush it to Docker Hub. You’ll need to be logged in with your Docker ID for this to work.\\n$ docker push nigelpoulton/ddd-book:ch8.node\\nThe push refers to repository [docker.io/nigelpoulton/ddd-book]\\ne4ef261755c8: Pushed\\nd25f74b85615: Pushed\\n7e1aebde141d: Pushed\\n7b3f8039e3c4: Pushed\\n2a2799ae89a2: Mounted from library/node\\n4927cb899c33: Mounted from library/node\\n579b34f0a95b: Pushed\\nced319b3ffb5: Pushed\\nch8.node: digest: sha256:24dd040fa06baf...1f7f862963527a size: 856\\nFigure 8.3 shows how Docker figured out where to push the image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 108}, page_content='8: Containerizing an app 102\\nFigure 8.3\\nNow that you’ve pushed the image to a registry, you can access it from anywhere with\\nan internet connection. You can also grant other people access to pull it and push\\nchanges.\\nRun the app\\nAs previously mentioned, the application is a web server that listens on port8080.\\nRun the following command to start it as a container. You’ll have to delete the\\nnigelpoulton image prefix or replace it with your ID.\\n$ docker run -d --name c1 \\\\\\n-p 5005:8080 \\\\\\nnigelpoulton/ddd-book:ch8.node\\nThe -d flag runs the container in the background, and the--name flag calls itc1. The\\n-p 5005:8080 maps port5005 on your Docker host to port8080 inside the container,\\nwhich means you’ll be able to point a browser to port5005 and reach the app. The last\\nline tells Docker to base the container on thenigelpoulton/ddd-book:ch8.node image\\nyou just built.\\nDocker will use the local copy of the image from the previous steps. It only pulls a copy\\nfrom Docker Hub if it doesn’t have a local copy.\\nCheck the container is running and verify the port mapping.\\n$ docker ps\\nID IMAGE COMMAND STATUS PORTS NAMES\\n49.. ddd-book:ch8.node \"node ./app.js\" UP 6 secs 0.0.0.0:5005->8080/tcp c1\\nI’ve snipped the output for readability, but the container is running, and port5005 is\\nmapped on all the Docker host’s interfaces (0.0.0.0:5005).'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 109}, page_content='8: Containerizing an app 103\\nTest the app\\nOpen a web browser and point it to the DNS name or IP address of your Docker host\\non port5005. If you’re using Docker Desktop or a similar local environment, you can\\nconnect tolocalhost:5005. Otherwise, use the IP or DNS of the Docker host on port\\n5005.\\nYou should see the app as shown in Figure 8.4.\\nFigure 8.4\\nYou can try the following if it doesn’t work:\\n1. Run adocker ps command to ensure thec1 container is running\\n2. Check port mapping is correct —0.0.0.0:5005->8080/tcp\\n3. Check that firewall and other network security settings aren’t blocking traffic to\\nyour Docker host on port5005\\nCongratulations, the application is containerized and running as a container!\\nLooking a bit closer\\nNow that you’ve containerized the application let’s take a closer look at how some of the\\nmachinery works.\\nThe docker build command parses the Dockerfile one line at a time, starting from the\\ntop.\\nYou can insert comments by starting a line with the# character, and the builder will\\nignore them.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 110}, page_content='8: Containerizing an app 104\\nAll non-comment lines are calledinstructions or steps and take the format<INSTRUCTION>\\n<arguments>. Instruction names are not case-sensitive, but it’s common to write them in\\nUPPERCASE to make them easier to read.\\nSome instructions create new layers, whereas others add metadata.\\nExamples of instructions that create new layers areFROM, RUN, COPY and WORKDIR.\\nExamples that create metadata includeEXPOSE, ENV, CMD, andENTRYPOINT. The premise\\nis this:\\n• Instructions that addcontent, such as files and programs, create new layers\\n• Instructions that don’t add content don’t add layers and only create metadata\\nYou can run adocker history command against any image to see the instructions that\\ncreated it.\\n$ docker history ddd-book:ch8.node\\nIMAGE CREATED BY SIZE\\n24dd...a06b CMD [\"/bin/sh\" \"-c\" \"node app.js\"] 0B buildkit.dockerfile.v0\\n<missing> EXPOSE map[8080/tcp:{}] 0B buildkit.dockerfile.v0\\n<missing> COPY . . # buildkit 86kB buildkit.dockerfile.v0\\n<missing> USER node 0B buildkit.dockerfile.v0\\n<missing> RUN /bin/sh -c npm ci --omit=dev # buildkit 12.8MB buildkit.dockerfile.v0\\n<missing> WORKDIR /usr/src/app 16.4kB buildkit.dockerfile.v0\\n<missing> ENV NODE_ENV=production 0B buildkit.dockerfile.v0\\n<missing> /bin/sh -c #(nop) CMD [\"node\"] 0B\\n<Snip>\\n<missing> /bin/sh -c #(nop) ADD file:ff3112828967e8… 8.35MB\\nA few things are worth noting from the output.\\nThe bottom few lines that I’ve snipped from the book related to the history of the\\nnode:20.8.0-alpine base image that was pulled by theFROM instruction.\\nAll lines ending withbuildkit.dockerfile.v0 relate to instructions from the Docker-\\nfile used to build the image.\\nThe CREATED BY column lists the exact Dockerfile instruction that created the layer or\\nmetadata.\\nLines with a non-zero value in theSIZE column created new layers, whereas the lines\\nwith 0B only added metadata. In this example, three lines/instructions created layers.\\nRun adocker inspect to see the list of image layers.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 111}, page_content='8: Containerizing an app 105\\n$ docker inspect ddd-book:ch8.node\\n<Snip>\\n},\\n\"RootFS\": {\\n\"Type\": \"layers\",\\n\"Layers\": [\\n\"sha256:5f4d9fc4d98de91820d2a9c81e501c8cc6429bc8758b43fcb2cd50f4cab9a324\",\\n\"sha256:6b20c4e93dbab9786f96268bbe32c208d385f2c4490a278ad3b1e55cc79480e4\",\\n\"sha256:012c308a78ec993a47fdb7c4c6d17b53d8ce2649a463be28ae5c48ab1af2e039\",\\n\"sha256:35a839ac7cc922afd896a0297e692141c77ed6e03eff6a70db13bb23f6cd4f8f\",\\n\"sha256:918caa8070410ccfb2c5b3b4d62ca66742c46bf21fe0bd433738b7796c530e68\",\\n\"sha256:a48b3b3d0c5a693840e7e4abd7971f130b4447573483628bcb996091e1e8e8b8\",\\n\"sha256:ea2d4594dbbef4009441a33dd1dd4c5076d7fe09a171381a6b7583605569dd11\"\\n]\\n},\\nAs previously mentioned, the output shows seven layers because the base image had four\\nlayers, and the Dockerfile added three more.\\nFigure 8.5 maps the Dockerfile instructions to image layers. The bold instructions with\\narrows create layers; the others create metadata. The layer IDs will be different in your\\nenvironment.\\nFigure 8.5\\nNote: Older builders didn’t create a layer forWORKDIR instructions. However,\\nthe instruction modifies filesystem permissions and the current builder\\ncreates a very small layer. This behavior may change in the future.\\nIt’s generally considered a good practice to useDocker Official Imagesand Verified Pub-\\nlisher images as thebase layerfor new images you create. This is because they maintain a'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 112}, page_content='8: Containerizing an app 106\\nhigh standard and quickly implement fixes for known vulnerabilities.\\nMoving to production with multi-stage builds\\nWhen it comes to container images…big is bad!For example:\\n• Big means slow\\n• Big means more potential vulnerabilities\\n• Big means a larger attack surface\\nFor these reasons, your container images should only contain the stuffneeded to run\\nyour applications in production.\\nThis is wheremulti-stage buildscome into play.\\nAt a high level, multi-stage builds use a single Dockerfile with multipleFROM instructions\\n— eachFROM instruction represents a newbuild stage. This allows you to have astage\\nwhere you do the heavy lifting of building the app inside a large image with compilers\\nand other build tools, but then you have another stage where you copy the compiled app\\ninto a slim image for production. The builder can even run different stages in parallel\\nfor faster builds.\\nFigure 8.6 shows a high-level workflow. Stage 1 builds an image with all the required\\nbuild and compilation tools. Stage 2 copies the app code into the image and builds it.\\nStage 3 creates a small production-ready image containing only the compiled app and\\nanything needed to run it.\\nFigure 8.6\\nLet’s look at an example!'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 113}, page_content='8: Containerizing an app 107\\nWe’ll work with the code in themulti-stage folder of the book’s GitHub repo. It’s a\\nsimple Go app with a client and server borrowed from theDocker samples buildmerepo\\non GitHub. Don’t worry if you’re not a Go programmer; you don’t need to be. You only\\nneed to know that it compiles theclient and server apps into executable files thatdo not\\nneed the Go language or any other tools or runtimes to execute.\\nHere’s the Dockerfile:\\nFROM golang:1.22.1-alpine AS base <<---- Stage 0\\nWORKDIR /src\\nCOPY go.mod go.sum .\\nRUN go mod download\\nCOPY . .\\nFROM base AS build-client <<---- Stage 1\\nRUN go build -o /bin/client ./cmd/client\\nFROM base AS build-server <<---- Stage 2\\nRUN go build -o /bin/server ./cmd/server\\nFROM scratch AS prod <<---- Stage 3\\nCOPY --from=build-client /bin/client /bin/\\nCOPY --from=build-server /bin/server /bin/\\nENTRYPOINT [ \"/bin/server\" ]\\nThe first thing to note is that there are fourFROM instructions. Each of these is a distinct\\nbuild stage, and Docker numbers them starting from 0. However, we’ve given each stage a\\nfriendly name:\\n• Stage 0 is calledbase and builds an image with compilation tools, etc\\n• Stage 1 is calledbuild-client and compiles the client executable\\n• Stage 2 is calledbuild-server and compiles the server executable\\n• Stage 3 is calledprod and copies the client and server executables into a slim image\\nEach stage outputs an intermediate image that later stages can use. However, Docker\\ndeletes them when the final stage completes.\\nThe goal of thebase stage is to create a reusable build image with all the tools stages\\n1 and 2 need to build the client and server applications. The image created by this\\nstage is only used to compile the executables and not for production. It pulls the\\ngolang:1.22.1-alpine image, which is over 300MB when uncompressed. It sets the\\nworking directory to/src and copies in thego.mod and go.sum files from your working\\ndirectory. These files list the application dependencies and hashes. After that, it uses the\\nRUN instruction to install the dependencies and then theCOPY instruction to copy the\\napplication source code into the image. All of this creates a large image with three layers'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 114}, page_content='8: Containerizing an app 108\\ncontaining a lot of build stuff but not much app stuff. When this build stage completes, it\\noutputs a large image that later stages can use.\\nThe build-client stage doesn’t pull a new image. Instead, it uses theFROM base AS\\nbuild-client instruction to use the intermediate image created by thebase stage. It\\nthen issues aRUN instruction to compile the client app into a binary executable. The goal\\nof this stage is to create an image with the compiledclient binary that can be referenced\\nby later stages.\\nThe build-server stage does the same for theserver component and outputs a similar\\nimage for use by later stages.\\nThe prod stage pulls the minimalscratch image. It then runs twoCOPY --from instruc-\\ntions to copy the compiledclient appfrom thebuild-client stage and the compiled\\nserver appfrom thebuild-server stage. It then tells Docker to run the server app when\\nit’s started as a container. This stage outputs the final production image containing just\\nthe client and server binaries inside a tiny scratch image and the metadata telling Docker\\nhow to start the app.\\nThe builder will run thebase stage first, then run thebuild-client and build-server\\nstages in parallel, and finally run theprod stage.\\nIt will always attempt to run stages in parallel, but it can only do this when no depen-\\ndency exists. For example, thebuild-client and build-server stages depend on the\\nbase stage and cannot run until that stage completes (both start withFROM base...).\\nHowever, thebuild-client and build-server can run in parallel because they don’t\\ndepend on each other. To work out if build stages can run in parallel, start reading\\nthe Dockerfile from the top and check if theFROM instructions reference otherFROM\\ninstructions immediately before or after — if they do, they can’t run in parallel.\\nLet’s see it in action.\\nChange into themulti-stage directory and verify the Dockerfile and associated app\\nfiles exist.\\n$ ls -l\\ntotal 28\\n-rw-rw-r-- 1 ubuntu ubuntu 368 Mar 25 10:09 Dockerfile\\n-rw-rw-r-- 1 ubuntu ubuntu 433 Mar 25 10:09 Dockerfile-final\\n-rw-rw-r-- 1 ubuntu ubuntu 305 Mar 25 10:09 README.md\\ndrwxrwxr-x 4 ubuntu ubuntu 4096 Mar 25 10:09 cmd\\n-rw-rw-r-- 1 ubuntu ubuntu 1013 Mar 25 10:09 go.mod\\n-rw-rw-r-- 1 ubuntu ubuntu 5631 Mar 25 10:09 go.sum\\nBuild the image and watch thebuild-client and build-server stages execute in\\nparallel. This can significantly improve the performance of large builds.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 115}, page_content='8: Containerizing an app 109\\n$ docker build -t multi:full .\\n[+] Building 30.1s (15/15) FINISHED\\n=> [internal] load build definition from Dockerfile 0.0s\\n=> => transferring dockerfile: 409B 0.0s\\n<Snip>\\n=> [build-client 1/1] RUN go build -o /bin/client ./cmd/client 4.9s <<---- parallel\\n=> [build-server 1/1] RUN go build -o /bin/server ./cmd/server 4.8s <<---- parallel\\n<Snip>\\nRun adocker images to see the new image.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\nmulti full a7a01440f2b5 5 seconds ago 25.2MB\\nThe final production image is only 25MB, much smaller than the 300MB+ base image\\npulled by thebase stage to build and compile the app. This is because the finalprod\\nstage extracted the compiled client and server binaries and placed them in a tiny new\\nscratch image.\\nRun adocker history to see the final production image. It only has two layers — one\\ncreated by copying in the client binary and the other by copying in the server binary.\\nNone of the previous build stages are included in the final production image.\\n$ docker history multi:full\\nIMAGE CREATED CREATED BY SIZE\\na7a01440f2b5 4 minutes ago ENTRYPOINT [\"/bin/server\"] 0B\\n<missing> 4 minutes ago COPY /bin/server /bin/ # buildkit 8.2MB\\n<missing> 4 minutes ago COPY /bin/client /bin/ # buildkit 8.028MB\\nMulti-stage builds and build targets\\nYou can also build multiple images from a single Dockerfile.\\nThe previous example compiled client and server apps and copied both into the same\\nimage. However, Docker makes it easy to create a separate image for each by splitting\\nthe finalprod stage into two stages as follows:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 116}, page_content='8: Containerizing an app 110\\nFROM golang:1.20-alpine AS base\\nWORKDIR /src\\nCOPY go.mod go.sum .\\nRUN go mod download\\nCOPY . .\\nFROM base AS build-client\\nRUN go build -o /bin/client ./cmd/client\\nFROM base AS build-server\\nRUN go build -o /bin/server ./cmd/server\\nFROM scratch AS prod-client <<---- New stage\\nCOPY --from=build-client /bin/client /bin/\\nENTRYPOINT [ \"/bin/client\" ]\\nFROM scratch AS prod-server <<---- New stage\\nCOPY --from=build-server /bin/server /bin/\\nENTRYPOINT [ \"/bin/server\" ]\\nI’ve pre-created the file and called itDockerfile-final in themulti-stage folder, but\\nyou can see the only change is splitting the finalprod stage into two stages — one for\\nthe client build and the other for the server build. With a Dockerfile like this, you tell a\\ndocker build command which of the two final stages to target for the build.\\nLet’s do it.\\nRun the following two commands to create two different images from the same\\nDockerfile-final file. Both commands use the-f flag to tell Docker to use the\\nDockerfile-final file. They also use the--target flag to tell the builder which stage\\nto build from.\\n$ docker build -t multi:client --target prod-client -f Dockerfile-final .\\n<Snip>\\n$ docker build -t multi:server --target prod-server -f Dockerfile-final .\\n<Snip>\\nCheck the builds and image sizes.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nmulti full a7a01440f2b5 4 hours ago 25.2MB\\nmulti server a75778df1b9c 4 seconds ago 11.7MB\\nmulti client 02b621e9415f 37 seconds ago 11.9MB\\nYou now have three images, and theclient and server images are each about half the\\nsize of thefull image. This makes sense because thefull image contains the client and\\nserver binaries, whereas the others only include one.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 117}, page_content='8: Containerizing an app 111\\nBuildx, BuildKit, drivers, and Build Cloud\\nThis section takes a quick look at the major components that power builds.\\nBehind the scenes, Docker’s build system has a client and server:\\n• Client: Buildx\\n• Server: BuildKit\\nBuildx is Docker’s latest and greatest build client. It’s implemented as a CLI plugin and\\nsupports all the latest features of BuildKit, such as multi-stage builds, multi-architecture\\nimages, advanced caching, and more. It’s been the default build client since Docker v23.0\\nand Docker Desktop v4.19.\\nYou can configure Buildx to talk to multiple BuildKit instances, and we call each\\ninstance of BuildKit abuilder. Builders can be on your local machine, in your cloud or\\ndatacenter, or Docker’sBuild Cloud.\\nIf you point buildx at a local builder, image builds will be done on your local machine.\\nIf you point it at a remote builder, such as Docker Build Cloud, builds will be done on\\nremote infrastructure.\\nFigure 8.7 shows a Docker environment configured to talk to a local and a remote\\nbuilder.\\nFigure 8.7 - Docker build architecture\\nIn the diagram, the local builder uses thedocker-container driver to create a BuildKit\\ninstance inside a dedicated container. All builds using this driver will run in the\\ndedicated container. The other option uses the cloud driver to send builds to Docker’s'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 118}, page_content='8: Containerizing an app 112\\nBuild Cloud service. Build Cloud offers fast builds and a shared cache but requires a\\npaid subscription.\\nWhen you run adocker build command, buildx interprets the command and sends\\nthe build request to the selectedbuilder. This includes the Dockerfile, command line\\narguments, caching options, export options, and the build context (app and dependency\\nlist). Thebuilder performs the build and exports the image, while the buildx client\\nreports on progress.\\nRun the following command to see the builders you have configured on your system.\\nI’ve trimmed the output in the book, but you can see a local and a remote builder.\\n$ docker buildx ls\\nNAME/NODE DRIVER/ENDPOINT PLATFORMS\\nbuilder * docker-container\\nbuilder0 desktop-linux linux/arm64, linux/amd64, linux/amd64/v2,\\nlinux/riscv64, linux/ppc64le, linux/s390x,\\nlinux/386, linux/mips64le, linux/mips64,\\nlinux/arm/v7, linux/arm/v6\\ncloud-nigelpoulton-ddd cloud\\nlinux-arm64 cloud://nigel...arm64 linux/arm64*\\nlinux-amd64 cloud://nigel...amd64 linux/amd64*, linux/amd64/v2,\\nlinux/amd64/v3,linux/amd64/v4\\n<Snip>\\nNotice how the first builder supports more platforms than the cloud builder. This is\\nbecause thedocker-container driver utilizes QEMU to emulate target hardware. It\\nusually works but can be slow.\\nThe second builder is Docker’s Build Cloud, which only supports AMD and ARM builds.\\nBuilds running in Build Cloud run on native hardware and offer a shared cache so that\\nteammates can share a common cache for even faster builds. Complex builds can be\\nmuch quicker when executed on native hardware such as Build Cloud.\\nRun adocker buildx inspect command against one of your builders.\\n$ docker buildx inspect cloud-nigelpoulton-ddd\\nName: cloud-nigelpoulton-ddd\\nDriver: cloud\\nNodes:\\nName: linux-arm64\\nEndpoint: cloud://nigelpoulton/ddd_linux-arm64\\nStatus: running\\nBuildkit: v0.12.5\\nPlatforms: linux/arm64*\\nLabels:\\norg.mobyproject.buildkit.worker.executor: oci'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 119}, page_content='8: Containerizing an app 113\\norg.mobyproject.buildkit.worker.hostname: 2d0e9f41699c\\norg.mobyproject.buildkit.worker.network: host\\norg.mobyproject.buildkit.worker.oci.process-mode: sandbox\\norg.mobyproject.buildkit.worker.selinux.enabled: false\\norg.mobyproject.buildkit.worker.snapshotter: overlayfs\\nGC Policy rule#0:\\nAll: true\\nKeep Bytes: 21.42GiB\\n<Snip>\\nLet’s see how to perform multi-architecture builds.\\nMulti-architecture builds\\nYou can use thedocker build command to build images for multiple architectures,\\nincluding ones different from your local machine. For example:\\n• Docker on an AMD machine can build ARM images\\n• Docker on an ARM machine can build AMD images\\nYou also have the option to perform build builds locally or in the cloud. Both work with\\nthe standarddocker build command and only require minimal backend configuration.\\nRun the following command to list your current builders. Remember, abuilder is an\\ninstance of BuildKit that will perform builds.\\n$ docker buildx ls\\nNAME/NODE DRIVER/ENDPOINT PLATFORMS\\nbuilder * docker-container\\nbuilder0 desktop-linux linux/arm64, linux/amd64, linux/amd64/v2,\\nlinux/riscv64, linux/ppc64le, linux/s390x,\\nlinux/386, linux/mips64le, linux/mips64,\\nlinux/arm/v7, linux/arm/v6\\ncloud-nigelpoulton-ddd cloud\\nlinux-arm64 cloud://nigel...arm64 linux/arm64*\\nlinux-amd64 cloud://nigel...amd64 linux/amd64*, linux/amd64/v2, linux/amd64/v3,\\nlinux/amd64/v4\\n<Snip>\\nThe book’s output shows two builders; the one with the asterisk (*) is the default builder.\\nIn this example, the default builder is calledbuilder and uses thedocker-container\\ndriver to perform builds inside a local build container. Unless you specify a different\\nbuilder, all builds will run inside this build container. It supports multiple architectures,\\nincluding AMD, ARM, RISC-V, s390x, and more.\\nIf you don’t already have one, create a new builder calledcontainer that uses thedocker-\\ncontainer driver with the following command.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 120}, page_content='8: Containerizing an app 114\\n$ docker buildx create --driver=docker-container --name=container\\nbuilder\\nRun anotherdocker buildx ls to show the new builder. Don’t worry if it shows as\\npresent but inactive.\\nMake it the default builder.\\n$ docker buildx use container\\nChange into theweb-app directory and run the following command to build the app\\ninto AMD and ARM images and export them directly to Docker Hub.\\nBe sure to substitute your Docker ID as the command pushes directly to Docker Hub\\nand will fail if you try to push it to my repositories. If you don’t have a Docker Hub\\naccount or don’t want to push the images, you can replace the--push with --load.\\n$ docker buildx build --builder=container \\\\\\n--platform=linux/amd64,linux/arm64 \\\\\\n-t nigelpoulton/ddd-book:ch8.1 --push .\\n[+] Building 79.3s (26/26) FINISHED\\n<Snip>\\n=> [linux/arm64 2/5] RUN apk add --update nodejs npm curl 19.0s\\n=> [linux/amd64 2/5] RUN apk add --update nodejs npm curl 17.4s\\n=> [linux/amd64 3/5] COPY . /src 0.0s\\n=> [linux/amd64 4/5] WORKDIR /src 0.0s\\n=> [linux/amd64 5/5] RUN npm install 7.3s\\n=> [linux/arm64 3/5] COPY . /src 0.0s\\n=> [linux/arm64 4/5] WORKDIR /src 0.0s\\n=> [linux/arm64 5/5] RUN npm install 5.6s\\n=> exporting to image\\n<Snip>\\n=> => pushing layers 31.5s\\n=> => pushing manifest for docker.io/nigelpoulton/ddd-book:web0.2@sha256:8fc61... 3.6s\\n=> [auth] nigelpoulton/ddd-book:pull,push token for registry-1.docker.io 0.0s\\nI’ve snipped the output, but you can still see two important things:\\n• Each Dockerfile instruction was executed twice — once for AMD and once for\\nARM\\n• The last few lines show the image layers being pushed directly to Docker Hub\\nNow that you’ve performed a build, the builder will show asactive and list the architec-\\ntures it supports.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 121}, page_content='8: Containerizing an app 115\\nFigure 8.8 shows how the images for both architectures appear on Docker Hub under\\nthe same repository and tag.\\nFigure 8.8 - Multi-platform image\\nYou can also perform the builds using Docker Build Cloud. This is a cloud-based service\\nthat offers fast builds and lets you share your build cache with teammates. It requires a\\npaid subscription.\\nIf you have a Docker subscription that grants you access to Build Cloud, you can go to\\nbuild.docker.com and configure your first cloud builder. You can also create cloud\\nbuilders from the CLI as follows. If you’re following along, you’ll need to give yours a\\ndifferent name.\\n$ docker buildx create --driver cloud nigelpoulton/ddd\\ncloud-nigelpoulton-ddd\\nOnce you have a cloud builder, you can either make it your default builder with a\\ndocker buildx use <builder> command, or you can specify it when performing\\nindividual builds.\\nThe following command uses the--builder flag to use thecloud-nigelpoulton-ddd\\ncloud builder to build the same images as in the previous steps. Remember to use your\\nown cloud builder if you’re following along.\\n$ docker buildx build \\\\\\n--builder=cloud-nigelpoulton-ddd \\\\\\n--platform=linux/amd64,linux/arm64 \\\\\\n-t nigelpoulton/ddd-book:ch8.1 --push .\\n=> [internal] connected to docker build cloud service 0.0s\\n<Snip>\\nAt the time of writing, Build Cloud supports various AMD and ARM architectures,\\nwhereas thedocker-container driver supports more but is slower and less reliable.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 122}, page_content='8: Containerizing an app 116\\nA few good practices\\nLet’s finish the chapter with a few best practices. This isn’t a full list, and the advice\\napplies to local and cloud builds.\\nLeverage the build cache\\nBuildKit uses a cache to speed up builds. The best way to see the impact is to build a\\nnew image on a clean Docker host and then repeat the same build immediately after.\\nThe first build will pull images and take time to build layers. The second build will\\ninstantly complete because the layers and other artifacts from the first build are cached\\nand leveraged by later builds.\\nIf you use a local builder, the cache is only available to other builds on the same system.\\nHowever, your entire team can share the cache on Docker Build Cloud.\\nFor each build, the builder iterates through the Dockerfile one line at a time, starting\\nfrom the top. For each line, it checks if it already has the layer in its cache. If it does,\\na cache hitoccurs, and it uses the cached layer. If it doesn’t, acache missoccurs, and it\\nbuilds a new layer from the instruction. Cache hits are one of the best ways to make\\nbuilds faster.\\nLet’s take a closer look.\\nAssume the following Dockerfile:\\nFROM alpine\\nRUN apk add --update nodejs npm\\nCOPY . /src\\nWORKDIR /src\\nRUN npm install\\nEXPOSE 8080\\nENTRYPOINT [\"node\", \"./app.js\"]\\nThe first instruction tells Docker to use thealpine:latest image as itsbase image. If\\nyou already have a copy of this image, the builder moves on to the next instruction. If\\nyou don’t have a copy, it pulls it from Docker Hub.\\nThe next instruction (RUN apk...) runs a command to update package lists and install\\nnodejs and npm. Before executing the instruction, Docker checks the build cache for a\\nlayer built from the same base image using the same instruction. In this case, it’s looking\\nfor a layer built by executing theRUN apk add --update nodejs npm instruction\\ndirectly on top of thealpine:latest image.\\nIf it finds a matching layer, it links to that layer and continues the build with the cache\\nintact. If it doesnot find a matching layer, it invalidates the cache and builds the layer.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 123}, page_content='8: Containerizing an app 117\\nInvalidating the cache means the builder has to execute all remaining Dockerfile\\ninstructions in full and cannot use the cache.\\nLet’s assume Docker had a cached layer for theRUN instruction and that the layer’s ID is\\nAAA.\\nThe next instruction runs aCOPY . /src command to copy the app code into the image.\\nThe previous instruction scored a cache hit, meaning Docker can check if it has a cached\\nlayer built by running aCOPY . /src against theAAA layer. If it has a cached layer for\\nthis, it links to the layer and proceeds to the next instruction. If it doesn’t have a cached\\nlayer, it builds it and invalidates the cache for the rest of the build.\\nThis process continues for the rest of the Dockerfile.\\nIt’s important to understand a few things.\\nAny time an instruction results in a cache miss, the cache is invalidated and no longer\\nchecked for the rest of the build. This means you should write your Dockerfiles so that\\ninstructions most likely to invalidate the cache go near the end of the Dockerfile. This\\nallows builds to benefit from the cache for as long as possible.\\nYou can force a build to ignore the cache by runningdocker build with the--no-cache\\noption.\\nIt’s also important to understand thatCOPY and ADD instructions include logic to ensure\\nthe content you’re copying into the image hasn’t changed since the last build. For exam-\\nple, you might have a cached layer that Docker built by running aCOPY . /src against\\nthe AAA image. However, if thefiles that theCOPY . /src instruction copies into the\\nlayer have changed since the cached layer was built, you cannot use the cached layer as\\nyou’d get old versions of the files. To protect against this, Docker performs checksums\\nagainst each file it copies. If the checksums don’t match, the cache is invalidated, and\\nDocker builds a new layer.\\nOnly install essential packages\\nWe often joke that weinstall the entire internetwhen we build apps. As a quick example,\\nthe simple Node.js app used earlier in the chapter depends on two packages:\\n• Express\\n• Pug\\nHowever, these packages depend on other packages, which in turn depend on others.\\nAt the time of writing, building this simple application with two dependencies actually\\ndownloads 97 packages!\\nFortunately, some package managers provide a way for you to only download and install\\nessential packages instead ofthe entire internet. One example is theapt package manager'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 124}, page_content='8: Containerizing an app 118\\nthat lets you specify theno-install-recommends flag so that it only installs packages\\nin thedepends field and not everyrecommended and suggested package. Each package\\nmanager does this differently, but it’s worth investigating as it can massively impact the\\nsize of your images.\\nClean up\\nIf you’ve followed along, you’ll have one running container and several images in your\\nlocal image repository. You should delete the running container and, optionally, the\\nlocal images.\\nRun the following command to delete the container.\\n$ docker rm c1 -f\\nOptionally delete the local images with the following command. Be sure to use the\\nnames of the images in your environment.\\n$ docker rmi \\\\\\nmulti:full multi:client multi:server ddd-book:ch8.node nigelpoulton/ddd-book:ch8.node\\nContainerizing an app – The commands\\n• docker build containerizes applications. It reads a Dockerfile and follows the\\ninstructions to create an OCI image. The-t flag tags the image, and the-f flag\\nlets you specify the name and location of the Dockerfile. Thebuild contextis where\\nyour application files exist and can be a directory on your local Docker host or a\\nremote Git repo.\\n• The DockerfileFROM instruction specifies the base image. It’s usually the first\\ninstruction in a Dockerfile, and it’s considered a good practice to build from\\nDocker Official Imagesor images fromVerified Publishers. FROM is also used to\\nidentify new build stages in multi-stage builds.\\n• The DockerfileRUN instruction lets you run commands during a build. It’s com-\\nmonly used to update packages and install dependencies. EveryRUN instruction\\ncreates a new image layer.\\n• The DockerfileCOPY instruction adds files to images, and you’ll regularly use it to\\ncopy your application code into a new image. EveryCOPY instruction creates an\\nimage layer.\\n• The DockerfileEXPOSE instruction documents an application’s network port.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 125}, page_content='8: Containerizing an app 119\\n• The DockerfileENTRYPOINT and CMD instructions tell Docker how to run the app\\nwhen starting a new container.\\n• Some other Dockerfile instructions includeLABEL, ENV, ONBUILD, HEALTHCHECK and\\nmore.\\nChapter summary\\nThis chapter taught you how to containerize an application. This is the process of\\nbuilding an app into a container image and running it as a container.\\nYou pulled some application source code from GitHub and used thedocker init\\ncommand to auto-generate a Dockerfile with instructions telling Docker how to build\\nthe app into a container image. You then useddocker build to create the image,docker\\npush to push it to Docker Hub, anddocker run to run it as a container.\\nAlong the way, you learned that Dockerfile instructions that add content to an image\\ncreate new layers, whereas instructions that don’t add content only add metadata.\\nAfter that, you learned how multi-stage builds allow you to create small and efficient\\nproduction images without the bloat carried over from compiling the app, etc.\\nAfter that, you learned that buildx is the default build client that integrates with the\\nlatest features of the BuildKit build engine. You learned how to create local and remote\\nbuilders (BuildKit instances) and how to use them to perform multi-architecture builds.\\nYou also learned the importance of the build cache for speeding up builds and how to\\noptimize Dockerfiles to leverage the build cache.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 126}, page_content='9: Multi-container apps with Compose\\nIn this chapter, you’ll deploy and manage a multi-container application using Docker\\nCompose. When we talk about Docker Compose, we usually shorten it toCompose and\\nalways write it with a capital “C”.\\nI’ve organized the chapter as follows:\\n• Docker Compose – The TLDR\\n• Compose background\\n• Installing Compose\\n• The sample app\\n• Compose files\\n• Deploying apps with Compose\\n• Managing apps with Compose\\nDocker Compose – The TLDR\\nWe create modern cloud-native applications by combining lots of small services that\\nwork together to form a useful app. We call themmicroservices applications, and they\\nbring a lot of benefits, such as self-healing, autoscaling, and rolling updates. However,\\nthey can be complex.\\nFor example, you might have a microservices app with the following services:\\n• Web front-end\\n• Ordering\\n• Catalog\\n• Back-end datastore\\n• Logging\\n• Authentication\\n• Authorization'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 127}, page_content='9: Multi-container apps with Compose 121\\nInstead of hacking together complex scripts and longdocker commands, Compose lets\\nyou describe the application in a simple YAML file called aCompose file. You then use the\\nCompose file with thedocker compose command to deploy and manage the app.\\nYou should keep your Compose files in a version control system such as Git.\\nThat’s the basics. Let’s dig deeper.\\nCompose background\\nWhen Docker was new, a company calledOrchard Labsbuilt a tool calledFig that made\\ndeploying and managing multi-container apps easy. It was a Python tool that ran on\\ntop of Docker and let you define complex multi-container microservices apps in a\\nsimple YAML file. You could even use thefig command-line tool to manage the entire\\napplication lifecycle.\\nBehind the scenes, Fig would read the YAML file and call the appropriate Docker\\ncommands to deploy and manage the app.\\nFig was so good that Docker, Inc. acquired Orchard Labs and rebranded Fig asDocker\\nCompose. They renamed the command-line tool fromfig to docker-compose, and then\\nmore recently, they folded it into thedocker CLI with its owncompose sub-command.\\nYou can now run simpledocker compose commands to easily manage multi-container\\nmicroservices apps.\\nThere is also aCompose Specification15 driving Compose as an open standard for\\ndefining multi-container microservices apps. The specification is community-led and\\nkept separate from the Docker implementation to maintain better governance and\\nclearer demarcation. However,Docker Composeis thereference implementation, and you\\nshould expect Docker to implement the full spec.\\nReading the spec is also a great way to learn the details.\\nInstalling Compose\\nAll modern versions of Docker come with Docker Compose pre-installed, and you no\\nlonger need to install it as a separate application.\\nTest it with the following command. Be sure to use thedocker compose command\\ninstead of the olderdocker-compose.\\n15https://www.compose-spec.io/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 128}, page_content=\"9: Multi-container apps with Compose 122\\n$ docker compose version\\nDocker Compose version v2.25.0\\nThe sample app\\nWe’ll use the sample app shown in Figure 9.1 with two services, a network, and a\\nvolume.\\nFigure9.1 - Sample app\\nThe web-fe service runs a web server that increments a counter in theredis service\\nevery time it receives a request for the web page. Both services are connected to the\\ncounter-net network and use it to communicate with each other. Theweb-fe service\\nmounts thecounter-vol volume.\\nThis is all defined in thecompose.yaml file in themulti-container folder of the book’s\\nGitHub repo.\\nIf you haven’t already done so, clone the repo so you have a local copy of everything\\nyou’ll need. You’ll needgit installed, and the command will create a new directory\\ncalled ddd-book.\\n$ git clone https://github.com/nigelpoulton/ddd-book.git\\nCloning into 'ddd-book'...\\nremote: Enumerating objects: 67, done.\\nremote: Counting objects: 100% (67/67), done.\\nremote: Compressing objects: 100% (47/47), done.\\nremote: Total 67 (delta 17), reused 63 (delta 16), pack-reused 0\\nReceiving objects: 100% (67/67), 173.61 KiB | 1.83 MiB/s, done.\\nResolving deltas: 100% (17/17), done.\\nChange into theddd-book/multi-container directory and list its contents.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 129}, page_content='9: Multi-container apps with Compose 123\\n$ cd ddd-book/multi-container/\\n$ ls -l\\ntotal 20\\ndrwxrwxr-x 4 ubuntu ubuntu 4096 May 21 15:53 app\\n-rw-rw-r-- 1 ubuntu ubuntu 288 May 21 15:53 Dockerfile\\n-rw-rw-r-- 1 ubuntu ubuntu 18 May 21 15:53 requirements.txt\\n-rw-rw-r-- 1 ubuntu ubuntu 355 May 21 15:53 compose.yaml\\n-rw-rw-r-- 1 ubuntu ubuntu 332 May 21 15:53 README.md\\nThis directory is yourbuild contextand contains all the app code and configuration files\\nneeded to deploy and manage the app.\\n• The app folder contains the application code, views, and templates\\n• The Dockerfile describes how to build the image for theweb-fe service\\n• The requirements.txt file lists the application dependencies\\n• The compose.yaml file is the Compose file that describes how the app works\\nFigure 9.2 is slightly more complex and shows how the files in your build context relate\\nto the app.\\nFigure 9.2 - Detailed view of sample app\\nWhen you deploy the app, you’ll use thedocker compose command to send the\\ncompose.yaml file to Docker. Docker will create thecounter-net network and the\\ncounter-vol volume and use the Dockerfile to build an OCI image for theweb-fe\\nservice. The Dockerfile tells Docker to copy the code from theapp folder into the image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 130}, page_content='9: Multi-container apps with Compose 124\\nDocker then starts theweb-fe, mounts thecounter-vol volume, and connects to the\\ncounter-net network. It also starts a container for theredis service and connects that\\nto the samecounter-net network.\\nDocker also uses the name of the build context directory to name the services, network,\\nand volume so that you can easily identify them.\\nNow that you know what the app looks like, let’s look at the Compose file.\\nCompose files\\nCompose uses YAML files to define microservices applications. We call themCompose\\nfiles, and Compose expects you to name themcompose.yaml or compose.yml. However,\\nyou can use the-f flag with thedocker compose command to specify files with\\narbitrary names.\\nHere is the Compose file we’ll be using. It’s calledcompose.yaml and is in themulti-\\ncontainer folder.\\nservices: <<---- Microservices are defined in the \"services\" block\\nweb-fe: <<---- This block defines the web front-end microservice\\ndeploy:\\nreplicas: 1\\nbuild: .\\ncommand: python app.py\\nports:\\n- target: 8080\\npublished: 5001\\nnetworks:\\n- counter-net\\nvolumes:\\n- type: volume\\nsource: counter-vol\\ntarget: /app\\nredis: <<---- This block defines the Redis back-end microservice\\ndeploy:\\nreplicas: 1\\nimage: \"redis:alpine\"\\nnetworks:\\ncounter-net:\\nnetworks: <<---- Networks are defined in this block\\ncounter-net:\\nvolumes: <<---- Volumes are defined in this block\\ncounter-vol:\\nThe first thing to note is that the file has three top-level keys with a block of code\\nbeneath each:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 131}, page_content='9: Multi-container apps with Compose 125\\n• services\\n• networks\\n• volumes\\nMore top-level keys exist, but this app only uses the three in the list.\\nLet’s have a closer look at each.\\nThe top-levelservices key is mandatory and is where you define application microser-\\nvices. This app has two microservices calledweb-fe and redis.\\nLet’s look at both, starting with theweb-fe service.\\nservices:\\nweb-fe: <<---- Service name. Containers will inherit this name\\ndeploy:\\nreplicas: 1 <<---- Deploy a single container for this service\\nbuild: . <<---- Build from the Dockerfile in the same directory\\ncommand: python app.py <<---- Execute this command when starting containers\\nports:\\n- target: 8080 <<---- Map port 8080 in the container...\\npublished: 5001 <<---- ...to port 5001 on the Docker host\\nnetworks:\\n- counter-net <<---- Attach containers to the \"counter-net\" network\\nvolumes:\\n- type: volume\\nsource: counter-vol <<---- Mount the \"counter-vol\" volume...\\ntarget: /app <<---- ...to \"/app\" in the containers for this service\\nLet’s step through it.\\n• web-fe: is the service’s name, and all containers created as part of this service will\\nhave “web-fe” in their names.\\n• deploy.replicas: 1 tells Docker to deploy a single container for this service. You\\ncan specify a different number of replicas to deploy multiple identical containers\\nfor the service. However, this won’t work on Docker Desktop installations as you\\nonly have a single Docker host, and only one container can use port5001 on the\\nDocker Desktop host. You’ll learn how to deploy multiple replicas for this service\\nin theDeploying apps with Docker stackschapter.\\n• build: . tells Docker to build the image for this service’s container from the\\nDockerfile in the current directory.\\n• command: python app.py is the command Docker will execute inside every\\ncontainer it creates for this service. Theapp.py file must exist in the image, and\\nthe image must have Python installed. The Dockerfile takes care of both of these\\nrequirements.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 132}, page_content='9: Multi-container apps with Compose 126\\n• ports: is where you map network ports from the service’s containers to the\\nDocker host. This example maps port5001 on the Docker host to port8080 inside\\nthe container.\\n• networks: tells Docker to attach this service’s containers to thecounter-net\\nnetwork. The network should already exist or be defined in thenetworks top-level\\nkey.\\n• volumes: tells Docker to mount thecounter-vol volume to the/app directory\\ninside all of the service’s containers. The volume should already exist or be defined\\nin thevolumes top-level key.\\nIn summary, Compose will instruct Docker to deploy a single container for theweb-fe\\nservice. It will build a new image from the Dockerfile in the same directory and start\\nthe container from that image. When it starts, the container will haveweb-fe in its\\nname and run thepython app.py command. It will attach to thecounter-net network,\\nexpose the web service on the host’s port5001, and mount thecounter-vol volume to\\n/app. If you defined multiple replicas (containers) they would all be identical.\\nNote: You don’t need to specify thecommand: python app.py option in the\\nCompose file as it’s already defined in the Dockerfile. However, I’ve shown\\nit here so you know how it works. You can also use Compose to override\\ninstructions set in Dockerfiles.\\nThe redis service is a lot simpler.\\nservices:\\n..\\nredis: <<---- Service name. Containers will inherit this name\\ndeploy:\\nreplicas: 1 <<---- Deploy a single container for this service\\nimage: \"redis:alpine\" <<---- Pull the \"redis:alpine\" image for this service\\nnetworks:\\ncounter-net: <<---- Attach containers to the \"counter-net\" network\\nLet’s step through this one.\\n• redis: is the service’s name, and all containers created as part of this service will\\ninherit “redis” as part of their names.\\n• deploy.replicas: 1 tells Docker to deploy a single container for this service.\\n• image: redis:alpine tells Docker to pull theredis:alpine image from Docker\\nHub and use it to start the service’s containers.\\n• networks: tells Docker to attach the service’s containers to thecounter-net\\nnetwork.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 133}, page_content='9: Multi-container apps with Compose 127\\nConnecting both services to thecounter-net network means they can resolve each\\nother by name and communicate. This is important, as the following extract from the\\napp.py file shows the web app communicating with theredis service by name.\\nimport time\\nimport redis\\nfrom flask import Flask, render_template\\napp = Flask(__name__)\\ncache = redis.Redis(host=\\'redis\\', port=6379) <<---- \"redis\" is the name of the service\\n<Snip>\\nThe network and volumes blocks are extremely simple and define a network called\\ncounter-net and a volume calledcounter-vol.\\nnetworks: <<---- This block defines a new network called \"counter-net\"\\ncounter-net:\\nvolumes: <<---- This block defines a new volume called \"counter-vol\"\\ncounter-vol:\\nNow that we understand how the Compose file works let’s deploy the app.\\nDeploying apps with Compose\\nIn this section, you’ll use the Compose file to deploy the app. You’ll need a local copy\\nof the book’s GitHub repo, and you’ll need to run all commands from themulti-\\ncontainer folder.\\nRun the following command to deploy the app. By default, it deploys the app defined in\\nthe compose.yaml file in the working directory.\\n$ docker compose up &\\n- redis 7 layers [||||||] 0B/0B Pulled 5.2s\\n- b0dd12c8e070: Pull complete\\n<Snip>\\n- 4f4fb700ef54: Pull complete\\n- redis Pulled\\n<Snip>\\n=> [web-fe internal] load build definition from Dockerfile\\n[+] Building 10.3s (9/9) FINISHED\\n<Snip>\\n[+] Running 4/4\\n- Network multi-container_counter-net Created 0.0s\\n- Volume \"multi-container_counter-vol\" Created 0.0s\\n- Container multi-container-redis-1 Created 0.1s\\n- Container multi-container-web-fe-1 Created 0.1s'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 134}, page_content='9: Multi-container apps with Compose 128\\nIt’ll take a few seconds to build and pull the images and then start the app. Once it’s\\nrunning, you’ll have to hitReturn to reclaim your shell prompt.\\nWe’ll review what happened in a second, but let’s talk about thedocker compose\\ncommand first.\\nRunning adocker compose up command is the most common way to deploy a\\nCompose app. It reads through the Compose file in the local directory and runs the\\nDocker commands required to deploy the app. This includes building and pulling all\\nthe necessary images, creating all required networks and volumes, and starting all\\ncontainers.\\nThe command you executed didn’t specify the name or location of the Compose file, so\\nDocker assumed it was calledcompose.yaml in the local directory. However, you can use\\nthe -f flag to point to a Compose file with a different name in a different directory. For\\nexample, the following command will deploy the application defined in a Compose file\\ncalled sample-app.yml in theapps/ddd-book directory.\\n$ docker compose -f apps/ddd-book/sample-app.yml up &\\nYou’ll normally use the--detach flag to bring the app up in the background, and we’ll\\nsee that later. However, bringing it up in the foreground as we did will print informative\\nmessages to your terminal that we’ll refer to later.\\nNow that you’ve deployed the app, you can run regulardocker commands to see\\nthe images, containers, networks, and volumes that Compose created — remember,\\nCompose is building and working with regular Docker constructs behind the scenes.\\nRun the following command to see the imagecreated for theweb-fe service and the\\nimage pulled for theredis service.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nredis alpine 7635b0bfdd7d 2 months ago 61.4MB\\nmulti-container-web-fe latest f8fc9490e335 24 minutes ago 113MB\\nDocker pulled theredis:alpine image from Docker Hub, but it used the Dockerfile to\\nbuild themulti-container-web-fe:latest image.\\nIf you look at the Dockerfile, you’ll see it pulls thepython:alpine image, copies in the\\napp code, installs requirements, and sets the command to start the app.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 135}, page_content='9: Multi-container apps with Compose 129\\nFROM python:alpine <<---- Base image\\nCOPY . /app <<---- Copy app code into image\\nWORKDIR /app <<---- Set working directory\\nRUN pip install -r requirements.txt <<---- Install requirements\\nENTRYPOINT [\"python\", \"app.py\"] <<---- Set the default app\\nNotice how the newly built image’s name is a combination of the project name and the\\nservice name. The project name is the name of the build context directory, which in our\\nexample ismulti-container, and the service name isweb-fe. Compose uses this format\\nto name all resources, and the following table shows the names it will give the resources\\nfor our sample app.\\nResource type Resource Name\\nService web-fe multi-container-web-fe-1\\nService redis multi-container-redis-1\\nNetwork counter-net multi-container_counter-net\\nVolume counter-vol multi-container_counter-vol\\nList running containers to see the containers Compose created for the app.\\n$ docker ps\\nID COMMAND STATUS PORTS NAMES\\n61.. \"python app/app.py\" Up 35 mins 0.0.0.0:5001->8080/tcp.. multi-container-web-fe-1\\n80.. \"docker-entrypoint..\" Up 35 mins 6379/tcp multi-container-redis-1\\nAs you can see, themulti-container-web-fe-1 container is running the Python web\\napp and is mapped to port5001 on all interfaces on the Docker host. We’ll connect to\\nthis later.\\nThe number at the end of the container names allows each service to have multiple\\nreplicas. For example, if theweb-fe service had three replicas they would be called\\nmulti-container-web-fe-1, multi-container-web-fe-2, andmulti-container-web-\\nfe-3.\\nRun the following commands to see thecounter-net network andcounter-vol\\nvolume.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 136}, page_content='9: Multi-container apps with Compose 130\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\n46100cae7441 multi-container_counter-net bridge local\\n<Snip>\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal multi-container_counter-vol\\n<Snip>\\nWith the application deployed, you can point a web browser at your Docker host on port\\n5001 to view it. You can connect tolocalhost:5001 if you’re running Docker Desktop.\\nRefreshing the page will cause the counter to increment. This is because the app counts\\npage hits and stores the counter in the Redis service.\\nIf you followed along and brought the application up in the foreground, you’ll see HTTP\\nresponse codes in your terminal for every page refresh. You won’t see this if you used\\nthe --detach flag to bring it up in the background.\\nweb-fe-1 | 192.168.65.1 - [31/Mar/2024 19:15:33] \"GET / HTTP/1.1\" 200 -\\nweb-fe-1 | 192.168.65.1 - [31/Mar/2024 19:15:33] \"GET /static/images/image.png HTTP/1.1\" 304\\nweb-fe-1 | 192.168.65.1 - [31/Mar/2024 19:15:33] \"GET /static/css/main.css HTTP/1.1\" 304\\nCongratulations. You’ve successfully deployed a multi-container application using\\nDocker Compose!\\nManaging apps with Compose\\nIn this section, you’ll see how to stop, restart, delete, and get the status of Compose apps.\\nRun the following command to shut down the app. If you started it in the foreground,\\nyou’ll see messages requesting the app to gracefully quit.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 137}, page_content='9: Multi-container apps with Compose 131\\n$ docker compose down\\nredis-1 | 1:signal-handler (1711912860) Received SIGTERM scheduling shutdown...\\nredis-1 | 1:M 31 Mar 2024 19:21:00.406 * User requested shutdown...\\nredis-1 | 1:M 31 Mar 2024 19:21:00.406 * Saving the final RDB snapshot before exiting.\\nredis-1 | 1:M 31 Mar 2024 19:21:00.409 * DB saved on disk\\nredis-1 | 1:M 31 Mar 2024 19:21:00.409 # Redis is now ready to exit, bye bye...\\n<Snip>\\n- Container multi-container-redis-1 Removed 0.1s\\n- Container multi-container-web-fe-1 Removed 0.2s\\n- Container multi-container-redis-1 Removed 0.1s\\n- Network multi-container_counter-net Removed 0.1s\\nThe output shows Docker removing both containers and the network. However, it\\ndoesn’t mention the volume.\\nRun adocker volumes ls command to see if the volume still exists.\\n$ docker volume ls\\n<Snip>\\nlocal multi-container_counter-vol\\nThe volume still exists, including the counter data stored in it. This is because Docker\\nknows that we store important information in volumes and might not want to delete\\nthem with other application resources. With this in mind, Docker decouples the lifecycle\\nof volumes from the rest of the application.\\nDocker also keeps the images it built and pulled when it started the app. This makes\\nfuture deployments faster.\\nFeel free to run adocker images command to verify the images still exist.\\nLet’s explore a few otherdocker compose sub-commands.\\nRun the following command to redeploy the app.\\n$ docker compose up --detach\\n<Snip>\\n[+] Running 2/3\\n- Network multi-container_counter-net Created 0.2s\\n- Container multi-container-redis-1 Started 0.2s\\n- Container multi-container-web-fe-1 Started 0.2s\\nNotice how it started much faster this time. This is because the volume and both images\\nalready exist.\\nCheck the current state of the app with thedocker compose ps command.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 138}, page_content='9: Multi-container apps with Compose 132\\n$ docker compose ps\\nNAME COMMAND SERVICE STATUS PORTS\\nmulti-container-redis-1 \"docker-entrypoint..\" redis Up 33 sec 6379/tcp\\nmulti-container-web-fe-1 \"python app/app.py\" web-fe Up 33 sec 0.0.0.0:5001->8080\\nThe output shows both containers, the commands they’re executing, their current state,\\nand the network ports they’re listening on.\\nRun adocker compose top to list the processes inside each container.\\n$ docker compose top\\nmulti-container-redis-1\\nUID PID PPID ... CMD\\nlxd 12023 11980 redis-server *:6379\\nmulti-container-web-fe-1\\nUID PID PPID ... CMD\\nroot 12024 12002 0 python app/app.py python app.py\\nroot 12085 12024 0 /usr/local/bin/python app/app.py python app.py\\nThe PID numbers returned are the PID numbers as seen from the Docker host (not from\\nwithin the containers).\\nYou’re about to stop and restart the app. However, before doing this, connect your\\nbrowser to the app again and refresh the page a few times. Make a note of the counter\\nvalue before continuing.\\nRun the following commands to stop the app and recheck its status.\\n$ docker compose stop\\n[+] Running 2/2\\n- Container multi-container-redis-1 Stopped 0.4s\\n- Container multi-container-web-fe-1 Stopped 0.5\\n$ docker compose ps\\nNAME COMMAND SERVICE STATUS PORTS\\nThe app is down, but Docker hasn’t deleted any of its resources. Verify this by checking\\nif the containers still exist.\\n$ docker compose ps -a\\nNAME IMAGE ... SERVICE STATUS\\nmulti-container-redis-1 redis:alpine ... redis Exited (0) 18 seconds ago\\nmulti-container-web-fe-1 multi-container-web-fe ... web-fe Exited (0) 18 seconds ago\\nBoth containers still exist in theExited state.\\nRestart the app with thedocker compose restart command.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 139}, page_content='9: Multi-container apps with Compose 133\\n$ docker compose restart\\n[+] Running 2/2\\n- Container multi-container-redis-1 Started 0.4s\\n- Container multi-container-web-fe-1 Started 0.5s\\nVerify the operation worked.\\n$ docker compose ls\\nNAME STATUS CONFIG FILES\\nmulti-container running(2) /Users/nigelpoulton/temp/ddd-book/multi-container/compose.yaml\\nGo back to your browser and refresh the page again. Notice how the counter picks up\\nfrom where you left it. This is because the Redis container persisted the count value\\nacross the stop and restart operations.\\nCongratulations. You’ve deployed and managed a multi-container microservices app\\nusing Docker Compose.\\nBefore cleaning up and reviewing the commands, it’s important to understand that\\nthis was a simple example and that Docker Compose can deploy and manage far more\\ncomplex applications.\\nClean up\\nRun the following command tostop and deletethe app. The--volumes flag will delete\\nall of the app’s volumes, and the--rmi all will delete all of its images.\\n$ docker-compose down --volumes --rmi all\\n- Container multi-container-web-fe-1 Removed 0.2s\\n- Container multi-container-redis-1 Removed 0.1s\\n- Volume multi-container_counter-vol Removed 0.0s\\n- Image multi-container-web-fe:latest Removed 0.1s\\n- Image redis:alpine Removed 0.1s\\n- Network multi-container_counter-net Removed 0.1s\\nDeploying apps with Compose – The commands\\n• docker compose up is the command to deploy a Compose app. It creates all\\nimages, containers, networks, and volumes the app needs. It expects you to call\\nthe Compose filecompose.yaml, but you can specify a custom filename with the-f\\nflag. You’ll normally start the app in the background with the--detach flag.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 140}, page_content='9: Multi-container apps with Compose 134\\n• docker compose stop will stop all containers in a Compose app without deleting\\nthem from the system. You can easily restart them withdocker compose restart,\\nand you shouldn’t lose any data.\\n• docker compose restart will restart a stopped Compose app. If you make\\nchanges to the Compose file while it’s stopped, these changes willnot appear in\\nthe restarted app. You need to redeploy the app to see any changes you made in the\\nCompose file.\\n• docker compose ps lists each container in the Compose app. It shows the current\\nstate, the command each container is running, and network ports.\\n• docker compose down will stop and delete a running Compose app. By default, it\\ndeletes containers and networks but not volumes and images.\\nChapter Summary\\nIn this chapter, you learned how to deploy and manage multi-container applications\\nusing Docker Compose.\\nCompose is now fully integrated into the Docker Engine and has its owndocker\\ncompose sub-command. It lets you define multi-container applications in declarative\\nconfiguration files and deploy them with a single command.\\nCompose files define all the containers, networks, volumes, secrets, and other configu-\\nrations an application needs. You then use thedocker compose command to post the\\nCompose file to Docker, and Docker deploys it.\\nOnce you’ve deployed the app, you can manage its entire lifecycle usingdocker\\ncompose sub-commands.\\nDocker Compose is popular with developers, and the Compose file is an excellent\\nsource of application documentation — it defines all the services that make up the app,\\nthe images they use, the ports they expose, the networks and volumes they use, and\\nmuch more. As such, it can help bridge the gap between development and operations\\nteams. You should also treat Compose files as code and store them in version control\\nsystems.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 141}, page_content='10: Docker Swarm\\nNow that you know how to install Docker, pull images, and work with containers, the\\nnext logical step is to do it all at scale. That’s where Docker Swarm comes into play.\\nI’ve split the chapter into the following main parts:\\n• Docker Swarm – The TLDR\\n• Swarm primer\\n• Build a secure Swarm cluster\\n• Deploy and manage an app on swarm\\n• Service logs\\n• The commands\\nDocker Swarm – The TLDR\\nDocker Swarm is two things:\\n1. An enterprise-grade cluster of Docker nodes\\n2. An orchestrator of microservices apps\\nOn theclustering front, Swarm groups one or more Docker nodes into a cluster. Out\\nof the box, you get an encrypted distributed cluster store, encrypted networks, mutual\\nTLS, secure cluster join tokens, and a PKI that makes managing and rotating certificates\\na breeze. You can even add and remove nodes non-disruptively. We call these clusters\\n“swarms”.\\nOn theorchestration front, Swarm makes deploying and managing complex microser-\\nvices apps easy. You define applications declaratively in Compose files and use simple\\nDocker commands to deploy them to theswarm. You can even perform rolling updates,\\nrollbacks, and scaling operations.\\nDocker Swarm is similar to Kubernetes — both are clusters that run and manage con-\\ntainerized applications. Kubernetes is more popular and has a more active community\\nand ecosystem. However, Swarm is easier to use and is a popular choice for small-to-\\nmedium businesses and smaller application deployments. Learning Swarm also prepares\\nyou to learn and work with Kubernetes. In fact, if you plan on learning Kubernetes, you\\nshould check out my books,Quick Start Kubernetesand The Kubernetes Book.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 142}, page_content='10: Docker Swarm 136\\nSwarm primer\\nOn the clustering front, aswarm is one or more Docker nodes that can be physical\\nservers, VMs, cloud instances, Raspberry Pi’s, and more. The only requirement is that\\nthey all run Docker and can communicate over reliable networks.\\nTerminology: When referring to theDocker Swarmtechnology, we’ll write\\nSwarm with an uppercase “S”. When referring to a swarm as a cluster of\\nnodes, we’ll use a lowercase “s”.\\nEvery node in a swarm is either amanager or aworker:\\n• Managers run thecontrol planeservices that maintain the state of the cluster and\\nschedule user applications to workers\\n• Workers run user applications\\nSwarm runs user applications on managersand workers in a default installation.\\nHowever, you can force user applications to run on worker nodes on important clusters,\\nallowing your managers to focus on cluster management operations.\\nThe swarm stores its state and configuration in an in-memory distributed database that\\nreplicates across all manager nodes. Fortunately, it’s automatically configured when you\\ncreate the swarm and takes care of itself.\\nSwarm uses TLS to encrypt communications, authenticate nodes, and authorize roles\\n(managers and workers). It also configures and performs automatic key rotation. And\\nas with the cluster store, it’s automatically configured when you create the swarm and\\ntakes care of itself.\\nFigure 10.1 shows a high-level view of a swarm with three managers, three workers, and\\na distributed cluster store replicated across all three managers.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 143}, page_content='10: Docker Swarm 137\\nFigure 10.1 High-level swarm\\nOn the orchestration front, Swarm makes it easy to deploy and manage containerized\\napplications. It balances workloads across the cluster and adds cloud-native capabilities\\nsuch as self-healing, scaling, rollouts, and rollbacks.\\nThat’s enough of a primer. Let’s build a swarm.\\nBuild a secure swarm cluster\\nIn this section, you’ll build a secure swarm cluster. I’ll build the cluster shown in Figure\\n10.2 with threemanagers and twoworkers. You can build a different cluster, but the\\nfollowing rules usually apply:\\n• Three or five managers are recommended for high-availability\\n• Enough workers to handle your application requirements'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 144}, page_content='10: Docker Swarm 138\\nFigure 10.2 - Sample lab environment\\nPre-reqs\\nIf you’re following along, I recommend using Multipass to create five Docker VMs on\\nyour laptop or local machine. Multipass is free and easy to use, and I’ll use it for the\\nexamples.\\nHowever, running five Multipass VMs requires a lot of system resources as each VM\\nneeds 2 CPUs, 40GB of disk space, and 4GB of memory. If your system can’t handle this,\\nyou can create fewer managers and workers or build your lab inPlay with Docker16.\\nPlay with Docker (PWD) is a free cloud-based service that lets you create up to five\\nnodes in a 4-hour Docker playground. Other Docker environments will also work. All\\nyou need is one or more Docker nodes that can communicate over a reliable network.\\nYou’ll also find it easier if you configure name resolution so you can reference nodes by\\nname rather than IP address.\\nI don’t recommend following along on Docker Desktop as it only supports a single node,\\nwhich isn’t enough for some of the examples later in the book.\\nBuild your Docker nodes\\nThis section uses Multipass to build the swarm in Figure 10.2.\\nSearch the web forinstall multipassand follow the instructions to install it on your\\nsystem.\\n16https://labs.play-with-docker.com/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 145}, page_content='10: Docker Swarm 139\\nRun the following fivemultipass launch commands to create five VMs running\\nDocker. We’ll refer to each VM as anode, and each will require 2 CPUs, 40GB of disk\\nspace, and 4GB of RAM. If your machine doesn’t have those resources, go to Play with\\nDocker and build your lab there.\\nName three of the nodes asmgr1, mgr2, andmgr3, and name the other twowrk1 and\\nwrk2. Yours will get different IPs, but that doesn’t matter. It can take a minute or two\\nto create each VM.\\n$ multipass launch docker --name mgr1\\nLaunched: mgr1\\n$ multipass launch docker --name mgr2\\nLaunched: mgr2\\n$ multipass launch docker --name mgr3\\nLaunched: mgr3\\n$ multipass launch docker --name wrk1\\nLaunched: wrk1\\n$ multipass launch docker --name wrk2\\nLaunched: wrk2\\nRun the following command and note the node IP addresses. Yours will have different\\naddresses, and you’re only interested in the192.168.x.x address for each.\\n$ multipass ls\\nName State IPv4 Image\\nmgr1 Running 192.168.64.61 Ubuntu 22.04 LTS\\n172.17.0.1\\nmgr2 Running 192.168.64.62 Ubuntu 22.04 LTS\\n172.17.0.1\\nmgr3 Running 192.168.64.63 Ubuntu 22.04 LTS\\n172.17.0.1\\nwrk1 Running 192.168.64.64 Ubuntu 22.04 LTS\\n172.17.0.1\\nwrk2 Running 192.168.64.65 Ubuntu 22.04 LTS\\n172.17.0.1\\nOnce you’ve built the nodes, you can log on to them with themultipass shell\\ncommand, and you can log out by typingexit.\\nLog on to one of the nodes and check that Docker is installed and running.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 146}, page_content='10: Docker Swarm 140\\n$ multipass shell mgr1\\n$ docker --version\\nDocker version 26.1.1, build 2ae903e\\nType exit to log out of the node and return to your local shell.\\nOnce you’ve confirmed your nodes are working, you can move to the next section and\\nbuild a swarm.\\nInitializing a new swarm\\nThe process of building a swarm is calledinitializing a swarm, and this is the high-level\\nprocess:\\n1. Initialize the first manager\\n2. Join additional managers\\n3. Join workers.\\nBefore a Docker node joins a swarm, it runs insingle-engine modeand can only run\\nregular containers. After joining a swarm, it switches intoswarm modeand can run\\nadvanced containers calledswarm services. More onservices later.\\nWhen you run adocker swarm init command on a Docker node that’s currently in\\nsingle-engine mode, Docker switches it intoswarm mode, creates a newswarm, and makes\\nthe node the firstmanager of the swarm. You can then add more nodes as managers and\\nworkers, and these nodes also get switched into swarm mode.\\nYou’re about to complete all of the following:\\n1. Initialize a new swarm frommgr1\\n2. Join wrk1 and wrk2 as worker nodes\\n3. Join mgr2 and mgr3 as additional managers\\nAfter completing the procedure, all five nodes will be in swarm mode and part of the\\nsame swarm.\\nThe examples will use the names and IPs from Figure 10.2. Yours will be different, and\\nyou should use your own.\\n1. Log on tomgr1 and initialize a new swarm. Be sure to use the private IP address of\\nyour mgr1 node.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 147}, page_content='10: Docker Swarm 141\\n$ docker swarm init \\\\\\n--advertise-addr 192.168.64.61:2377 \\\\\\n--listen-addr 192.168.64.61:2377\\nSwarm initialized: current node (d21lyz...c79qzkx) is now a manager.\\n<Snip>\\nLet’s step through the different parts of the command:\\n• docker swarm init tells Docker to initialize a new swarm with this node as\\nthe first manager.\\n• The --advertise-addr flag is optional and tells Docker which of the node’s\\nIP addresses to advertise as the swarm API endpoint. It’s usually one of the\\nnode’s IP addresses but can also be an external load balancer.\\n• The --listen-addr flag tells Docker which of the node’s interfaces to accept\\nswarm traffic on. It defaults to the same value as--advertise-addr if you\\ndon’t specify it. However, if--advertise-addr is a load balancer, you must\\nuse --listen-addr to specify a local IP.\\nI recommend specifying both flags in important production environments.\\nHowever, for most lab environments, you can just run adocker swarm init\\nwithout any flags.\\nYou can also specify a different port for the advertise and listen addresses, but\\nusing the default2377 is common.\\n2. List the nodes in the swarm.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\nd21...qzkx * mgr1 Ready Active Leader\\nmgr1 is the only node in the swarm and is listed as theLeader. We’ll talk about\\nleaders later.\\n3. Run the following command frommgr1 to see the commands and tokens needed\\nto add new workers and managers.\\n$ docker swarm join-token worker\\nTo add a manager to this swarm, run the following command:\\ndocker swarm join --token SWMTKN-1-0uahebax...c87tu8dx2c 192.168.64.61:2377\\n$ docker swarm join-token manager\\nTo add a manager to this swarm, run the following command:\\ndocker swarm join --token SWMTKN-1-0uahebax...ue4hv6ps3p 192.168.64.61:2377\\nNotice how the commands are identical except for the join tokens (SWMTKN...).\\nYou should keep your join tokens in a safe place, as they’re all that’s required to\\njoin other nodes to your swarm!'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 148}, page_content='10: Docker Swarm 142\\n4. Log on towrk1 and use the correct command and token to join it as a worker\\nnode. I’ve added the optionaladvertise-addr and listen-addr flags in the\\nexample so you know how to use them. You can leave them off, but if you do use\\nthem, be sure to use the correct IPs for your environment.\\n$ docker swarm join \\\\\\n--token SWMTKN-1-0uahebax...c87tu8dx2c \\\\\\n10.0.0.1:2377 \\\\\\n--advertise-addr 192.168.64.64:2377 \\\\\\n--listen-addr 192.168.64.64:2377\\nThis node joined a swarm as a worker.\\nIf you experience joining problems, make sure the following network ports are\\nopen between all nodes:\\n• 2377/tcp: for secure client-to-swarm communication\\n• 7946/tcp and udp: for control plane gossip\\n• 4789/udp: for VXLAN-based overlay networks\\n5. Repeat the previous step onwrk2 so that you have two workers. If you specify\\nthe --advertise-addr and --listen-addr flags, make sure you usewrk2’s IP\\naddresses.\\n6. Log on tomgr2 and magr3 and join them as managers using themanager token\\nfrom step 3. Again, you don’t have to use the--advertise-addr and --listen-\\naddr flags, but if you do, be sure to use the correct IPs for your environment.\\n$ docker swarm join \\\\\\n--token SWMTKN-1-0uahebax...ue4hv6ps3p \\\\\\n10.0.0.1:2377 \\\\\\n--advertise-addr 192.168.64.62:2377 \\\\\\n--listen-addr 192.168.64.62:2377\\nThis node joined a swarm as a manager.\\n7. List the nodes in your swarm by runningdocker node ls from any of the\\nmanager nodes.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\\n0g4rl...babl8 * mgr2 Ready Active Reachable 26.1.1\\n2xlti...l0nyp mgr3 Ready Active Reachable 26.1.1\\nd21ly...9qzkx mgr1 Ready Active Leader 26.1.1\\n8yv0b...wmr67 wrk1 Ready Active 26.1.1\\ne62gf...l5wt6 wrk2 Ready Active 26.1.1'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 149}, page_content='10: Docker Swarm 143\\nIf you look closely at theMANAGER STATUS column, you’ll see all three managers are\\nshowing as eitherReachable or Leader. The nodes with nothing in this column are\\nworker nodes. The manager with the asterisk after its name is the one you’re executing\\ncommands from.\\nCongratulations. You’ve created a five-node swarm with three managers and two\\nworkers. The Docker Engine on each node is now operating in swarm mode, and the\\nswarm is secured with TLS.\\nSwarm manager high availability\\nSwarm clusters are highly available (HA), meaning one or more managers can fail, and\\nthe swarm will keep running.\\nTechnically speaking, Swarm implementsactive/passive multi-manager HA. This means\\na swarm with three managers will have one active manager, and the other two will be\\npassive. In a swarm, we call the active manager theleader and the passive managers\\nfollowers, and the leader is the only manager that can update the swarm configuration.\\nIf the leader fails, one of the followers will be elected as the new leader and the swarm\\nwill keep running without any service interruption. If you send commands to a follower,\\nit proxies them to the leader.\\nFigure 10.3 shows you issuing a command to a follower manager requesting an update\\nto the swarm. Step 1 shows you issuing the command. Step 2 shows the follower\\nmanager receiving and proxying the command to the leader. Step 3 shows the leader\\nexecuting it.\\nFigure 10.3\\nLeader and follower is Raft terminology, and we use it because Swarm implements the\\nRaft consensus algorithm17 to maintain a consistent cluster state across multiple highly-\\navailable managers.\\n17https://raft.github.io/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 150}, page_content='10: Docker Swarm 144\\nThe following good practices apply when it comes to manager HA:\\n1. Always deploy an odd number of managers\\n2. Don’t deploy too many managers (3 or 5 is usually enough)\\n3. Spread managers across availability zones\\nConsider the two examples shown in Figure 10.4. The swarm on the left has an even\\nnumber of managers, and a network incident has created a network partition with two\\nmanagers on either side. We call this asplit brainbecause neither side can be sure it has\\na majority, and the cluster goes into read-only mode. When this happens, your apps\\ncontinue working but you can’t make changes to them or to the cluster. However, the\\nswarm on the right has anodd number of managersand remains fully operational in\\nread-write mode because the two managers on the right side of the network partition\\nknow they have a majority (quorum). So, even though the swarm on the right has fewer\\nmanagers than the one on the left, it has better availability.\\nFigure 10.4 - Swarm high availability\\nAs with all consensus algorithms, more participants means longer times to achieve\\nconsensus. It’s like choosing where to eat — it’s always quicker and easier for three\\npeople to decide than it is for 33! With this in mind, it’s usually a best practice to have\\nthree or five managers for HA. Seven might work, but three or five usually right for\\nmost swarms.\\nA final word of caution regarding manager HA. While you should definitely spread\\nyour managers across availability zones, they need to be connected by fast and reliable\\nnetworks. This means you should think carefully and do lots of testing before imple-\\nmenting multi-cloud clusters implementing multi-cloud HA.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 151}, page_content='10: Docker Swarm 145\\nBuilt-in Swarm security\\nSwarm ships with a lot of security features, such as a built-in certificate authority (CA),\\nmutual TLS, an encrypted cluster store, encrypted networks, cryptographic node IDs\\nand join tokens, and more. Fortunately, Swarm automatically configures them with\\nsensible defaults.\\nLocking a Swarm\\nDespite all of Swarm’s security features, restarting an old manager or restoring an old\\nbackup can potentially compromise your cluster. For example, a malicious actor with\\naccess to an old manager node may be able to re-join it to the swarm and gain unwanted\\naccess.\\nFortunately, you can use Swarm’sautolock feature to force restarted managers to present\\na key before being admitted back into the swarm.\\nYou can autolock new swarms at build time by passing the--autolock flag to the\\ndocker swarm init command. For existing swarms, you can autolock them with the\\ndocker swarm update command.\\nRun the following command from one of your swarm managers to lock your existing\\nswarm cluster.\\n$ docker swarm update --autolock=true\\nSwarm updated.\\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock` command and\\nprovide the following key:\\nSWMKEY-1-XDeU3XC75Ku7rvGXixJ0V7evhDJGvIAvq0D8VuEAEaw\\nPlease remember to store this key in a password manager...\\nWhile it’s possible to run adocker swarm unlock-key command on any manager to\\nview your unlock key, you should also keep a secure copy outside of your cluster in case\\nyou’re ever locked out of your swarm.\\nRestart Docker on one of your managers to see if it automatically re-joins the cluster.\\n$ sudo systemctl restart docker\\nTry to list the nodes in the swarm.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 152}, page_content='10: Docker Swarm 146\\n$ docker node ls\\nError response from daemon: Swarm is encrypted and needs to be unlocked before it can be used.\\nDocker has restarted, but the manager hasn’t been allowed to re-join the swarm. You\\ncan prove this by running adocker node ls from one of your other managers and the\\nrestarted manager will be listed asdown and unreachable.\\nRun thedocker swarm unlock command to unlock the swarm for the restarted\\nmanager and provide the unlock key.\\n$ docker swarm unlock\\nPlease enter unlock key: <enter your key>\\nDocker will re-admit the node to the swarm, and it will show asready and reachable in\\nfuture commands.\\nYou should lock your production swarms and protect the unlock keys.\\nDedicated manager nodes\\nBy default, Swarm runs user applications on workersand managers. However, you may\\nwant to configure your production swarm clusters to only run user applications on\\nworkers. This allows managers to focus exclusively on control-plane duties.\\nRun the following command on any manager to prevent it from running user applica-\\ntions. You’ll need to use the names of your managers, and you’ll need to run it on all\\nmanagers if you want to force user applications to run only on workers.\\n$ docker node update --availability drain mgr1\\nYou’ll see the impact of this in later steps when you deploy services with multiple\\nreplicas.\\nNow that you’ve built a swarm and understand the concepts ofleaders and manager HA,\\nlet’s move on to the application side of things.\\nDeploy and manage an app on swarm\\nSwarm nodes can run regular containers, but they can also run enhanced containers\\ncalled services. Eachservice takes a single container definition and augments it with\\ncloud-native features such as self-healing, scaling, and automated rollouts and rollbacks.\\nIn this section, you’ll deploy a simple web server as aswarm serviceand see how to scale\\nit, perform rollouts, and delete it.\\nSwarm lets you create and manage services in two ways:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 153}, page_content='10: Docker Swarm 147\\n1. Imperatively on the command line\\n2. Declaratively with a Compose file\\nWe’ll look at Compose files in the next chapter. For now, we’ll focus on the imperative\\nmethod.\\nRun the following command to deploy a new service calledweb-fe with five identical\\nreplicas.\\n$ docker service create --name web-fe \\\\\\n-p 8080:8080 \\\\\\n--replicas 5 \\\\\\nnigelpoulton/ddd-book:web0.1\\nz7ovearqmruwk0u2vc5o7ql0p\\noverall progress: 5 out of 5 tasks\\n1/5: running [==================================================>]\\n2/5: running [==================================================>]\\n3/5: running [==================================================>]\\n4/5: running [==================================================>]\\n5/5: running [==================================================>]\\nverify: Service converged\\nThe docker service create command tells Docker to deploy a new service. The--\\nname flag calls this oneweb-fe, the-p flag maps port8080 on every swarm node to8080\\ninside of each service replica (container), and the--replicas flag tells Swarm to deploy\\nfive replicas of the service. The last line tells Swarm which image to build the replicas\\nfrom.\\nTerminology: We use the termsreplicas and containers interchangeably when\\nreferring to services. For example, a service defining fivereplicas will deploy\\nfive identicalcontainers.\\nYour Docker client sent the command to a swarm manager, and theleader manager\\ninitiated the work to deploy the five replicas across theswarm. You configured this\\nswarm so that user applications won’t run on managers, meaning all five replicas will\\nbe running on your worker nodes. Each worker node that received a work task pulled\\nthe image, started the required containers, and created the port mappings.\\nHowever, it doesn’t end there. The command you issued requested a new service with\\nfive replicas, and Swarm stored this in the cluster store as yourdesired state. In the\\nbackground, Swarm runs areconciliation loopthat constantly compares theobserved state\\nof the cluster with thedesired state. When the two states match, the world is a happy\\nplace, and no further action is needed. When they don’t match, Swarm takes action to\\nbring theobserved stateinto line withdesired state.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 154}, page_content='10: Docker Swarm 148\\nFor example, if aworker hosting one of the five replicas fails, theobserved stateof the\\ncluster will drop from 5 replicas to 4 and no longer match thedesired stateof 5. As soon\\nas the swarm observes the difference, it will start a new replica to bring theobserved state\\nback in line withdesired state. We call thisreconciliation or self-healing, and it’s a key tenet\\nof cloud-native applications.\\nYou’ll soon learn that desired state and reconciliation are also at the heart of scaling,\\nrollouts, and rollbacks.\\nViewing and inspecting services\\nRun adocker service ls command to see a list of all the services on your swarm.\\n$ docker service ls\\nID NAME MODE REPLICAS IMAGE PORTS\\nz7o...uw web-fe replicated 5/5 nigelpoulton/ddd... *:8080->8080/tcp\\nThe output shows a single service along with some basic config and state info. Among\\nother things, you can see the name of the service and that all five replicas are running.\\nSome of the replicas might not be in the running state if you run the command too soon\\nafter deploying the service. This is usually because the workers are still pulling the image\\nand starting the replicas.\\nYou can run adocker service ps command to see a list of service replicas and the state\\nof each.\\n$ docker service ps web-fe\\nID NAME IMAGE NODE DESIRED CURRENT\\n817...f6z web-fe.1 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 2 mins\\na1d...mzn web-fe.2 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 2 mins\\ncc0...ar0 web-fe.3 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 2 mins\\n6f0...azu web-fe.4 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 2 mins\\ndyl...p3e web-fe.5 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 2 mins\\nThe output displays each replica on its own line, including the node it’s running on\\nand each replica’s desired state and current observed state. As expected, you have five\\nreplicas balanced across all of your workers.\\nIf you need detailed information about a service, you can run thedocker service\\ninspect command.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 155}, page_content='10: Docker Swarm 149\\n$ docker service inspect --pretty web-fe\\nID: z7ovearqmruwk0u2vc5o7ql0p\\nName: web-fe\\nService Mode: Replicated\\nReplicas: 5\\nPlacement:\\nUpdateConfig:\\nParallelism: 1\\nOn failure: pause\\nMonitoring Period: 5s\\nMax failure ratio: 0\\nUpdate order: stop-first\\nRollbackConfig:\\nParallelism: 1\\nOn failure: pause\\nMonitoring Period: 5s\\nMax failure ratio: 0\\nRollback order: stop-first\\nContainerSpec:\\nImage: nigelpoulton/ddd-book:web0.1@sha256:8d6280c0042...1b9e4336730e5\\nInit: false\\nResources:\\nEndpoint Mode: vip\\nPorts:\\nPublishedPort = 8080\\nProtocol = tcp\\nTargetPort = 8080\\nPublishMode = ingress\\nThe example uses the--pretty flag to limit the output to the most interesting items\\nprinted in an easy-to-read format. You’ll get more info if you leave off the--pretty\\nflag. I highly recommend reading through the output ofdocker inspect commands,\\nas they’re a great way to learn what’s going on under the hood.\\nWe’ll come back to some of these outputs later.\\nReplicated vs global services\\nSwarm has two modes for deploying replicas to nodes:\\n• Replicated (default)\\n• Global\\nThe defaultreplicated mode allows you to deploy as many replicas as you need and\\nattempts to distribute them evenly across available nodes.\\nThe global mode deploys a single replica on every available node in the swarm.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 156}, page_content='10: Docker Swarm 150\\nBoth modes respect node availability and will only deploy replicas to eligible nodes. For\\nexample, if you don’t allow your swarm to deploy applications to managers, both modes\\nwill only deploy to workers.\\nIf you want to deploy aglobal service, you need to pass the--mode global flag to the\\ndocker service create command. Obviously, it doesn’t support the--replicas flag,\\nas it always deploys a single replica on each available node.\\nScaling services\\nAnother powerful feature ofservices is the ability to scale them up and down by adding\\nor removing replicas.\\nAssume business is booming and you’re experiencing double the number of requests to\\nyour web front-end. Fortunately, you can easily scale the number of replicas with the\\ndocker service scale command.\\nRun the following command to scale theweb-fe service from 5 to 10 replicas.\\n$ docker service scale web-fe=10\\nweb-fe scaled to 10\\noverall progress: 10 out of 10 tasks\\n1/10: running [==================================================>]\\n2/10: running [==================================================>]\\n3/10: running [==================================================>]\\n4/10: running [==================================================>]\\n5/10: running [==================================================>]\\n6/10: running [==================================================>]\\n7/10: running [==================================================>]\\n8/10: running [==================================================>]\\n9/10: running [==================================================>]\\n10/10: running [==================================================>]\\nverify: Service web-fe converged\\nBehind the scenes, the command updated the desired state of theweb-fe service in the\\ncluster store from 5 replicas to 10. The reconciliation loop read the new desired state of\\n10 replicas, compared it with the current observed state of 5 replicas, and added 5 more\\nto bring the observed state into sync with the new desired state.\\nRun anotherdocker service ls to verify the operation completed successfully.\\n$ docker service ls\\nID NAME MODE REPLICAS IMAGE PORTS\\nz7o...uw web-fe replicated 10/10 nigelpoulton/ddd... *:8080->8080/tcp\\nIf you run adocker service ps, you’ll see that the replicas are balanced evenly across\\nall available nodes. In this example, they’re balanced across both available workers. If\\nyour swarm had ten worker nodes, you would probably have one replica running on\\neach worker.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 157}, page_content='10: Docker Swarm 151\\n$ docker service ps web-fe\\nID NAME IMAGE NODE DESIRED CURRENT\\nnwf...tpn web-fe.1 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 7 mins\\nyb0...e3e web-fe.2 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 7 mins\\nmos...gf6 web-fe.3 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 7 mins\\nutn...6ak web-fe.4 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 7 mins\\n2ge...fyy web-fe.5 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 7 mins\\n64y...m49 web-fe.6 nigelpoulton/ddd-book:web0.1 wrk2 Running Running about a min\\nild...51s web-fe.7 nigelpoulton/ddd-book:web0.1 wrk1 Running Running about a min\\nvah...rjf web-fe.8 nigelpoulton/ddd-book:web0.1 wrk1 Running Running about a min\\nxe7...fvu web-fe.9 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 45 secs ago\\nl7k...jkv web-fe.10 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 46 secs ago\\nBehind the scenes, Swarm runs a scheduling algorithm calledspread that attempts to\\nbalance replicas as evenly as possible across available nodes.\\nRun anotherdocker service scale command to bring the number of replicas back\\ndown to 5.\\n$ docker service scale web-fe=5\\nweb-fe scaled to 5\\noverall progress: 5 out of 5 tasks\\n1/5: running [==================================================>]\\n2/5: running [==================================================>]\\n3/5: running [==================================================>]\\n4/5: running [==================================================>]\\n5/5: running [==================================================>]\\nverify: Service web-fe converged\\nNow that you know how to scale services, let’s see how to delete them.\\nDeleting services\\nDeleting services is simple — may be too simple.\\nRun the following command to delete the web-fe service. Be careful using this com-\\nmand, as it doesn’t ask for confirmation.\\n$ docker service rm web-fe\\nweb-fe\\nConfirm it’s gone.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 158}, page_content='10: Docker Swarm 152\\n$ docker service ls\\nID NAME MODE REPLICAS IMAGE PORTS\\nLet’s look at how to perform a rollout.\\nRollouts\\nApplication updates are a fact of life, and for the longest time they were painful. I’ve\\npersonally lost more than enough weekends to major application updates and I’ve no\\nintention of doing it again.\\nFortunately, thanks to Dockerservices, updating well-designed microservices apps is\\neasy.\\nTerminology: We use terms likerollouts, updates,and rolling updatesto mean\\nthe same thing — updating a live application.\\nYou’re about to deploy a new service to help demonstrate a rollout. However, before\\ndoing that, you’ll create a new overlay network for the service. This isn’t necessary, but I\\nwant you to see how to attach services to networks.\\nRun the following two commands to create a new overlay network calleduber-net and\\nthen check if it exists.\\n$ docker network create -d overlay uber-net\\n43wfp6pzea470et4d57udn9ws\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\n43wfp6pzea47 uber-net overlay swarm\\n<Snip>\\nThe new network exists and was successfully created as anoverlay network scoped to\\nthe entireswarm. You’ll learn about overlay networks in a later chapter, but for now,\\nit’s enough to know that they span all swarm nodes and that all containers on the same\\noverlay network can communicate, even if they’re deployed on different nodes.\\nFigure 10.5 shows four swarm nodes connected to two different underlay networks\\nconnected by a layer 3 router. The overlay network spans all four nodes and creates a\\nsingle flat layer 2 network abstracting all the underlying networks. All container replicas\\nare connected to the overlay and can communicate with each other.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 159}, page_content='10: Docker Swarm 153\\nFigure 10.5\\nLet’s create a new service and attach it to theuber-net network.\\n$ docker service create --name uber-svc \\\\\\n--network uber-net \\\\\\n-p 8080:8080 --replicas 12 \\\\\\nnigelpoulton/ddd-book:web0.1\\ndhbtgvqrg2q4sg07ttfuhg8nz\\noverall progress: 12 out of 12 tasks\\n1/12: running [==================================================>]\\n2/12: running [==================================================>]\\n3/12: running [==================================================>]\\n<Snip>\\n12/12: running [==================================================>]\\nverify: Service converged\\nLet’s step through what just happened.\\nOn the first line, you named the serviceuber-svc. You then used the--network flag\\nto tell Docker to attach all service replicas to the newuber-net network. Then, you\\nmapped 8080 on every swarm node to port8080 in all 12 replicas. Finally, you told\\nDocker which image to base the 12 replicas on.\\nThis mode of publishing a service on every swarm node, including nodes not even\\nrunning replicas, is calledingress modeand is the default. The alternative ishost mode,\\nwhich only publishes a service on nodes running replicas.\\nRun adocker service ls and adocker service ps to verify the state of the service\\nand each replica.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 160}, page_content='10: Docker Swarm 154\\n$ docker service ls\\nID NAME REPLICAS IMAGE\\ndhbtgvqrg2q4 uber-svc 12/12 nigelpoulton/ddd-book:web0.1\\n$ docker service ps uber-svc\\nID NAME IMAGE NODE DESIRED CURRENT STATE\\n0v...7e5 uber-svc.1 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nbh...wa0 uber-svc.2 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n23...u97 uber-svc.3 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n82...5y1 uber-svc.4 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nc3...gny uber-svc.5 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne6...3u0 uber-svc.6 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n78...r7z uber-svc.7 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\n2m...kdz uber-svc.8 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nb9...k7w uber-svc.9 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nag...v16 uber-svc.10 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\ne6...dfk uber-svc.11 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne2...k1j uber-svc.12 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nConnect to the service by opening a web browser and pointing it to the IP address of\\nany swarm node on port8080. If you’re following along on Multipass, you’ll need to\\nconnect to the192.168 address of one of your nodes.\\nFigure 10.6\\nYou should also point your browser to the IP of one of your managers to prove you can\\naccess the service from nodes that are not running replicas. That’s the beauty ofingress\\nmode — you can access a service from every node in the cluster, even nodes not running\\nreplicas.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 161}, page_content='10: Docker Swarm 155\\nRight now, the web page has links to two books. Let’s assume you need to add a third\\nbook. In the real world, you’d update the app code and create a new image containing\\nthe updates. I’ve already done this and uploaded the image to the same Docker Hub repo\\nwith theweb0.2 tag. All that’s left for you to do is perform the rollout.\\nLet’s assume you need the rollout to proceed in a staged manner by updating two\\nreplicas at a time with a 20-second delay between each. You can use the following\\ncommand to accomplish this.\\n$ docker service update \\\\\\n--image nigelpoulton/ddd-book:web0.2 \\\\\\n--update-parallelism 2 \\\\\\n--update-delay 20s \\\\\\nuber-svc\\nuber-svc\\noverall progress: 2 out of 12 tasks\\n1/12: running [==================================================>]\\n2/12: running [==================================================>]\\n3/12: ready [======================================> ]\\n4/12: ready [======================================> ]\\n5/12:\\n6/12:\\n<Snip>\\n11/12:\\n12/12:\\nLet’s review the command.\\ndocker service update lets you update existing services. This example tells Docker\\nto update all 12 replicas to the newweb0.2 version of the image. The--update-\\nparallelism flag tells Docker to update two replicas at a time, and the--update-delay\\nflag tells Docker to insert a 20-second cool-off period after each batch of updates. The\\nlast line tells it which service to update.\\nIf you run adocker service ps uber-svc while the update is in progress, some\\nreplicas will show the new version and some will show the old version. If you give the\\noperation enough time to complete, all replicas will eventually reach the new desired\\nstate and show theweb0.2 image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 162}, page_content='10: Docker Swarm 156\\n$ docker service ps uber-svc\\nID NAME IMAGE NODE DESIRED CURRENT STATE\\n7z...nys uber-svc.1 nigelpoulton/ddd-book:web0.2 wrk1 Running Running 13 secs\\n0v...7e5 \\\\_uber-svc.1 nigelpoulton/ddd-book:web0.1 wrk1 Shutdown Shutdown 13 secs\\nbh...wa0 uber-svc.2 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\ne3...gr2 uber-svc.3 nigelpoulton/ddd-book:web0.2 wrk2 Running Running 13 secs\\n23...u97 \\\\_uber-svc.3 nigelpoulton/ddd-book:web0.1 wrk2 Shutdown Shutdown 13 secs\\n82...5y1 uber-svc.4 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nc3...gny uber-svc.5 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne6...3u0 uber-svc.6 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n78...r7z uber-svc.7 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\n2m...kdz uber-svc.8 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nb9...k7w uber-svc.9 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nag...v16 uber-svc.10 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\ne6...dfk uber-svc.11 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne2...k1j uber-svc.12 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nYou can observe the update in real-time by opening a web browser to any swarm node\\non port8080 and hitting refresh a few times. Some of the requests will respond with\\nthe old version and some will respond with the new. After enough time, all requests will\\nretrieve the new version.\\nCongratulations. You’ve just completed a zero-downtime rolling update of a live\\ncontainerized application.\\nIf you run adocker service inspect --pretty command against the service, you’ll\\nsee theupdate parallelismand update delaysettings have been merged into the service’s\\ndefinition. This means future updates will automatically use these settings unless you\\noverride them as part of the update command.\\n$ docker service inspect --pretty uber-svc\\nID: mub0dgtc8szm80ez5bs8wlt19\\nName: uber-svc\\nService Mode: Replicated\\nReplicas: 12\\n<Snip>\\nUpdateConfig:\\nParallelism: 2 <<--------\\nDelay: 20s <<--------\\n<Snip>\\nService logs\\nYou can inspect a service’s logs with thedocker service logs command. It gathers the\\nlogs from every replica and displays them in a single output.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 163}, page_content='10: Docker Swarm 157\\nIf you’ve been following along, you’ll have a service calleduber-svc with 12 replicas\\nmapped to port8080 on every cluster node. Open a browser tab to the IP address of any\\ncluster node on port8080 and refresh the page a few times.\\nNow run adocker service logs command to see any log messages generated by the\\nservice.\\n$ docker service logs uber-svc\\nuber-svc.8.r33cra80eeut@wrk1 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.2.vid8et52oj48@wrk1 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.7.jv67x3zfkyar@wrk2 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.1.zlmvvo5k1i56@wrk2 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.3.kleeiy0vuue5@wrk2 | {\"level\":\"info\",\"message\":\"/\"}\\nThe output shows five lines relating to five requests serviced by five different replicas.\\nThe long string on the left of each line is the name of the replica which Docker creates\\nby concatenating the service name, replica number, replica ID, and the name of the node\\nit’s scheduled on. The following table maps the names of the five replicas that serviced\\nrequests.\\nService name Replica number Replica ID node\\nuber-svc 8 r33cra80eeut @wrk1\\nuber-svc 2 vid8et52oj48 @wrk1\\nuber-svc 7 jv67x3zfkyar @wrk2\\nuber-svc 1 zlmvvo5k1i56 @wrk2\\nuber-svc 3 kleeiy0vuue5 @wrk2\\nNotice how Swarm balanced the requests across five different replicas. In the real world,\\nyour requests probably won’t be as evenly balanced as this due to the way browsers and\\ndifferent tools create and handle requests. However, if you refresh enough times, you’ll\\neventually see the requests being serviced by different replicas.\\nBy default, Docker configures services to use thejson-file log driver, but other drivers\\nexist, including:\\n• awslogs\\n• gelf\\n• gcplogs\\n• journald (only works on Linux hosts runningsystemd)\\n• splunk\\n• syslog\\njson-file and journald are the easiest to configure, and they both work with thedocker\\nservice logs command. However, other drivers don’t always work with the native\\nDocker commands and you may have to use the logging platform’s native tools.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 164}, page_content='10: Docker Swarm 158\\nThe following snippet is from adaemon.json configuration file on a Docker host\\nconfigured to use thesyslog driver. Thedaemon.json file is usually located in/etc/-\\ndocker/daemon.json, but sometimes it doesn’t exist unless you create it manually for\\ncustom settings.\\n{\\n\"log-driver\": \"syslog\"\\n}\\nYou can force new services to use specific log drivers by starting them with the--log-\\ndriver and --log-opts flags. Doing this will override anything set in thedaemon.json\\nfile.\\nService logs expect applications to run as PID 1 and send logs toSTDOUT and errors to\\nSTDERR. The logging driver then forwards the logs to the locations configured via the\\nlogging driver.\\nYou can run adocker service logs command with the--follow flag to follow logs,\\nthe --tail flag to tail them, and the--details flag to get extra information.\\nDocker Swarm – The Commands\\n• docker swarm init creates a new swarm. The node you run the command on\\nbecomes the first manager and the Docker Engine on that node switches into\\nswarm mode.\\n• docker swarm join-token reveals the commands and tokens you need to join\\nworkers and managers to the swarm. You addmanager to the command to get the\\nmanager join token andworker to get the worker token. Be sure to keep your join\\ntokens secure!\\n• docker node ls lists managers and workers, including which manager is the\\nleader.\\n• docker service create creates a new service.\\n• docker service ls lists running services and gives basic information on their\\nstate and the number of replicas they’re running.\\n• docker service ps <service> gives more detailed information about individual\\nservice replicas.\\n• docker service inspect gives very detailed information on a service. You can\\nadd the--pretty flag to get a nicely formatted summary view.\\n• docker service scale lets you scale the number of service replicas.\\n• docker service update lets you update the properties of a running service.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 165}, page_content='10: Docker Swarm 159\\n• docker service logs shows service logs.\\n• docker service rm deletes a service without asking for confirmation.\\nChapter summary\\nSwarm is Docker’s native technology for managing clusters of Docker nodes and\\norchestrating microservices applications. It is similar to Kubernetes but less advanced\\nand easier to use.\\nThe enterprise-grade clustering component offers a wealth of security and HA features\\nthat are automatically configured and extremely simple to modify.\\nThe orchestration component deploys and manages cloud-native microservices\\napplications.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 166}, page_content='11: Deploying apps with Docker Stacks\\nDeploying and managing cloud-native microservices applications at scale is hard.\\nFortunately, Docker Stacks are here to help.\\nI’ve split this chapter as follows:\\n• Deploying apps with Docker Stacks – The TLDR\\n• Build a Swarm lab\\n• The sample app\\n• Deploy the app\\n• Manage the app\\nDeploying apps with Docker Stacks – The TLDR\\nDocker Stacks combine Compose and Swarm to create a platform for easy deployment\\nand management of complex multi-container apps on secure, highly available infrastruc-\\nture.\\nYou build a swarm, define your apps in Compose files, and deploy and manage them\\nwith thedocker stack command.\\nFrom an architecture perspective, stacks are at the top of the Docker application\\nhierarchy — they build on top ofservices, which in turn build on top of containers, and\\nthey only run on swarms.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 167}, page_content='11: Deploying apps with Docker Stacks 161\\nFigure 11.1\\nIf you’ve been following along, you’ll already know Compose and Swarm and find\\nDocker Stacks easy.\\nBuild a Swarm lab\\nThis section shows you how to build a three-node swarm lab with a single manager and\\ntwo workers. You can skip to the next section if you already have a swarm.\\nYou can build the lab in Multipass, Play with Docker, or just about any Docker envi-\\nronment. However, I don’t recommend using Docker Desktop, as you only get a single\\nswarm node.\\nRun the following command from the node that will be your swarm manager.\\n$ docker swarm init\\nSwarm initialized: current node (lhma...w4nn) is now a manager.\\n<Snip>\\nCopy thedocker swarm join command shown in the output and paste it into the two\\nnodes you want to join as workers.\\nWorker 1\\n$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377\\nThis node joined a swarm as a worker.\\nWorker 2'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 168}, page_content='11: Deploying apps with Docker Stacks 162\\n$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377\\nThis node joined a swarm as a worker.\\nRun the following command from your manager node to confirm the swarm is initial-\\nized with one manager and two workers.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\nlhm...4nn * mgr1 Ready Active Leader\\nb74...gz3 wrk1 Ready Active\\no9x...um8 wrk2 Ready Active\\nNow that you have a swarm lab, let’s look at the sample app.\\nThe sample app\\nFigure 11.2 shows the application we’ll use for the rest of the chapter. It’s a multi-\\ncontainer microservices application with:\\n• Two services (web-fe and redis)\\n• An encrypted overlay network (counter-net)\\n• A volume (counter-vol)\\n• A published port (5001)\\nFigure 11.2'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 169}, page_content=\"11: Deploying apps with Docker Stacks 163\\nTerminology: Throughout this chapter, we’ll use the termservice to refer to\\nthe Docker service object that manages one or more identical containers on\\na swarm. We’ll also refer to the Compose file as thestack file, and sometimes\\nwe’ll refer to the application as thestack.\\nIf you haven’t already done so, log on to a swarm manager and clone the book’s GitHub\\nrepo.\\n$ git clone https://github.com/nigelpoulton/ddd-book.git\\nCloning into 'ddd-book'...\\nremote: Enumerating objects: 8904, done.\\nremote: Counting objects: 100% (74/74), done.\\nremote: Compressing objects: 100% (52/52), done.\\nremote: Total 8904 (delta 21), reused 70 (delta 18), pack-reused 8830\\nReceiving objects: 100% (8904/8904), 74.00 MiB | 4.18 MiB/s, done.\\nResolving deltas: 100% (1378/1378), done.\\nChange directory into theddd-book/swarm-app folder.\\n$ cd ddd-book/swarm-app\\nFeel free to look at the application in theapp subfolder, but we’ll focus on thecom-\\npose.yaml file.\\nIf you look at the Compose file, you’ll see it has three top-level keys.\\n• networks\\n• volumes\\n• services\\nNetworks is where you define the networks the app needs,volumes defines the\\nvolumes it needs, andservices defines the microservices that make up the app. The file\\nis a simple example ofinfrastructure as code— the application and its infrastructure are\\ndefined in a configuration file that you deploy and manage them from.\\nIf you expand each top-level key, you’ll see how things map to Figure 11.2 with a\\nnetwork calledcounter-net, a volume calledcounter-vol, and two services calledweb-\\nfe and redis.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 170}, page_content=\"11: Deploying apps with Docker Stacks 164\\nnetworks:\\ncounter-net:\\nvolumes:\\ncounter-vol:\\nservices:\\nweb-fe:\\nredis:\\nLet’s look at each section of thestack file.\\nLooking closer at the stack file\\nStack filesare identical to Compose files with a few differences at run-time. For example,\\nDocker Compose lets you build images at run-time, but Docker Stacks don’t.\\nLet’s look at the networking elements defined in our stack file.\\nNetworks and networking\\nOne of the first things Docker does when you deploy an app from a stack file is create\\nthe networks listed under thenetworks key. If they already exist, Docker doesn’t need to\\ndo anything. But if they don’t exist, it creates them.\\nThis app defines a single encrypted overlay network calledcounter-net.\\nnetworks:\\ncounter-net: <<---- Network name\\ndriver: overlay <<---- Driver/type of network\\ndriver_opts:\\nencrypted: 'yes' <<---- Encrypt the data plane\\nIt needs to be an overlay network so it can span all nodes in the swarm.\\nEncrypting the data plane will keep the application traffic private, but it incurs a\\nperformance penalty that varies based on factors such as traffic type and traffic flow. It’s\\nnot uncommon for the performance penalty to be around 10%, but you should perform\\nyour own testing against your own applications.\\nThe stack also publishes theweb-fe service on port5001 on the swarm-wideingress\\nnetwork and maps it back to port8080 on all service replicas. This allows external clients\\nto reach the service by hitting any swarm node on port5001.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 171}, page_content='11: Deploying apps with Docker Stacks 165\\nservices:\\nweb-fe:\\n<Snip>\\nports:\\n- target: 8080 <<---- Service replicas listen on this port\\npublished: 5001 <<---- and are published externally on this port\\nLet’s look at the volumes and mounts.\\nVolumes and mounts\\nThe app defines a single volume calledcounter-vol and mounts it into the/app\\ndirectory on allweb-fe replicas. This means all reads and writes to the/app folder will\\nbe read and written to the volume.\\nvolumes:\\ncounter-vol: <<---- Volume name\\nservices:\\nweb-fe:\\n<Snip>\\nvolumes:\\n- type: volume\\nsource: counter-vol <<---- Mount the \"counter-vol\" volume to\\ntarget: /app <<---- \"/app\" in all service replicas\\nLet’s look at the services.\\nServices\\nServices are where most of the action happens. Our application defines two, and we’ll\\nlook at each in turn.\\nThe web-fe service\\nAs you can see, theweb-fe service defines an image, an app, the desired number of\\nreplicas, rules for updating the app, a restart policy, which network to connect to, which\\nports to publish, and how to mount a volume. That’s a lot to take in, so I’ve annotated it\\nin the book. Take a minute to read through the file and annotations.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 172}, page_content='11: Deploying apps with Docker Stacks 166\\nweb-fe:\\nimage: nigelpoulton/ddd-book:swarm-app <<---- Create all replicas with this image\\ncommand: python app.py <<---- Run this command when each replica starts\\ndeploy:\\nreplicas: 4 <<---- Deploy 4 replicas\\nupdate_config: ------┐ When you perform an update,\\nparallelism: 2 | update 2 replicas at a time,\\ndelay: 10s | wait 10 seconds in between each set of two replicas\\nfailure_action: rollback ------┘ and perform a rollback if you encounter issues\\nplacement: ------┐\\nconstraints: | Only run replicas on worker nodes\\n- \\'node.role == worker\\' ------┘\\nrestart_policy: ------┐ Only restart replicas if\\ncondition: on-failure | they\\'ve failed (non-zero return code),\\ndelay: 5s | wait five seconds between each restart attempt,\\nmax_attempts: 3 | only try three restarts,\\nwindow: 120s ------┘ and give up trying after two minutes\\nnetworks:\\n- counter-net <<---- Connect replicas to the \"counter-net\" network\\nports:\\n- published: 5001 ------┐ Publish the service externally on port 5001 and\\ntarget: 8080 ------┘ map traffic to each replica on port 8080\\nvolumes:\\n- type: volume\\nsource: counter-vol ------┐ Mount the \"counter-vol\" volume to\\ntarget: /app ------┘ \"/app\" in each service replica\\nThe image key is the only mandatory key and tells Docker which image to use when\\ncreating service replicas. Swarm stacks don’t support building images at deploy time, so\\nthe image must exist before you deploy the app. Docker is alsoopinionated and assumes\\nyou want to pull images from Docker Hub. However, you can add the registry’s DNS\\nname before the image name if you want to use a third-party registry. For example,\\nadding ghcr.io before an image name will pull it from GitHub Container Registry.\\nThe command key tells Docker how to start the app in each replica. Our example tells it to\\nexecute python app.py in replicas.\\nThe deploy.replicas key tells Swarm to deploy and manage four service replicas\\n(identical containers).\\nweb-fe:\\ndeploy:\\nreplicas: 4\\nIf you need to change the number of replicas after you’ve deployed the service, you\\nshould update the value ofdeploy.replicas in the stack file and then redeploy the\\nstack. We’ll see this later.\\nThe deploy.update_config block tells Docker how to perform rollouts.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 173}, page_content=\"11: Deploying apps with Docker Stacks 167\\nweb-fe:\\ndeploy:\\nupdate_config:\\nparallelism: 2\\ndelay: 10s\\nfailure_action: rollback\\nThis application tells Docker to update two replicas at a time, wait 10 seconds between\\neach set of two, and perform a rollback if it encounters any problems.\\nRolling back replaces the new replicas with fresh copies of the old version. The default\\nvalue forfailure_action is pause, which will halt the update where it is and may result\\nin some replicas running the old version and some running the new. The other option is\\ncontinue.\\nThe deploy.placement block forces all replicas onto worker nodes. You’ll need to delete\\nthis section if you’re running a single-node cluster with one manager and zero workers.\\nweb-fe:\\ndeploy:\\nplacement:\\nconstraints:\\n- 'node.role == worker'\\nThe deploy.restart_policy block tells Docker what to do when it encounters a failure\\nduring a rollout.\\nweb-fe:\\ndeploy:\\nrestart_policy:\\ncondition: on-failure\\nmax_attempts: 3\\ndelay: 5s\\nwindow: 120s\\nThis app tells Docker to restart replicas if they fail, to attempt a maximum of 3 restarts,\\nto wait 5 seconds between each restart attempt, and to wait up to 120 seconds to decide\\nif the restart worked.\\nThe networks key tells Docker to attach all replicas to thecounter-net network.\\nweb-fe:\\nnetworks:\\n- counter-net\\nThe ports block publishes the app on port5001 on the ingress network and port8080\\non the counter-net network. This means external traffic can hit any swarm node on\\n5001 and get redirected to the service replicas on8080.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 174}, page_content='11: Deploying apps with Docker Stacks 168\\nweb-fe:\\nports:\\n- published: 5001\\ntarget: 8080\\nFinally, thevolumes block mounts thecounter-vol volume into each service replica’s\\n/app directory.\\nweb-fe:\\nvolumes:\\n- type: volume\\nsource: counter-vol\\ntarget: /app\\nThe redis service\\nThe redis service is much simpler. It pulls theredis:alpine image, starts a single\\nreplica, and attaches it to thecounter-net network. This is the same network as the\\nweb-fe service, meaning the two services will be able to communicate with each other\\nby name (“redis” and “web-fe”).\\nredis:\\nimage: \"redis:alpine\"\\nnetworks:\\ncounter-net:\\nAs previously mentioned, Compose files are a great source of application documenta-\\ntion. We can read this file and know the application has two services, two networks, and\\none volume. We know how the services communicate, how they’re published outside of\\nthe swarm, and a bit about how they’ll be deployed, updated, and restarted from failures.\\nLet’s deploy and manage the app.\\nDeploying the app\\nIf you plan on following this section, you’ll need a copy of the book’s GitHub repo and\\na swarm. The examples use a three-node swarm with a single manager calledmgr1 and\\ntwo workers calledwrk1 and wrk2.\\nDeploy the app\\nYou deploy Stacks with thedocker stack deploy command, and in its basic form it\\naccepts two arguments:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 175}, page_content='11: Deploying apps with Docker Stacks 169\\n• The name of the stack file (Compose file)\\n• The name of the stack (app)\\nWe’ll use thecompose.yaml file in theddd-book/swarm-app folder, and we’ll call the app\\nddd.\\nRun all of the following commands from theswarm-app directory on a swarm manager.\\nDeploy the stack.\\n$ docker stack deploy -c compose.yaml ddd\\nCreating network ddd_counter-net\\nCreating service ddd_web-fe\\nCreating service ddd_redis\\nA few things to note from the output.\\nDocker creates the networks and volumes before services. This is because services\\nconsume networks and volumes, and will fail to start if they don’t exist.\\nDocker also prefixes the name of the stack to every resource. We called our stackddd,\\nmeaning Docker will prefix every resource name withddd:\\n• ddd_counter-net\\n• ddd_counter-vol\\n• ddd_web-fe\\n• ddd_redis\\nYou can check the status of services, volumes, and networks with their usual commands,\\nbut thedocker stack command also has a couple of options for checking stack status:\\n• docker stack ls prints a list of running stacks and how many services they have\\n• docker stack ps <stack-name> gives more detailed information about a specific\\nstack and its replicas\\nLet’s look at both.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 176}, page_content='11: Deploying apps with Docker Stacks 170\\n$ docker stack ls\\nNAME SERVICES\\nddd 2\\n$ docker stack ps ddd\\nNAME IMAGE NODE DESIRED STATE CURRENT STATE\\nddd_redis.1 redis:alpine mgr1 Running Running 4 mins\\nddd_web-fe.1 nigelpoulton/ddd-book:swarm-app wrk1 Running Running 4 mins\\nddd_web-fe.2 nigelpoulton/ddd-book:swarm-app wrk2 Running Running 4 mins\\nddd_web-fe.3 nigelpoulton/ddd-book:swarm-app wrk2 Running Running 4 mins\\n<Snip>\\nddd_web-fe.10 nigelpoulton/ddd-book:swarm-app wrk1 Running Running 4 mins\\nThe docker stack ps command is great for troubleshooting services. You can see the\\nimage each replica is based on, which nodes replicas are running on, anddesired stateand\\ncurrent state.\\nThe following output shows two failed attempts to startweb-fe replicas on thewrk2\\nnode.\\n$ docker stack ps ddd\\nNAME NODE DESIRED CURRENT ERROR\\nweb-fe.1 wrk2 Shutdown Failed \"task: non-zero exit (1)\"\\n\\\\_web-fe.1 wrk2 Shutdown Failed \"task: non-zero exit (1)\"\\nYou can also use thedocker service logs command to inspect service logs. If you\\npass it the service name or ID, you’ll get the logs for all service replicas. If you pass it a\\nspecific replica name or ID, you’ll only get the logs for that replica.\\nThe following example shows the logs for all replicas in theddd_web-fe service.\\n$ docker service logs ddd_web-fe\\nddd_web-fe.9.i23puo71kq12@node2 | * Serving Flask app \\'app\\'\\nddd_web-fe.5.z4otpnjrvc58@node2 | * Debug mode: on\\n<Snip>\\nddd_web-fe.6.novrixi5iuxy@node2 | * Debug mode: on\\nddd_web-fe.6.novrixi5iuxy@node2 | * Debugger is active!\\nddd_web-fe.6.novrixi5iuxy@node2 | * Debugger PIN: 127-233-151\\nYou can follow logs (--follow), tail them (--tail), and you can sometimes get extra\\ndetails (--details).\\nSo far, you’ve used Docker commands to prove the stack is up and running. However,\\nyou can also test it with your browser.\\nRun adocker stack services command to discover the port theweb-fe service is\\npublished on.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 177}, page_content='11: Deploying apps with Docker Stacks 171\\n$ docker stack services ddd\\nNAME MODE REPLICAS IMAGE PORTS\\nddd_redis replicated 1/1 redis:alpine\\nddd_web-fe replicated 4/4 nigelpoulton/ddd-book:swarm-app *:5001->8080/tcp\\nIt’s published on port5001, meaning you can point your browser to the name or IP\\nof any swarm node on5001. If you’re using Multipass, the node IPs usually start with\\n192.168, and you can find them by running amultipass list command from your\\nhost’s terminal. Alternatively, you can run anip a command inside a VM and use the\\nIP address of theenp0s1 interface.\\nFigure 11.3 shows a browser connecting to the app via a swarm node with the192.168.64.41\\nIP address.\\nFigure 11.3\\nLet’s switch tack and see how to manage swarm stacks.\\nManaging the app\\nThere are two ways to manage Docker stacks:\\n• Imperatively\\n• Declaratively\\nThe imperative methodis where you run Docker commands to make changes to the stack.\\nFor example, using thedocker service scale command to increase and decrease the\\nnumber of service replicas.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 178}, page_content='11: Deploying apps with Docker Stacks 172\\nThe declarative methodis where you make all changes via the stack file. For example,\\nif you want to change the number of service replicas, you edit the stack file with the\\ndesired replica count and run anotherdocker stack deploy command.\\nThe declarative method is the preferred method.\\nConsider the following example that demonstrates why you should manage stacks\\ndeclaratively.\\nImagine you’ve deployed an app from a stack file that includes areporting service and acatalog\\nservice. The stack file includes other services that are part of the app, but we’re only interested\\nin these two. It’s currently running five replicas of the reporting service, but year-end reporting\\nhas started, and it’s experiencing slow performance due to increased demand. You decide to run\\nan imperativedocker service scale command to increase the number of reporting replicas\\nto 10. This fixes the performance issues, but the current state of the app is out of sync with the\\nstack file — the stack file defines five replicas but the cluster is running 10. Later in the day, a\\ncolleague is tasked with rolling out a new version of the catalog service — the catalog service is\\npart of the same app and, therefore, is defined in the same stack file as the reporting service. Your\\ncolleague makes the changes declaratively by editing the stack file with the new version of the\\ncatalog service and running adocker stack deploy command to push the updates. When\\nDocker receives the updated version of the stack file, it rolls out the new version of the catalog\\nservice and changes the number of reporting replicas back to five. This will cause the reporting\\nservice to start running slowly again.\\nThis is why you should make all changes declaratively via your stack files and manage\\nthem in a version control system.\\nGetting back to the app you deployed, let’s do the following:\\n• Increase the number ofweb-fe replicas from 4 to 10\\n• Update theweb-fe service to the newerswarm-appv2 image\\nEdit thecompose.yaml, increase theweb-fe replica count to 10, and change the image to\\nswarm-appv2.\\n<Snip>\\nservices:\\nweb-fe:\\nimage: nigelpoulton/ddd-book:swarm-appv2 <<---- changed to swarm-appv2\\ncommand: python app.py\\ndeploy:\\nreplicas: 10 <<---- Changed from 4 to 10\\n<Snip>\\nSave your changes and redeploy the app. This will cause Docker torollout a new version\\nof theweb-fe service with all 10 replicas running the new image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 179}, page_content='11: Deploying apps with Docker Stacks 173\\n$ docker stack deploy -c compose.yaml ddd\\nUpdating service ddd_redis (id: ozljsazuv7mmh14ep70pv43cf)\\nUpdating service ddd_web-fe (id: zbbplw0hul2gbr593mvwslz5i)\\nEven though it looks like Docker has redeployed both services, it hasn’t. It’s clever\\nenough only to redeploy the bits you’ve changed.\\nRun adocker stack ps to see the rollout’s progress.\\n$ docker stack ps ddd\\nNAME IMAGE NODE DESIRED CURRENT STATE\\nddd_web-fe.1 nigelpoulton/ddd-book:swarm-app node2 Running Running 8 mins ago\\nddd_web-fe.2 nigelpoulton/ddd-book:swarm-appv2 node2 Running Running 13 secs ago\\n\\\\_ddd_web-fe.2 nigelpoulton/ddd-book:swarm-app node2 Shutdown Shutdown 26 secs ago\\nddd_web-fe.3 nigelpoulton/ddd-book:swarm-app node1 Running Running 8 mins ago\\n<Snip>\\nI’ve trimmed the output to fit the book, and I’ve only listed some of the replicas.\\nHowever, you can see a few things.\\nThe top line shows theddd_web-fe.1 replica running the old image for the last 8\\nminutes.\\nThe next two lines show theddd_web-fe.2 replica. You can see the old copy was\\nrunning the old image, was shut down 26 seconds ago, and was replaced with a new\\nversion running the new image. The new replica has been running for 13 seconds.\\nThe last line shows theddd_web-fe.3 replica is still running the old version.\\nIt’s important to emphasize two things.\\nFirst, Docker followed the rules in thedeploy.update_config section of the Compose\\nfile. If you look at that section of the file again, you’ll see that Docker can update two\\nreplicas, wait for 10 seconds, update the next two, wait 10 seconds, and then attempt a\\nrollback to the previous version if it encounters any issues.\\nweb-fe:\\ndeploy:\\nupdate_config:\\nparallelism: 2\\ndelay: 10s\\nfailure_action: rollback\\nSecond, containers are immutable and Docker didn’tupdate the existing containers to\\nrun the new image. It deleted and replaced them with new containers running the new\\nimage.\\nAfter a short period, all 10 replicas will be running updated image.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 180}, page_content='11: Deploying apps with Docker Stacks 174\\nRefresh your browser to make sure you see the updated version of the app with a purple\\nWebAssembly is the futurebanner.\\nSomething needs to be fixed because it still displays the old web page.\\nThe issue is with the volume. Let’s walk through what happened.\\nThe new image has the updated app version with the purpleWebAssembly is the future\\nbanner. The rollout deleted the old replicas and replaced them with new ones running\\nthe new version of the app. So far, so good. However, when Docker started each new\\nreplica, it mounted the old volume with the old app version. This had the effect of\\noverwriting the new version of the app with the old.\\nYou should be aware of this when working with volumes. However, it will usually be\\nfine because you typically only use volumes for data stores, not for hosting application\\nbinaries.\\nLet’s assume you realize theweb-fe service is stateless and doesn’t require a volume.\\nThe declarative way to remove the volume is to edit the Compose file again, remove the\\nvolume and volume mount, and redeploy the app. Let’s do it.\\nEdit thecompose.yaml file and make the following changes.\\nvolumes: <<---- Delete this line\\ncounter-vol: <<---- Delete this line\\n<Snip>\\nservices:\\nweb-fe:\\nimage: nigelpoulton/ddd-book:swarm-appv2\\n<Snip>\\nvolumes: <<---- Delete this line\\n- type: volume <<---- Delete this line\\nsource: counter-vol <<---- Delete this line\\ntarget: /app <<---- Delete this line\\nSave your changes and redeploy.\\n$ docker stack deploy -c compose.yaml ddd\\nUpdating service ddd_redis (id: ozljsazuv7mmh14ep70pv43cf)\\nUpdating service ddd_web-fe (id: zbbplw0hul2gbr593mvwslz5i)\\nThe stack will update two replicas at a time and wait 10 seconds between each. Once the\\nstack has converged and all replicas are updated, you should see the new version of the\\napp as shown in Figure 11.4. Refresh your browser a few times to make sure it works.\\nDon’t worry if some requests get the old version while the rollout is in progress.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 181}, page_content='11: Deploying apps with Docker Stacks 175\\nFigure 11.4 - The updated app\\nThe volume will still exist, and you’ll need to delete it manually.\\nCongratulations. You’ve successfully deployed and managed a multi-container app using\\nDocker Stacks. You also learned to deploy and manage apps declaratively by making all\\nchanges via the stack file (Compose file).\\nClean up\\nIf you’ve been following along, you’ve deployed an app with two services, a network,\\nand a volume.\\nYou can delete the stack with thedocker stack rm command. However, be warned that\\nit deletes the stack without asking for confirmation.\\n$ docker stack rm ddd\\nRemoving service ddd_redis\\nRemoving service ddd_web-fe\\nRemoving network ddd_counter-net\\nThe command deleted the network and services but not the volume. This is because\\nvolume lifecycles are decoupled from containers and services, and you need to delete\\nthem manually.\\nRun the following command on every swarm node that hosted a replica.\\n$ docker volume rm ddd_counter-vol\\nddd_counter-vol'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 182}, page_content='11: Deploying apps with Docker Stacks 176\\nDeploying apps with Docker Stacks – The Commands\\n• docker stack deploy is the command you’ll run to deployand update stacks. You\\nneed to specify the name of the stack and the stack file. Docker expects the stack\\nfile to be calledcompose.yaml by default.docker stack ls lists all stacks on a\\nswarm and shows the number of services each one has.\\n• docker stack ps gives you detailed information about a stack. It tells you which\\nnodes replicas are running on, which images they’re based on, and shows the\\ndesired stateand current stateof each service replica.\\n• docker stack rm deletes a stack and doesn’t ask for confirmation.\\nChapter Summary\\nStacks are Docker’s native solution for running cloud-native microservices applications\\non Swarm clusters. They offer a simple declarative interface for managing the entire\\nlifecycle of applications and infrastructure.\\nYou start with application code and infrastructure requirements — things like networks,\\nports, volumes, and secrets. You containerize the application and combine all the\\nservices and infrastructure definitions into a single declarative stack file. You set the\\nnumber of replicas, as well as rollout and restart policies. Then, you use thedocker\\nstack deploy command to deploy the application from the stack file.\\nYou should also perform all updates declaratively by updating the stack file and rede-\\nploying the app.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 183}, page_content='12: Docker and WebAssembly\\nWebAssembly is driving the third of cloud computing, and Docker is evolving to take\\nadvantage.\\nWe built the first wave on virtual machines (VMs), the second on containers, and we’re\\nbuilding the third on WebAssembly. Each wave is driving smaller, faster, and more\\nsecure workloads, and all three will work together in the future of cloud computing.\\nIn this chapter, you’ll write a simple WebAssembly application and use Docker to\\ncontainerize and run it in a container. The goal is to introduce you to WebAssembly and\\nshow you how easy it is to work with Docker and WebAssembly together.\\nThe termsWebAssembly and Wasm mean the same thing, and we’ll use the term Wasm.\\nI’ve divided the chapter as follows:\\n• Pre-reqs\\n• Intro to Wasm\\n• Write a Wasm app\\n• Containerize a Wasm app\\n• Deploy a Wasm app'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 184}, page_content='12: Docker and WebAssembly 178\\nPre-reqs\\nYou’ll need all of the following if you plan on following along:\\n• Docker Desktop4.30+ with Wasm enabled\\n• Rust 1.72+ with the Wasm target installed\\n• Spin 2.5+\\nAt the time of writing, support for Wasm is a beta feature in Docker Desktop and doesn’t\\nwork with Multipass Docker VMs. This may change in the future. It also means there’s a\\nhigher risk of bugs. I’ve tested the examples in this chapter on Docker Desktop 4.30.0.\\nConfigure Docker Desktop for Wasm\\nOpen the Docker Desktop UI, click the Settings icon at the top right, and make sure\\nUse containerd for pulling and storing imagesis selected. Next, click theFeatures in\\ndevelopment tab, select theEnable Wasmoption and click the blueApply & restart\\nbutton.\\nSome of the settings are shown in Figure 12.2.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 185}, page_content=\"12: Docker and WebAssembly 179\\nFigure 12.2. Docker Desktop Wasm settings\\nInstall Rust and configure for Wasm\\nSearch the web forhow to install Rustand follow the instructions for your platform.\\nOnce you’ve installed Rust, run the following command to install thewasm32-wasi\\ntarget so that Rust can compile to Wasm.\\n$ rustup target add wasm32-wasi\\ninfo: downloading component 'rust-std' for 'wasm32-wasi'\\ninfo: installing component 'rust-std' for 'wasm32-wasi'\\nInstall Spin\\nSpin is a Wasm framework and runtime that makes building and running Wasm apps\\neasy.\\nSearch the web forhow to install Fermyon spinand follow the instructions for your\\nsystem.\"),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 186}, page_content='12: Docker and WebAssembly 180\\nRun the following command to ensure it’s installed correctly.\\n$ spin --version\\nspin 2.5.0 (83eb68d 2024-05-08)\\nYou’re now ready to build and run Wasm apps on your local machine.\\nIntro to Wasm and Wasm containers\\nWasm is a new type of application that is smaller, faster, and more portable than tradi-\\ntional Linux containers. However, traditional Linux containers can do a lot more than\\nWasm apps. For example, Wasm apps are currently great for AI workloads, serverless\\nfunctions, plugins, and edge devices but not so good for complex networking or heavy\\nI/O.\\nHowever, Wasm is evolving fast and may become better at other workloads in the\\nfuture.\\nDigging a little deeper…\\nAs we’re about to see, Wasm is a new virtual machine architecture that programming\\nlanguages compile to. So, instead of compiling apps toLinux on ARMor Linux on AMD,\\nyou compile them toWasm. You can then run these Wasm apps on any system with a\\nWasm runtime. Fortunately, Docker Desktop already ships with several Wasm runtimes.\\nRun the following command to see the list of Wasm runtimes installed as part of your\\nDocker Desktop environment. The first time you run the command, it will download\\nthe image.\\n$ docker run --rm -i --privileged --pid=host jorgeprendes420/docker-desktop-shim-manager:latest\\nio.containerd.wasmtime.v1\\nio.containerd.spin.v2\\nio.containerd.wws.v1\\nio.containerd.lunatic.v1\\nio.containerd.slight.v1\\nio.containerd.wasmedge.v1\\nio.containerd.wasmer.v1\\nMy installation has seven Wasm runtimes, including theio.containerd.spin.v2\\nruntime we’ll use in the examples.\\nThese Wasm runtimes allowcontainerd to deploy and manageWasm containers. AWasm\\ncontainer is a Wasm binary running inside a minimalscratch containerso that you can\\nbuild, ship, and run them with familiar Docker tools such as thedocker run command\\nand Docker Hub.\\nTalk is cheap though, let’s see it in action.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 187}, page_content='12: Docker and WebAssembly 181\\nWrite a Wasm app\\nWe’ll usespin to create a simple web server that we’ll compile as a Wasm app. In future\\nsteps, you’ll build, ship, and run the app as a Wasm container.\\nChange into a new directory and then run the following command to create a new\\nWasm app calledhello-world. Respond to the prompts as shown in the example.\\n$ spin new hello-world -t http-rust\\nDescription: Wasm app\\nHTTP path: /hello\\nThe command creates a newhello-world directory and scaffolds a simple Rust-based\\nweb app. Change into this directory and inspect the app files. If you don’t have thetree\\ncommand, you can run anls -l for similar results.\\n$ cd hello-world\\n$ tree\\n.\\n├── Cargo.toml\\n├── spin.toml\\n└── src\\n└── lib.rs\\nWe’re only interested in thespin.toml and src/lib.rs files.\\nEdit thesrc/lib.rs file and change the text inside the double quotes as shown in the\\nfollowing snippet. This configures the app to displayDocker loves Wasm.\\nuse spin_sdk::http::{IntoResponse, Request};\\n<Snip>\\nOk(http::Response::builder()\\n.status(200)\\n.header(\"content-type\", \"text/plain\")\\n.body(\"Docker loves Wasm\")?) <<---- Change text inside quotes\\n}\\nOnce you’ve saved your changes, run aspin build command to compile the app as a\\nWasm binary.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 188}, page_content='12: Docker and WebAssembly 182\\n$ spin build\\nBuilding component with `cargo build --target wasm32-wasi --release`\\n<Snip>\\nFinished building all Spin components\\nIf you look at the first line of the output, you’ll see it’s running a more complexcargo\\nbuild command that compiles the app as a Wasm binary.\\nRun anothertree command to see the Wasm binary.\\n$ tree\\n<Snip>\\n└── target\\n└── wasm32-wasi\\n└── release\\n└── hello_world.wasm\\nThe output is much longer this time, and I’ve trimmed the example in the book so you\\nonly see thehello_world.wasm binary. This is the Wasm app and will run on any system\\nwith thespin Wasm runtime.\\nYou’ll containerize the app in the next section, but you should test it works before\\nproceeding.\\nRun aspin up command to start a local instance of the app. This will start the app using\\nthe localspin runtime you installed earlier.\\n$ spin up\\nLogging component stdio to \".spin/logs/\"\\nServing http://127.0.0.1:3000\\nAvailable Routes:\\nhello-world: http://127.0.0.1:3000/hello\\nPoint your browser tohttp://127.0.0.1:3000/hello and make sure the app works.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 189}, page_content='12: Docker and WebAssembly 183\\nFigure 12.3. Wasm app running locally\\nCongratulations. You just built a simple web server, compiled it to Wasm, and executed\\nit locally usingspin! In the next section, you’ll containerize it.\\nPress Ctrl-C to kill the app.\\nContainerize a Wasm app\\nDocker Desktop lets you containerize Wasm apps so you can use familiar Docker tools\\nto push and pull them to OCI registries and run them inside containers.\\nAs always, you need a Dockerfile that tells Docker how to package the app as an image.\\nCreate a new file calledDockerfile in your current directory and populate it with the\\nfollowing three lines.\\nFROM scratch\\nCOPY /target/wasm32-wasi/release/hello_world.wasm .\\nCOPY spin.toml .\\nThe file references a special empty base image calledscratch because Wasm containers\\ndon’t need a Linux OS.\\nThe twoCOPY instructions copy thehello_world.wasm Wasm app and thespin.toml\\nfile into the image.\\nIf you look closely, you’ll see that thespin.toml file expects the Wasm app to be in the\\ntarget/wasm32-wasi/release/ directory. However, the secondCOPY instruction places'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 190}, page_content='12: Docker and WebAssembly 184\\nit in the root folder. This means we’ll need to update thespin.toml file so it knows\\nwhere to find the app after it’s copied into the root of the image.\\nEdit thespin.toml file and remove the leading path for thesource line as shown.\\n<Snip>\\n[component.hello-world]\\nsource = \"hello_world.wasm\" <<---- Remove any leading directories so it looks like this\\n<Snip>\\nSave your changes.\\nRun the following command to containerize the Wasm app. Be sure to tag the image\\nwith your own Docker Hub username instead of mine.\\n$ docker build \\\\\\n--platform wasi/wasm \\\\\\n--provenance=false \\\\\\n-t nigelpoulton/ddd-book:wasm .\\nThe --platform wasi/wasm flag sets the image as a Wasm image.\\nSome older versions of Docker have an older builder and will fail. If this happens, try\\nrunning the same command, but change the first line todocker buildx build \\\\.\\nList the images on your system.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book wasm 7b55889f1006 2 minutes ago 620kB\\nSee how the Wasm image looks like a regular image, just smaller.\\nYou can push and pull the image to Docker Hub and other OCI registries as normal.\\nThe following command pushes the image to one of my repos in Docker Hub. Be sure to\\ntag the image with your own Docker username.\\n$ docker push nigelpoulton/ddd-book:wasm\\nThe push refers to repository [docker.io/nigelpoulton/ddd-book]\\n301823195c36: Pushed\\n8966226af76a: Pushed\\nwasm: digest: sha256:7b55889f1006285ed6c394dcc7a56aca8955c107587b2216340e592299b8ae4c size: 695\\nIf you look at Docker Hub, you can see Docker Hub has recognized it as awasi/wasm\\nimage. You’ll also see there’s no vulnerability analysis data. This is because image\\nscanning tools can’t analyze Wasm images yet.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 191}, page_content='12: Docker and WebAssembly 185\\nFigure 12.4. Wasm image on Docker Hub\\nRun a Wasm container\\nNow that you’ve packaged the Wasm app as an OCI image and pushed it to a registry,\\nyou can run it as a container.\\nThe following command runs it in a new container calledwasm-ctr and maps it to\\nport 5556 on your Docker host. The--runtime flag makes sure Docker executes the\\ncontainer with thespin Wasm runtime. Versions of Docker Desktop older than 4.24\\nmay not have thespin runtime and will fail.\\n$ docker run -d --name wasm-ctr \\\\\\n--runtime=io.containerd.spin.v2 \\\\\\n--platform=wasi/wasm \\\\\\n-p 5556:80 \\\\\\nnigelpoulton/ddd-book:wasm /\\nYou can check it’s running with a regulardocker ps command.\\nConnect your browser tohttp://localhost:5556/hello to see the app.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 192}, page_content='12: Docker and WebAssembly 186\\nFigure 12.5. Wasm app running in container\\nCongratulations, your Wasm app is running inside a Wasm container.\\nClean up\\nRun the following commands to delete the container and the local image.\\n$ docker rm wasm-ctr -f\\nwasm-ctr\\n$ docker rmi nigelpoulton/ddd-book:wasm\\nUntagged: nigelpoulton/ddd-book:wasm\\nDeleted: sha256:7b55889f1006285ed6c394dcc7a56aca8955c107587b2216340e592299b8ae4c\\nYou’ll still have a copy of the image on Docker Hub and the spin app in your local\\nfilesystem. Feel free to delete these as well.\\nChapter summary\\nIn this chapter, you containerized a Wasm app and ran it in a Wasm container.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 193}, page_content='12: Docker and WebAssembly 187\\nWasm is a new technology driving the next wave of cloud computing. Wasm apps\\nare smaller, faster, more secure, and more portable than traditional Linux containers.\\nHowever, they’re not as flexible. For example, at the time of writing, Wasm apps aren’t\\ngreat for apps with heavy I/O requirements or complex networking. This will change\\nquickly as the Wasm ecosystem is evolving fast.\\nFortunately, Docker already works with Wasm, and Docker Desktop ships with lots\\nof popular Wasm runtimes. This means you can use industry-standard tools such as\\ndocker build and docker run to containerize and run Wasm apps. You can even push\\nthem to OCI registries such as Docker Hub.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 194}, page_content='13: Docker Networking\\nIt’s always the network!\\nAny time we experience infrastructure issues, we always blame the network. One of the\\nreasons we do this is that networks are at the center of everything. With this in mind, it’s\\nimportant you have a strong understanding of Docker networking.\\nIn the early days of Docker, networking was hard. Fortunately, these days it’s almost a\\npleasure ;-)\\nThis chapter will get you up to speed with the fundamentals of Docker networking.\\nYou’ll learn all the theory behind theContainer Network Model (CNM)and libnetwork, and\\nyou’ll get your hands dirty with lots of examples. You’ll learn about overlay networks in\\nthe next chapter.\\nI’ve divided the chapter into the following sections:\\n• Docker networking – the TLDR\\n• Docker networking theory\\n• Single-host bridge networks\\n• External access via port mappings\\n• Connecting to existing networks and VLANs\\n• Service Discovery\\n• Ingress load balancing\\nA few quick things before we start.\\nEverything we’ll cover relates to Linux containers, and I recommend you follow along\\nusing something like Multipass or Play with Docker, as they give you easy access to\\nsome of the Linux commands we’ll use. I don’t recommend following along on Docker\\nDesktop as it runs everything inside a Linux VM and you won’t have access to the Linux\\ncommands.\\nSome of the examples explain how networking works on a swarm. You’ll only be able to\\nfollow these if you’re following along with a Swarm cluster.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 195}, page_content='13: Docker Networking 189\\nDocker Networking – The TLDR\\nDocker runs microservices applications comprised of many containers that work\\ntogether to form the overall app. These containers need to be able to communicate,\\nand some will have to connect with external services, such as physical servers, virtual\\nmachines, or something else.\\nFortunately, Docker has solutions for both of these requirements.\\nDocker networking is based onlibnetwork, which is the reference implementation of an\\nopen-source architecture called theContainer Network Model (CNM).\\nFor a smooth out-of-the-box experience, Docker ships with everything you need\\nfor the most common networking requirements, including multi-host container-to-\\ncontainer networks and options for plugging into existing VLANs. However, the model\\nis pluggable, and the ecosystem can extend Docker’s networking capabilities via drivers\\nthat plug into libnetwork.\\nLast but not least, libnetwork also provides native service discovery and basic load\\nbalancing.\\nThat’s the big picture. Let’s get into the detail.\\nDocker networking theory\\nAt the highest level, Docker networking is based on the following three components:\\n• The Container Network Model (CNM)\\n• Libnetwork\\n• Drivers\\nThe CNMis the design specification and outlines the fundamental building blocks of a\\nDocker network.\\nLibnetwork is a real-world implementation of the CNM. It’s open-sourced as part of\\nthe Moby project18 and used by Docker and other platforms.\\nDrivers extend the model by implementing specific network topologies such as VXLAN\\noverlay networks.\\nFigure 13.1 shows all three.\\n18https://mobyproject.org/'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 196}, page_content='13: Docker Networking 190\\nFigure 13.1\\nLet’s take a closer look at each.\\nThe Container Network Model (CNM)\\nEverything starts with a design.\\nThe design guide for Docker networking is the CNM that outlines the fundamental\\nbuilding blocks of a Docker network.\\nI recommend you read thespecification document19, but at a high level, it defines three\\nbuilding blocks:\\n• Sandboxes\\n• Endpoints\\n• Networks\\nA sandbox is an isolated network stack inside a container. It includes Ethernet interfaces,\\nports, routing tables, DNS configuration, and everything else you’d expect from a\\nnetwork stack.\\nEndpoints are virtual network interfaces that look, smell, and feel like regular network\\ninterfaces. They connect sandboxes to networks.\\nNetworks are virtual switches (usually software implementations of an 802.1d bridge). As\\nsuch, they group together and isolate one or more endpoints that need to communicate.\\nFigure 13.2 shows how all three connect and relate to familiar infrastructure compo-\\nnents. Using CNM terminology,endpoints connect sandboxes to networks. Every container\\nyou create will have a sandbox with at least one endpoint connecting it to a network.\\n19https://github.com/moby/moby/blob/master/libnetwork/docs/design.md'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 197}, page_content='13: Docker Networking 191\\nFigure 13.2 The Container Network Model (CNM)\\nAs the name suggests, theContainer Network Model is all about providing networking\\nfor containers. Figure 13.3 shows how CNM components relate to containers —\\neach container gets its ownsandbox which hosts the container’s entire network stack,\\nincluding one or more endpoints that act as Ethernet interfaces and can be connected to\\nnetworks.\\nFigure 13.3\\nContainer Ahas a single interface (endpoint) and is only connected toNetwork A.\\nHowever, Container Bhas two interfaces connected toNetwork Aand Network B.\\nThe containers can communicate with each other because they are both connected to\\nNetwork A. However, the two endpoints inside ofContainer Bcannot communicate\\nwith each other as they’re on different networks.\\nIt’s also important to understand that endpoints behave exactly like regular network\\nadapters, meaning you can only connect them to a single network. This is whyCon-\\ntainer Bneeds two endpoints if it wants to connect to both networks.\\nFigure 13.4 extends the diagram further by adding the Docker host. Even though both\\ncontainers are running on the same host this time, their network stacks are completely\\nisolated and can only communicate via a network.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 198}, page_content='13: Docker Networking 192\\nFigure 13.4\\nLibnetwork\\nLibnetwork is the reference implementation of the CNM. It’s open-source, cross-\\nplatform (Linux and Windows), maintained by the Moby project, and used by Docker.\\nBefore Docker created libnetwork, it implemented all of its networking code inside\\nthe daemon. However, over time, the daemon became bloated and difficult for other\\nprojects to use. As a result, Docker removed the networking code from the daemon and\\nrefactored it as an external library calledlibnetwork based on the CNM design. Today,\\nDocker implements all of its core networking in libnetwork.\\nAs well as implementing the core components of the CNM, libnetwork also implements\\nthe network control plane, including management APIs, service discovery, and ingress-\\nbased container load balancing.\\nDrivers\\nLibnetwork implements the control plane, but it relies on drivers to implement the data\\nplane. For example, drivers are responsible for creating networks and ensuring isolation\\nand connectivity.\\nDocker ships with several built-in drivers that we sometimes callnative driversor local\\ndrivers. These includebridge, overlay, andmacvlan, and they build the most common\\nnetwork topologies. Third parties can also write network drivers to implement other\\nnetwork topologies and more advanced configurations.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 199}, page_content='13: Docker Networking 193\\nFigure 13.5 shows the roles of libnetwork and drivers and how they relate to control\\nplane and data place responsibilities.\\nFigure 13.5\\nEvery network you create is owned by a driver, and the driver creates and manages\\neverything about the network. For example, if you create an overlay network called\\nprod-fe-cuda, Docker will invoke the overlay driver to create the network and its\\nresources.\\nTo meet the demands of complex, highly fluid environments, a single Docker host or\\nSwarm cluster can have multiple heterogeneous networks managed by different drivers.\\nLet’s look at single-host bridge networks and connecting to existing networks. You’ll\\nlearn about overlay networks in the next chapter.\\nSingle-host bridge networks\\nThe simplest type of Docker network is thesingle-host bridge network.\\nThe name tells us two things:\\n• Single-host tells us the network only spans a single Docker host\\n• Bridge tells us that it’s an implementation of an 802.1d bridge (layer 2 switch)\\nDocker creates single-host bridge networks with the built-inbridge driver. If you run\\nWindows containersyou’ll need to use thenat driver, but for all intents and purposes they\\nwork the same.\\nFigure 13.6 shows two Docker hosts with identical local bridge networks, both called\\nmynet. Even though the networks are identical, they are independent and isolated,'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 200}, page_content='13: Docker Networking 194\\nmeaning the containers in the picture cannot communicate, even if the nodes are part\\nof the same swarm.\\nFigure 13.6\\nEvery new Docker host gets a default single-host bridge network calledbridge that\\nDocker connects new containers to unless you override it with the--network flag.\\nThe following commands show the output of adocker network ls command on\\nDocker installation.\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\nc7464dce29ce bridge bridge local <<---- Default on all Docker hosts\\nc65ab18d0580 host host local\\n42a783df0fbe none null local\\nAs always, you can rundocker inspect commands to get more information. I highly\\nrecommend running the command on your own system and studying the output.\\n$ docker network inspect bridge\\n[\\n{\\n\"Name\": \"bridge\",\\n\"Id\": \"c7464dce2...ba2e3b8\",\\n\"Scope\": \"local\",\\n\"Driver\": \"bridge\",\\n\"EnableIPv6\": false,\\n\"IPAM\": {\\n\"Driver\": \"default\",\\n\"Options\": null,\\n\"Config\": [\\n{\\n\"Subnet\": \"172.17.0.0/16\",\\n\"Gateway\": \"172.17.0.1\"\\n}'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 201}, page_content='13: Docker Networking 195\\n]\\n},\\n\"Internal\": false,\\n\"Attachable\": false,\\n\"Ingress\": false,\\n\"ConfigFrom\": {\\n\"Network\": \"\"\\n},\\n<Snip>\\n}\\n]\\nAll bridge networks are based on the battle-hardenedLinux bridgetechnology that has\\nexisted in the Linux kernel for over 20 years. This means they’re high-performance and\\nhighly stable. It also means you can inspect them using standard Linux utilities.\\nThe defaultbridge network on all Linux-based Docker hosts is calledbridge and maps to\\nan underlyingLinux bridgein the host’s kernel calleddocker0. This is shown in Figure\\n13.7.\\nFigure 13.7 - Mapping the default Docker “bridge” network to the “docker0” bridge in the host’s kernel\\nYou can run adocker network inspect command to confirm that thebridge network\\nis based on thedocker0 bridge in the host’s kernel. If you’re on Windows using Power-\\nShell, you’ll need to replace\\x18grep** with **SelectString‘.\\n$ docker network inspect bridge | grep bridge.name\\n\"com.docker.network.bridge.name\": \"docker0\",\\nNow run these Linux commands to inspect thedocker 0bridge from the Linux host.\\nYou might need to manually install thebrctl utility.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 202}, page_content='13: Docker Networking 196\\n$ brctl show\\nbridge name bridge id STP enabled interfaces\\ndocker0 8000.0242aff9eb4f no\\ndocker_gwbridge 8000.02427abba76b no\\n$ ip link show docker0\\n3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc...\\nlink/ether 02:42:af:f9:eb:4f brd ff:ff:ff:ff:ff:ff\\nThe first command lists all thebridges on your Docker host and shows if they have any\\ndevices connected to them. The example in the book shows thedocker0 bridge with no\\ndevices connected in theinterfaces column. You’ll only see thedocker_gwbridge if\\nyour host is a member of a swarm cluster.\\nThe second command shows the configuration and state of thedocker0 bridge.\\nFigure 13.8 shows the complete stack with containers connecting to thebridge\\nnetwork, which, in turn, maps to thedocker0 Linux bridge in the host’s kernel. It also\\nshows how you can use port mappings to publish connected devices on the Docker\\nhost’s interface. More on port mappings later.\\nFigure 13.8\\nIn the next few steps, you’ll complete all of the following:\\n1. Create a new Docker bridge network\\n2. Connect a container to the new network'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 203}, page_content='13: Docker Networking 197\\n3. Inspect the new network\\n4. Test name-based discovery\\nRun the following command to create a new single-host bridge network calledlocal-\\nnet.\\n$ docker network create -d bridge localnet\\nf918f1bb0602373bf949615d99cb2bbbef14ede935fbb2ff8e83c74f10e4b986\\nThe long number returned by the command is the network’s ID and you’ll need it in the\\nnext step.\\nAs expected, the command creates a new Docker bridge network calledlocalnet that\\nyou can list and inspect with the usualdocker commands. However, behind the scenes,\\nit also creates a new Linux bridge in the host’s kernel.\\nRun anotherbrctl show command to see it.\\n$ brctl show\\nbridge name bridge id STP enabled interfaces\\nbr-f918f1bb0602 8000.0242372a886b no\\ndocker0 8000.024258ee84bc no\\ndocker_gwbridge 8000.02427abba76b no\\nThe example in the book shows a new bridge calledbr-f918f1bb0602 with no devices\\nconnected. If you look closely at the name, you’ll recognizef918f1bb0602 as the first 12\\ncharacters from the ID of the newlocalnet network you just created.\\nAt this point, the bridge configuration on the host looks like Figure 13.9, with three\\nDocker networks and three associated bridges in the host’s kernel.\\nFigure 13.9\\nLet’s create a new container calledc1 and attach it to the newlocalnet bridge network.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 204}, page_content='13: Docker Networking 198\\n$ docker run -d --name c1 \\\\\\n--network localnet \\\\\\nalpine sleep 1d\\nOnce you’ve created the container, inspect thelocalnet network and verify the\\ncontainer is connected to it. You’ll need thejq utility installed for the command to work.\\nLeave off the\"| jq\" if it doesn’t work.\\n$ docker network inspect localnet --format \\'{{json .Containers}}\\' | jq\\n{\\n\"09c5f4926c87da12039b3b510a5950b3fe9db80e13431dc17d870450a45fd84a\": {\\n\"Name\": \"c1\",\\n\"EndpointID\": \"27770ac305773b352d716690fb9f8e05c1b71e10dc66f67b88e93cb923ab9749\",\\n\"MacAddress\": \"02:42:ac:15:00:02\",\\n\"IPv4Address\": \"172.21.0.2/16\",\\n\"IPv6Address\": \"\"\\n}\\n}\\nThe output shows thec1 container and its IP address. This proves Docker connected it\\nto the network.\\nIf you run anotherbrctl show command, you’ll see thec1 container’s interface\\nconnected to thebr-1597657726bc bridge.\\n$ brctl show\\nbridge name bridge id STP enabled interfaces\\nbr-f918f1bb0602 8000.0242372a886b no veth833aaf9\\ndocker0 8000.024258ee84bc no\\ndocker_gwbridge 8000.02427abba76b no\\nFigure 13.10 shows the updated configuration. Your veth IDs will be different, but the\\nimportant thing to understand is that every veth is like a cable with an interface on\\neither end. One end is connected to the Docker network, and the other end is connected\\nto the associated bridge in the kernel.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 205}, page_content='13: Docker Networking 199\\nFigure 13.10\\nIf you add more containers to thelocalnet network, they’ll all be able to communicate\\nusing names. This is because Docker automatically registers container names with an\\ninternal DNS service and allows containers on the same network to find each other by\\nname. The exception to this rule is the built-inbridge network that does not support\\nDNS resolution.\\nLet’s test name resolution by creating a new container calledc2 on the samelocalnet\\nnetwork and seeing if it can ping thec1 container.\\nRun the following command to create thec2 container on thelocalnet network. You’ll\\nneed to typeexit if you’re still logged in to thec1 container.\\n$ docker run -it --name c2 \\\\\\n--network localnet \\\\\\nalpine sh\\nYour terminal will switch into thec2 container.\\nTry to ping thec1 container by name.\\n# ping c1\\nPING c1 (172.21.0.2): 56 data bytes\\n64 bytes from 172.21.0.2: seq=0 ttl=64 time=1.564 ms\\n64 bytes from 172.21.0.2: seq=1 ttl=64 time=0.338 ms\\n64 bytes from 172.21.0.2: seq=2 ttl=64 time=0.248 ms\\n<Control-c>\\nIt works! This is because all containers run a DNS resolver that forwards name lookups\\nto Docker’s internal DNS server that holds name-to-IP mappings for all containers\\nstarted with the--name or --net-alias flag.\\nType exit to log out of the container and return to your local shell.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 206}, page_content='13: Docker Networking 200\\nExternal access via port mappings\\nSo far, we’ve said that containers on bridge networks can only communicate with\\nother containers on the same network. However, you can get around this by mapping\\ncontainers to ports on the Docker host. It’s a bit clunky and has a lot of limitations, but it\\nmight be useful for occasional testing and development work.\\nFigure 13.11 shows a single Docker host running two containers. Theweb container on\\nthe right is running a web server on port80 that is mapped to port5005 on the Docker\\nhost. Theclient container on the left is sending requests to the Docker host on port\\n5005 and the external client at the bottom is doing the same. Both requests will hit the\\nhost’s IP on port5005 and be redirected to the web server running in theweb container.\\nFigure 13.11\\nLet’s test the setup to see if it works.\\nCreate a new container calledweb running NGINX on port80 and map it to port5005\\non the Docker host. If you’re still logged on to the container from the previous example,\\nyou’ll need to typeexit first.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 207}, page_content='13: Docker Networking 201\\n$ docker run -d --name web \\\\\\n--network localnet \\\\\\n--publish 5005:80 \\\\\\nnginx\\nVerify the port mapping.\\n$ docker port web\\n80/tcp -> 0.0.0.0:5005\\n80/tcp -> [::]:5005\\nThe output shows the port mapping exists on all interfaces on the Docker host.\\nYou can test external access by pointing a web browser to the Docker host on port5005.\\nYou’ll need to know the IP or DNS name of your Docker host (if you’re following along\\non Multipass it will probably be your Multipass VM’s192.168.x.x address). You’ll see\\nthe Welcome to nginx!page.\\nLet’s create another container and see if it can reach the web container via the port\\nmapping.\\nRun the following command to create a new container calledclient on thebridge\\nnetwork.\\n$ docker run -it --name client --network bridge alpine sh\\n#\\nThe command will log you into the container and your prompt will change.\\nInstall thecurl utility.\\n# apk add curl\\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.19/main/aarch64/APKINDEX.tar.gz\\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.19/community/aarch64/APKINDEX.tar.gz\\n(1/8) Installing ca-certificates (20240226-r0)\\n<Snip>\\nNow connect to the IP of your Docker host on port5005 to see if you can reach the\\ncontainer.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 208}, page_content='13: Docker Networking 202\\n# curl 192.168.64.69:5005\\n<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Welcome to nginx!</title>\\n...\\n</html>\\nYou’ve reached the NGINX web server running on thec1 container via a port mapping\\nto the Docker host’s IP.\\nEven though this works, it’s clunky and doesn’t scale. For example, no other containers\\nor host processes will be able to use port5005 on the host. This is one of the reasons\\nthat single-host bridge networks are only useful for local development or very small\\napplications.\\nConnecting to existing networks and VLANs\\nThe ability to connect containerized apps to external systems and physical networks is\\nimportant. A common example is partially containerized apps where the parts running\\nin containers need to be able to communicate with the parts not running in containers.\\nThe built-inMACVLAN driver (transparent if you’re using Windows containers) was\\ncreated with this in mind. It gives every container its own IP and MAC address on the\\nexternal physical network, making each one look, smell, and feel like a physical server or\\nVM. This is shown in Figure 13.12.\\nFigure 13.12 - MACVLAN driver making containers visible on external networks\\nOn the positive side, MACVLAN performance is good as it doesn’t require port\\nmappings or additional bridges. However, you need to run your host NICs inpromis-\\ncuous mode, which isn’t allowed on many corporate networks and public clouds. So,\\nMACVLAN will work on your data center networksif your network team allows\\npromiscuous mode, but it probably won’t work on your public cloud.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 209}, page_content='13: Docker Networking 203\\nLet’s dig a bit deeper with the help of some pictures and a hypothetical example. This\\nexample will only work if your host NIC is in promiscuous mode on a network that\\nallows it. It also requires an existing VLAN 100. You can adapt it if the VLAN config on\\nyour physical network is different. You can follow along without the VLANs, but you\\nwon’t get the full experience.\\nAssume you have the network shown in Figure 13.13 with two VLANs:\\nFigure 13.13\\nNext, you add a Docker host and connect it to the network.\\nFigure 13.14\\nNow comes the requirement to attach a container to VLAN 100. To do this, you create a\\nnew Docker network with themacvlan driver and configure it with all of the following:\\n• Subnet info\\n• Gateway\\n• Range of IPs it can assign to containers\\n• Which of the host’s interfaces or sub-interfaces to use\\nRun the following command to create a new MACVLAN network calledmacvlan100\\nthat will connect containers to VLAN 100. You may need to change the name of the'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 210}, page_content='13: Docker Networking 204\\nparent interface to match the parent interface name on your system. For example,\\nchanging -o parent=eth0.100 to -o parent=enp0s1.100. The parent interface must\\nbe connected to the VLAN, and you’ll need to typeexit if you’re still logged on to the\\ncontainer from the previous example.\\n$ docker network create -d macvlan \\\\\\n--subnet=10.0.0.0/24 \\\\\\n--ip-range=10.0.0.0/25 \\\\\\n--gateway=10.0.0.1 \\\\\\n-o parent=eth0.100 \\\\ <<---- Make sure this matches your system\\nmacvlan100\\nDocker will create themacvlan100 network and a new sub-interface on the host called\\neth0.100@eth0. The config now looks like this.\\nFigure 13.15\\nThe MACVLAN driver creates standard Linuxsub-interfaces and tags them with the ID\\nof the VLAN they will connect to. In this example, we’re connecting to VLAN 100, so we\\ntag the sub-interface with.100 (-o parent=eth0.100).\\nWe also used the--ip-range flag to tell the new network which sub-set of IP addresses\\nit can assign to containers. It’s vital that you reserve this range of addresses for Docker,'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 211}, page_content='13: Docker Networking 205\\nas the MACVLAN driver has no management plane feature to check if IPs are already in\\nuse.\\nIf you inspect the network, you’ll be able to see the important configuration information.\\nI’ve snipped the output to show the most relevant parts.\\n$ docker network inspect macvlan100\\n[\\n{\\n\"Name\": \"macvlan100\",\\n\"Driver\": \"macvlan\",\\n\"IPAM\": {\\n\"Config\": [\\n{\\n\"Subnet\": \"10.0.0.0/24\",\\n\"IPRange\": \"10.0.0.0/25\",\\n\"Gateway\": \"10.0.0.1\"\\n}\\n]\\n},\\n\"Options\": {\\n\"parent\": \"enp0s1.100\"\\n},\\n}\\n]\\nOnce you’ve created themacvlan100 network, you can connect containers to it and\\nDocker will assign the IP and MAC addresses on the underlying VLAN so they’ll be\\nvisible to other systems.\\nThe following command creates a new container calledmactainer1 and connects it to\\nthe macvlan100 network.\\n$ docker run -d --name mactainer1 \\\\\\n--network macvlan100 \\\\\\nalpine sleep 1d\\nThe config now looks like Figure 13.16.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 212}, page_content='13: Docker Networking 206\\nFigure 13.16\\nHowever, remember that the underlying network (VLAN 100) does not see any of\\nthe MACVLAN magic, it only sees the container with its MAC and IP addresses,\\nmeaning themactainer1 container will be able to communicate with every other system\\nconnected to VLAN 100!\\nNote: If you can’t get this to work, it might be because your host NIC isn’t\\nin promiscuous mode. Also, remember that public cloud platforms normally\\nblock promiscuous mode.\\nAt this point, you’ve got a MACVLAN network and used it to connect a new container\\nto an existing VLAN. If you have the complete setup, with the existing VLAN, you can\\ntest that the container is reachable form other system on the VLAN.\\nHowever, it doesn’t stop there. The Docker MACVLAN driver supports VLAN trunking.\\nThis means you can create multiple MACVLAN networks that connect to different\\nVLANs. Figure 13.17 shows a single Docker host running two MACVLAN networks\\nconnecting containers to two different VLANs.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 213}, page_content='13: Docker Networking 207\\nFigure 13.17\\nTroubleshooting connectivity problems\\nA quick note on troubleshooting connectivity issues before moving on to service\\ndiscovery.\\nDaemon logs and container logs can be useful when troubleshooting connectivity issues.\\nIf you’re running Windows containers, you can view them in the Windows Event\\nViewer or directly in\\x18\\\\AppData\\\\Local\\\\Docker. For Linux containers, it depends on\\nwhich init system you’re using. If you’re running a systemd, Docker will post logs to\\njournald and you can view them with thejournalctl -u docker.service command.\\nIf you’re using a different init system, you might want to check the following locations:\\n• Ubuntu systems runningupstart: /var/log/upstart/docker.log\\n• RHEL-based systems:/var/log/messages\\n• Debian: /var/log/daemon.log\\nYou can also tell Docker how verbose you want daemon logging to be. To do this, edit\\nthe daemon config file at/etc/docker/daemon.json and set\"debug\" to \"true\" and\\n\"log-level\" to one of the following:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 214}, page_content='13: Docker Networking 208\\n• debug – the most verbose option\\n• info – the default value and second-most verbose option\\n• warn – third most verbose option\\n• error – fourth most verbose option\\n• fatal – least verbose option\\nThe following snippet from adaemon.json enables debugging and sets the level to\\ndebug. It will work on all Docker platforms.\\n{\\n<Snip>\\n\"debug\":true,\\n\"log-level\":\"debug\",\\n<Snip>\\n}\\nIf yourdaemon.json file doesn’t exist, create it. Also, be sure to restart Docker after\\nmaking any changes to the file.\\nThat was the daemon logs. What about container logs?\\nYou can normally view container logs with thedocker logs command. If you’re\\nrunning Swarm, you should use thedocker service logs command. However,\\nDocker supports a few different log drivers, and they don’t all work with native Docker\\ncommands. For some of them, you might have to view logs using the platform’s native\\ntools.\\njson-file and journald are probably the easiest to configure and they both work with\\nthe docker logs and docker service logs commands.\\nThe following snippet from adaemon.json shows a Docker host configured to use\\njournald.\\n{\\n\"log-driver\": \"journald\"\\n}\\nYou can also start a container or a service with the--log-driver and --log-opts flags\\nto override the settings indaemon.json.\\nContainer logs work on the premise that your application runs as PID 1 and sends logs\\nto STDOUT and errors toSTDERR. The logging driver then forwards everything to the\\nlocations configured via the logging driver.\\nThe following is an example of running thedocker logs command against a container\\ncalled vantage-db that is configured with thejson-file logging driver.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 215}, page_content='13: Docker Networking 209\\n$ docker logs vantage-db\\n1:C 2 Feb 09:53:22.903 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\\n1:C 2 Feb 09:53:22.904 # Redis version=4.0.6, bits=64, commit=00000000, modified=0, pid=1\\n1:C 2 Feb 09:53:22.904 # Warning: no config file specified, using the default config.\\n1:M 2 Feb 09:53:22.906 * Running mode=standalone, port=6379.\\n1:M 2 Feb 09:53:22.906 # WARNING: The TCP backlog setting of 511 cannot be enforced...\\n1:M 2 Feb 09:53:22.906 # Server initialized\\n1:M 2 Feb 09:53:22.906 # WARNING overcommit_memory is set to 0!\\nThere’s a good chance you’ll find network connectivity errors in the daemon logs or\\ncontainer logs.\\nService discovery\\nAs well as core networking,libnetwork also providesservice discoverythat allows all\\ncontainers and Swarm services to locate each other by name. The only requirement is\\nthat the containers be on the same network.\\nUnder the hood, Docker implements a native DNS server and configures every con-\\ntainer to use it for name resolution.\\nFigure 13.18 shows a container calledc1 pinging another container calledc2 by name.\\nThe same principle applies to Swarm service replicas.\\nFigure 13.18\\nLet’s step through the process.\\n• Step 1:The c1 container issues aping c2 command. The container’s local DNS\\nresolver checks its cache to see if it has an IP address forc2. All Docker containers\\nhave a local DNS resolver.\\n• Step 2:The local resolver doesn’t have an IP address forc2, so it initiates a\\nrecursive query to the embedded Docker DNS server. All Docker containers are\\npre-configured to know how to send queries to the embedded DNS server.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 216}, page_content='13: Docker Networking 210\\n• Step 3:The Docker DNS server maintains name-to-IP mappings for every\\ncontainer you create with the--name or --net-alias flags. This means it knows\\nthe IP address of thec2 container.\\n• Step 4:The DNS server returns the IP address of thec2 container to the local\\nresolver in thec1 container. Ifc1 and c2 are on different Docker networks it won’t\\nreturn the IP address — name resolution only works for containers on the same\\nnetwork.\\n• Step 5:The c1 container sends the ping request (ICMP echo request) to the IP\\naddress ofc2.\\nJust to confirm a few points.\\nDocker will automatically register the name and IP of every container you create\\nwith the--name or net-alias flag with the embedded Docker DNS service. It also\\nautomatically configures every container to use the embedded DNS service to convert\\nnames to IPs. And name resolution (service discovery) isnetwork scoped, meaning it only\\nworks for containers and services on the same network.\\nOne last point on service discovery and name resolution…\\nYou can use the--dns flag to start containers and services with a customized list of\\nDNS servers, and you can use the--dns-search flag to add custom search domains\\nfor queries against unqualified names (i.e., when the application doesn’t specify fully\\nqualified DNS names for services they consume). You’ll find both of these useful if your\\napplications query names outside of your Docker environment such as internet services.\\nBoth of these options work by adding entries to the container’s/etc/resolv.conf file.\\nRun the following command to start a new container with the infamous8.8.8.8\\nGoogle DNS server andnigelpoulton.com as a search domain for unqualified queries.\\n$ docker run -it --name custom-dns \\\\\\n--dns=8.8.8.8 \\\\\\n--dns-search=nigelpoulton.com \\\\\\nalpine sh\\nYour shell prompt will change to indicate you’re connected to the container.\\nInspect its/etc/resolv.conf file.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 217}, page_content='13: Docker Networking 211\\n# cat /etc/resolv.conf\\nGenerated by Docker Engine.\\nThis file can be edited; Docker Engine will not make further changes once it\\nhas been modified.\\nnameserver 8.8.8.8\\nsearch nigelpoulton.com\\nThe file’s contents might be slightly different if you connect the container to a custom\\nnetwork, but the options work the same.\\nType exit to return to your local terminal.\\nIngress load balancing\\nThis section only applies to Docker Swarm.\\nSwarm supports two ways of publishing services to external clients:\\n• Ingress mode (default)\\n• Host mode\\nExternal clients can accessingress modeservices via any swarm node — even nodes not\\nhosting a service replica. However, they can only accesshost modeservices via nodes\\nrunning replicas. Figure 13.19 shows both modes.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 218}, page_content='13: Docker Networking 212\\nFigure 13.19\\nIngress mode is the default, meaning any time you create a service with-p or --publish,\\nDocker will publish it iningress mode. If you want to publish a service inhost mode, you’ll\\nneed to use the--publish flag with themode=host option. The following example\\npublishes a service in host mode and will only work on a swarm.\\n$ docker service create -d --name svc1 \\\\\\n--publish published=5005,target=80,mode=host \\\\\\nnginx\\nA few notes about the command.docker service create lets you publish services\\nusing eitherlong form syntaxor short form syntax.\\nThe short form looks like-p 5005:80 and you’ve seen it a few times already. However,\\nyou cannot publish a service inhost modeusing the short form.\\nLong form looks like this:--publish published=5005,target=80,mode=host. It’s a\\ncomma-separated list with no whitespace after the commands, and the options work as\\nfollows:\\n• published=5005 makes the service available to external clients via port5005\\n• target=80 makes sure requests hitting thepublished port get mapped back to port\\n80 on service replicas'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 219}, page_content='13: Docker Networking 213\\n• mode=host makes sure requests will only reach the service if they arrive on nodes\\nrunning a service replica\\nYou’ll almost always use ingress mode.\\nBehind the scenes, ingress mode uses a layer 4 routing mesh that Docker calls the\\nservice meshor theswarm-mode service mesh. Figure 13.20 shows the basic traffic\\nflow when an external request hits the cluster for a service exposed in ingress mode.\\nFigure 13.20\\nLet’s quickly walk through the diagram.\\nThe command at the top deploys a new Swarm service calledsvc1 with one replica,\\nattaches it to theovernet network and publishes it on port5005 on theingress network.\\nDocker automatically creates the ingress network when you create the swarm, and\\nit attaches every node to it. The act of publishing the service on port5005 makes it\\naccessible via port5005 on every swarm node because every node is connected to the\\ningress network. Docker also creates a swarm-wide rule to route all traffic hitting nodes\\non port5005 to port80 in the svc1 replicas via the ingress network.\\nNow let’s track that external request.\\n1. The external client sends a request toNode 1on port5005\\n2. Node 1 receives the request and knows to forward traffic arriving on port5005 to\\nthe ingress network\\n3. The ingress network forwards the request toNode 2which is running a replica\\n4. Node 2receives the request and passes it to the replica on port80'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 220}, page_content='13: Docker Networking 214\\nIf the service has multiple replicas, swarm is clever enough to balance requests across\\nthem all.\\nClean-up\\nIf you’ve been following along, you’ll have a lot of containers, networks, and services\\nthat you probably want to clean up.\\nRun the following command to delete the services you created.\\n$ docker service rm svc1\\nNow, delete the standalone containers you created.\\n$ docker rm c1 c2 client web mactainer1 -f\\nFinally, delete the networks you created.\\n$ docker network rm localnet macvlan100\\nDocker Networking – The Commands\\nDocker networking has its owndocker network sub-command, and the main com-\\nmands include:\\n• docker network ls lists all the Docker networks available to the host.\\n• docker network create is how you create a new Docker network. You have\\nto give the network a name and you can use the-d flag to specify which driver\\ncreates it.\\n• docker network inspect provides detailed configuration information about\\nDocker networks.\\n• docker network prune deletes all unused networks on a Docker host.\\n• docker network rm Deletes specific networks on a Docker host or swarm.\\nYou also ran some native Linux commands.\\n• brctl show prints a list of all kernel bridges on the Docker host and shows if any\\ncontainers are connected.\\n• ip link show prints bridge configuration data. You ran anip link show\\ndocker0 to see the configuration of thedocker0 bridge on your Docker host.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 221}, page_content='13: Docker Networking 215\\nChapter Summary\\nThe Container Network Model (CNM) is the design document for Docker networks\\nand defines the three major constructs —sandboxes, endpoints, andnetworks.\\nLibnetwork is the reference implementation of the CMN and is an open-source project\\nmaintained by the Moby project. Docker uses it to implement its core networking,\\nincluding control plane services such as service discovery.\\nDrivers extend the capabilities of libnetwork by implementing specific network topolo-\\ngies, such as bridge and overlay networks. Docker ships with built-in drivers, but you\\ncan also use third-party drivers.\\nSingle-host bridge networksare the most basic type of Docker network but are only\\nsuitable for local development and very small applications. They do not scale, and you\\nneed to map containers to host ports if you want to publish services outside of the\\nnetwork.\\nOverlay networksare all the rage and are excellent container-only multi-host networks.\\nWe’ll talk about them in-depth in the next chapter.\\nThe macvlan driver lets you create Docker networks that connect containers to existing\\nphysical networks and VLANs. They make containers first-class citizens on external\\nnetworks by giving them their own MAC and IP addresses. Unfortunately, you have to\\nrun your host NICs in promiscuous mode, meaning they won’t work in public clouds.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 222}, page_content='14: Docker overlay networking\\nOverlay networks are at the center of most cloud-native microservices apps, and this\\nchapter will get you up to speed on how they work in Docker.\\nI’ve divided the chapter into the following sections:\\n• Docker overlay networking – The TLDR\\n• Docker overlay networking history\\n• Building and testing overlay networks\\n• Overlay networks explained\\nLet’s do some networking magic!\\nDocker overlay networking – The TLDR\\nReal-world containers need a reliable and secure way to communicate without caring\\nwhich host they’re running on or which networks those hosts are connected to. This\\nis where overlay networks come into play — they create flat, secure, layer 2 networks\\nthat span multiple hosts. Containers on different hosts can connect to the same overlay\\nnetwork and communicate directly.\\nDocker offers native overlay networking that is simple to configure and secure by\\ndefault.\\nBehind the scenes, Docker builds overlay networking on top oflibnetwork and the native\\noverlay driver. Libnetwork is the canonical implementation of the Container Network\\nModel (CNM), and theoverlay driver implements all of the machinery to build overlay\\nnetworks.\\nDocker overlay networking history\\nIn March 2015, Docker, Inc. acquired a container networking startup calledSocket Plane\\nwith two goals in mind:\\n1. Bring overlay networking to Docker'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 223}, page_content='14: Docker overlay networking 217\\n2. Make container networking simple for developers\\nThey accomplished both goals, and overlay networking continues to be at the heart of\\ncontainer networking in 2024 and the foreseeable future.\\nHowever, there’s a lot of complexity hiding behind the simple Docker commands.\\nKnowing the commands is probably enough if you’re a casual Docker user. However,\\nif you plan to use Docker in production, especially if you plan to use Swarm and Docker\\nnetworking, then the things we’ll cover will be vital.\\nBuilding and testing Docker overlay networks\\nYou’ll need at least two Docker nodes configured in a swarm to follow along. The\\nexamples in the book show the two nodes on different networks connected by a router,\\nbut yours can be on the same network. You can follow along with two Multipass VMs\\non the same laptop or computer, but any Docker configuration will work as long as the\\nnodes can communicate. I don’t recommend using Docker Desktop as you only get a\\nsingle node and won’t get the full experience.\\nFigure 14.1 shows the initial lab configuration. Remember, your nodes can be on\\nthe same network, this will just mean yourunderlay networkis simpler. We’ll explain\\nunderlay networks later.\\nFigure 14.1'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 224}, page_content='14: Docker overlay networking 218\\nBuild a Swarm\\nIf you’re following along, you’ll need a swarm because overlay networks leverage the\\nswarm’s key-value store and other security features.\\nThis section builds a two-node swarm with two Docker nodes callednode1 and node2.\\nIf you already have a swarm, you can skip this section.\\nYou’ll need to substitute the IP addresses and names with the values from your environ-\\nment. You’ll also need to ensure the following network ports are open between the two\\nnodes:\\n• 2377/tcp for management plane comms\\n• 7946/tcp and 7946/udp for control plane comms (SWIM-based gossip)\\n• 4789/udp for the VXLAN data plane\\nRun the following command onnode1.\\n$ docker swarm init\\nSwarm initialized: current node (1ex3...o3px) is now a manager.\\nThe command output includes adocker swarm join command. Copy this command\\nand run itnode2.\\n$ docker swarm join \\\\\\n--token SWMTKN-1-0hz2ec...2vye \\\\\\n172.31.1.5:2377\\nThis node joined a swarm as a worker.\\nYou now have a two-node Swarm withnode1 as a manager andnode2 as a worker.\\nCreate a new overlay network\\nLet’s create a new encrypted overlay network calleduber-net.\\nRun the following command from your manager node (node1).\\n$ docker network create -d overlay -o encrypted uber-net\\nvdu1yly429jvt04hgdm0mjqc6'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 225}, page_content='14: Docker overlay networking 219\\nThat’s it. You’ve created a brand-new encrypted overlay network. The network spans\\nboth nodes in the swarm and Docker uses TLS to encrypt it (AES in GCM mode). It also\\nrotates the encryption keys every 12 hours.\\nIf you don’t specify the-o encrypted flag, Docker will still encrypt the control plane\\n(management traffic) but won’t encrypt the data plane (application traffic). This can\\nbe important, as encrypting the data plane can decrease network performance by\\napproximately 10%.\\nList the networks onnode1.\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\n65585dda7500 bridge bridge local\\n7e368a1105c7 docker_gwbridge bridge local\\na38083cdab1c host host local\\n4dsqo7jc36ip ingress overlay swarm\\nd97e92a23945 none null local\\nvdu1yly429jv uber-net overlay swarm <<---- New overlay network\\nThe new network is at the bottom of the list calleduber-net and is scoped to the entire\\nswarm (SCOPE = swarm). This means it spans every node in the swarm. However, if you\\nlist networks onnode2 you won’t see theuber-net network. This is because Docker\\nonly extends overlay networks to worker nodes when they need them. In our example,\\nDocker will extend theuber-net network tonode2 when it runs a container that needs\\nit. This lazy approach to network deployment improves scalability by reducing the\\namount of network gossip on the swarm.\\nAttach a container to the overlay network\\nNow that you have an overlay network let’s connect a container to it.\\nBy default, you can only attach containers that are part ofswarm servicesto overlay\\nnetworks. If you want to addstandalone containers, you need to create the overlay with\\nthe --attachable flag.\\nThe example will create a swarm service calledtest with two replicas on theuber-net\\nnetwork. One replica will be deployed tonode1 and the other tonode2, causing Docker\\nto extend the overlay network tonode2.\\nRun the following commands fromnode1.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 226}, page_content='14: Docker overlay networking 220\\n$ docker service create --name test \\\\\\n--network uber-net \\\\\\n--replicas 2 \\\\\\nubuntu sleep infinity\\nCheck the status of the service.\\n$ docker service ps test\\nID NAME IMAGE NODE DESIRED STATE CURRENT STATE\\nsm1...1nw test.1 ubuntu:latest node1 Running Running\\ntro...kgk test.2 ubuntu:latest node2 Running Running\\nThe NODE column shows one replica running on each node.\\nSwitch over tonode2 and run adocker network ls to verify it can now see theuber-\\nnet network.\\nCongratulations. You’ve created a new overlay network spanning two nodes on separate\\nunderlay networks and attached two containers to it. You’ll appreciate the simplicity\\nof what you’ve done when we reach the theory section and learn about the outrageous\\ncomplexity going on behind the scenes!\\nTest the overlay network\\nFigure 14.2 shows the current setup with two containers running on different Docker\\nhosts but connected to the same overlay.\\nFigure 14.2'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 227}, page_content='14: Docker overlay networking 221\\nThe following steps will walk you through obtaining the container names and IP\\naddresses and then seeing if they can ping each other.\\nSwitch back tonode1 and run adocker network inspect to see the overlay network’s\\nsubnet information and any IP addresses it’s assigned to service replicas.\\n$ docker network inspect uber-net\\n[\\n{\\n\"Name\": \"uber-net\",\\n\"Id\": \"vdu1yly429jvt04hgdm0mjqc6\",\\n\"Scope\": \"swarm\",\\n\"Driver\": \"overlay\",\\n\"EnableIPv6\": false,\\n\"IPAM\": {\\n\"Driver\": \"default\",\\n\"Options\": null,\\n\"Config\": [\\n{\\n\"Subnet\": \"10.0.0.0/24\", <<---- Subnet info\\n\"Gateway\": \"10.0.0.1\" <<---- Subnet info\\n}\\n\"Containers\": {\\n\"Name\": \"test.1.tro80xqwm7k1bsyn3mt1fjkgk\", <<---- Replica ID\\n\"IPv4Address\": \"10.0.0.3/24\", <<---- Container IP\\n<Snip>\\n},\\n<Snip>\\nI’ve snipped the output and highlighted the subnet info and the IPs of connected con-\\ntainers. One thing to note is that Docker only shows you the IP addresses of containers\\nrunning on the local node. For example, the output in the book only shows the IP of the\\nfirst replica calledtest.1.tro...kgk. If you run the same command onnode2, you’ll\\nsee the name and IP of the other replica.\\nRun the following commands on both nodes to get the local container names, IDs, and\\nIP addresses of both replicas and make a note of them.\\nThe ID at the end of the second command (d7766923a5a7) is the container ID as\\nreturned by thedocker ps command. You’ll need to substitute the value from your\\nenvironment.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 228}, page_content='14: Docker overlay networking 222\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAME\\nd7766923a5a7 ubuntu:latest \"sleep infinity\" 2 hrs ago Up 2 hrs test.1.tro...kgk\\n$ docker inspect \\\\\\n--format=\\'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\\' d7766923a5a7\\n10.0.0.3\\nI have the following in my environment :\\n• replica 1:ID=d7766923a5a7, Name=test.1.tr0...kgk, IP=10.0.0.3\\n• replica 2:ID=b6c897d1186d, Name=test.2.sm1...1nw, IP=10.0.0.4\\nFigure 14.3 shows the configuration so far. Subnet and IP addresses may be different in\\nyour lab.\\nFigure 14.3\\nAs you can see, a layer 2 overlay network spans both nodes, and each container is\\nconnected to it with its own IP. This means the container onnode1 can ping the\\ncontainer onnode2 even though both nodes are on different underlay networks.\\nLet’s test it. You’ll need the names and IPs of your containers.\\nLog on to either of the containers and install theping utility.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 229}, page_content='14: Docker overlay networking 223\\n$ docker exec -it d7766923a5a7 bash\\n# apt update && apt-get install iputils-ping -y\\n<Snip>\\nReading package lists... Done\\nBuilding dependency tree\\nReading state information... Done\\n<Snip>\\nSetting up iputils-ping (3:20190709-3) ...\\nProcessing triggers for libc-bin (2.31-0ubuntu9) ...\\nNow ping the remote container by IP and then by replica ID.\\n# ping 10.0.0.4\\nPING 10.0.0.4 (10.0.0.4) 56(84) bytes of data.\\n64 bytes from 10.0.0.4: icmp_seq=1 ttl=64 time=1.06 ms\\n64 bytes from 10.0.0.4: icmp_seq=2 ttl=64 time=1.07 ms\\n64 bytes from 10.0.0.4: icmp_seq=3 ttl=64 time=1.03 ms\\n64 bytes from 10.0.0.4: icmp_seq=4 ttl=64 time=1.26 ms\\n^C\\n# ping test.2.sm180xqwm7k1bsyn3mt1fj1nw\\nPING test.2.sm180xqwm7k1bsyn3mt1fj1nw (10.0.0.4) 56(84) bytes of data.\\n64 bytes from test.2.sm1...1nw.uber-net (10.0.0.4): icmp_seq=1 ttl=64 time=2.83 ms\\n64 bytes from test.2.sm1...1nw.uber-net (10.0.0.4): icmp_seq=2 ttl=64 time=8.39 ms\\n64 bytes from test.2.sm1...1nw.uber-net (10.0.0.4): icmp_seq=3 ttl=64 time=5.88 ms\\n^C\\nCongratulations. The containers can ping each other via the overlay network, and all the\\ntraffic is encrypted.\\nYou can also trace the route of the ping command. This will report a single hop, proving\\nthat the containers are communicating directly via the overlay network — blissfully\\nunaware of any underlay networks being traversed.\\nYou’ll need to installtraceroute in the container for this to work.\\n# apt install traceroute\\n<Snip>\\n# traceroute 10.0.0.4\\ntraceroute to 10.0.0.4 (10.0.0.4), 30 hops max, 60 byte packets\\n1 test-svc.2.sm180xqwm7k1bsyn3mt1fj1nw.uber-net (10.0.0.4) 1.110ms 1.034ms 1.073ms\\nSo far, you’ve created an overlay network and a swarm service that connected two\\ncontainers to it. Swarm scheduled the containers to two different nodes and you proved\\nthey could ping each other via the overlay network.\\nNow that you’ve seen how easy it is to build and use secure overlay networks, let’s find\\nout how Docker builds them behind the scenes.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 230}, page_content='14: Docker overlay networking 224\\nOverlay networks explained\\nFirst and foremost, Docker usesVXLAN tunnelsto create virtual layer 2 overlay net-\\nworks. So, let’s do a quick VXLAN primer.\\nVXLAN primer\\nAt the highest level, Docker uses VXLANs to create layer 2 networks on top of existing\\nlayer 3 infrastructure. That’s a lot of jargon that means you can create simple networks\\non top of complex networks. The hands-on example in the previous sections created\\na new 10.0.0.0/24 layer 2 network that abstracted a more complex network topology\\nbelow. See Figure 14.4 and remember that your underlay network configuration was\\nprobably different.\\nFigure 14.4\\nFortunately, VXLAN is anencapsulation technology and, therefore, transparent to exist-\\ning routers and network infrastructure. This means the routers and other infrastructure\\nin the underlay network see the VXLAN/overlay traffic as regular IP/UDP packets and\\nhandle it without requiring changes.\\nTo create the overlay, Docker creates aVXLAN tunnelthrough the underlay networks,\\nand this tunnel is what allows the overlay traffic to flow freely without having to\\ninteract with the complexity of the underlay networks.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 231}, page_content='14: Docker overlay networking 225\\nTerminology: We use the termsunderlay networksor underlay infrastructureto\\nrefer to the networks the overlay tunnels through.\\nEach end of the VXLAN tunnel is terminated by aVXLAN Tunnel Endpoint (VTEP), and\\nit’s this VTEP that encapsulates and de-encapsulates the traffic entering and exiting the\\ntunnel. See Figure 14.5.\\nFigure 14.5\\nThe image shows the layer 3 infrastructure as a cloud for two reasons:\\n• It can be a lot more complex than the two networks and a single router from the\\nprevious diagrams\\n• The VXLAN tunnel abstracts the complexity and makes it opaque\\nIn reality, the VXLAN tunnel traverses the underlay network. However, I don’t show\\nthis in the diagram to keep the diagram simple.\\nTraffic flow example\\nThe hands-on examples from earlier had two hosts connected via an IP network. You\\ndeployed an overlay network across both hosts, connected two containers to it, and did\\na ping test. Let’s explain some of the things that happened behind the scenes.\\nDocker created a newsandbox (network namespace) on each host with a new switch\\ncalled Br0. It also created a VTEP with one end connected to theBr0 virtual switch and'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 232}, page_content='14: Docker overlay networking 226\\nthe other end connected to the host’s network stack. The end in the host’s network stack\\ngot an IP address on the underlay network that the host is connected to and was bound\\nto UDP port4789. Finally, the two VTEPs on each host created a VXLAN tunnel as the\\nbackbone for the overlay network.\\nFigure 14.6 shows the configuration. Remember, the VXLAN tunnel goes through the\\nnetworks at the bottom of the diagram; I’ve just drawn it higher up for readability.\\nFigure 14.6\\nAt this point, you’ve created the VXLAN overlay, and you’re ready to connect contain-\\ners.\\nDocker now creates a virtual Ethernet adapter (veth) in each container and connects it\\nto the localBr0 virtual switch. The final topology looks like Figure 14.7, and although\\nit’s complex, you should now see how the containers communicate over the VXLAN\\noverlay despite their hosts being on two separate networks — the overlay is a virtual\\nnetwork tunneled through the underlay networks.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 233}, page_content='14: Docker overlay networking 227\\nFigure 14.7\\nNow that you know how Docker creates overlay networks, let’s see how the two\\ncontainers communicate.\\nWarning! This section is very technical, and you don’t need to understand it\\nall for day-to-day operations.\\nFor this example, we’ll call the container on node1“C1” and the container on node2\\n“C2”. We’ll also assumeC1 wants to pingC2 like we did in the practical example earlier.\\nFigure 14.8 shows the full configuration with container names and IPs added.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 234}, page_content='14: Docker overlay networking 228\\nFigure 14.8\\nC1 initiates a ping request to10.0.0.4 — the IP address ofC2.\\nC1 doesn’t have an entry for10.0.0.4 in its local MAC address table (ARP cache), so\\nit floods the packet on all interfaces, including the veth interface connected to theBr0\\nbridge. TheBr0 bridge knows it can forward traffic for10.0.0.4 to the connected\\nVTEP interface and sends a proxy ARP reply to the container. This results in the veth\\nlearning how to forward the packet by updating its own MAC table to send all future\\npackets for10.0.0.4 directly to the local VTEP. TheBr0 switch knew about theC2\\ncontainer because Docker propagates details of all new containers to every swarm node\\nvia the network’s built-in gossip protocol.\\nNext, the veth in theC1 container sends the ping to the VTEP interface which encapsu-\\nlates it for transmission through the VXLAN tunnel. The encapsulation adds a VXLAN\\nheader containing a VXLAN network ID (VNID) that maps traffic from VLANs to\\nVXLANs and vice versa — each VLAN gets mapped to its own VNID so that packets\\ncan be de-encapsulated on the receiving end and forwarded to the correct VLAN. This\\nmaintains network isolation.\\nThe encapsulation also wraps the frame in a UDP packet and adds the IP of the remote\\nVTEP on node2 in thedestination IP field. It also adds the UDP/4789 socket information.\\nThis encapsulation allows the packets to be routed across the underlay networks\\nwithout the underlays knowing anything about VXLAN.\\nWhen the packet arrives at node2, the host’s kernel sees it’s addressed to UDP port\\n4789 and knows it has a VTEP bound to this socket. This means it sends the packet to\\nthe VTEP, which reads the VNID, de-encapsulates it, and sends it to its own localBr0\\nswitch on the VLAN corresponding to the VNID. From there, it delivers it to theC2'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 235}, page_content='14: Docker overlay networking 229\\ncontainer.\\nAnd that, my friends, is how Docker uses VXLAN to build and operate overlay networks\\n— a whole load of mind-blowing complexity beautifully hidden behind a single Docker\\ncommand.\\nI’m hoping that’s enough to get you started and help you when talking to your network-\\ning team about the networking aspects of your Docker infrastructure. On the topic of\\ntalking to your networking team… don’t approach them thinking that you now know\\neverything about VXLAN. If you do, you’ll probably embarrass yourself. I’m speaking\\nfrom experience ;-)\\nOne final thing. Docker also supports layer 3 routingwithin an overlay network. For\\nexample, you can create a single overlay network with two subnets, and Docker will\\nhandle the routing. The following command will create a new overlay calledprod-net\\nwith two subnets. Docker will automatically create two virtual switches calledBr0 and\\nBr1 inside thesandbox and handle all the routing.\\n$ docker network create --subnet=10.1.1.0/24 --subnet=11.1.1.0/24 -d overlay prod-net\\nClean up\\nIf you followed along, you’ll have created an overlay network calleduber-net and\\ndeployed a service calledtest. Youmay also have created a swarm.\\nRun the following command to delete thetest service.\\n$ docker service rm test\\nDelete theuber-net network with the following command. You may have to wait a few\\nseconds while Docker deletes the service using it.\\n$ docker network rm uber-net\\nIf you no longer need the swarm, you can run adocker swarm leave -f command on\\nboth nodes. You should run it onnode2 first.\\nDocker overlay networking – The commands\\n• docker network create tells Docker to create a new network. You use the-d\\noverlay flag to use the overlay driver to create an overlay network. You can also\\npass the-o encrypted flag to tell Docker to encrypt network traffic. However,\\nperformance may drop in the region of 10%.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 236}, page_content='14: Docker overlay networking 230\\n• docker network ls lists all the container networks visible to a Docker host.\\nDocker hosts running inswarm modeonly see overlay networks if they run\\ncontainers attached to the network. This keeps network-related management\\ntraffic to a minimum.\\n• docker network inspect shows detailed information about a particular container\\nnetwork. You can find out the scope, driver, IPv4 and IPv6 info, subnet configura-\\ntion, IP addresses of connected containers, VXLAN network ID, encryption state,\\nand more.\\n• docker network rm deletes a network.\\nChapter Summary\\nIn this chapter, you created a new Docker overlay network and learned about the\\ntechnologies Docker uses to build them.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 237}, page_content='15: Volumes and persistent data\\nStateful applications that create and manage data are a big part of modern cloud-native\\napps. This chapter explains how Docker volumes help stateful applications manage their\\ndata.\\nI’ve split the chapter into the following parts:\\n• Volumes and persistent data – The TLDR\\n• Containers without volumes\\n• Containers with volumes\\n• The commands\\nVolumes and persistent data – The TLDR\\nThere are two main types of data — persistent and non-persistent.\\nPersistent datais the stuff you care about and need tokeep. It includes things like cus-\\ntomer records, financial data, research results, audit data, and even some types of logs.\\nNon-persistent datais the stuff you don’t care about and don’t need to keep. We call\\napplications that create and manage persistent datastateful apps, and applications that\\ndon’t create or manage persistent datastateless apps.\\nBoth are important, and Docker has solutions for both.\\nFor stateless apps, Docker creates every container with an area of non-persistent local\\nstorage that’s tied to the container lifecycle. This storage is suitable for scratch data\\nand temporary files, but you’ll lose it when you delete the container or the container\\nterminates.\\nDocker hasvolumes for stateful apps that create and manage important data. Volumes are\\nseparate objects that you mount into containers, and they have their own lifecycles. This\\nmeans you don’t lose the volumes or the data on them when you delete containers. You\\ncan even mount volumes into different containers.\\nThat’s the TLDR. Let’s take a closer look.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 238}, page_content='15: Volumes and persistent data 232\\nContainers without volumes\\nIn the early days of Docker, containers were only good for stateless applications that\\ndidn’t generate important data. However, despite beingstateless, many of these apps still\\nneeded a place to write temporary scratch data. So, as shown in Figure 15.1, Docker\\ncreates containers by stacking read-only image layers and placing a thin layer of local\\nstorage on top. The same technology allows multiple containers to share the same read-\\nonly image layers.\\nFigure 15.1 Ephemeral container storage\\nThis thin layer of local storage is integral to the read-write nature of containers. For\\nexample, if an application needs to update existing files or add new files, it makes\\nthe changes in the local storage layer, and Docker merges them into the view of the\\ncontainer. However, the local storage is coupled to the container’s lifecycle, meaning it\\ngets created when you create the container, and deleted when you delete it. This means\\nit’s not a good place for data that you need to keep (persist).\\nDocker keeps the local storage layer on the Docker host’s filesystem, and you’ll hear it\\ncalled various names such asthe thin writeable layer, ephemeral storage, read-write storage,\\nand graphdriver storage. It’s usually located in the following locations on your Docker\\nhosts:\\n• Linux containers:/var/lib/docker/<storage-driver>/...\\n• Windows containers:C:\\\\ProgramData\\\\Docker\\\\windowsfilter\\\\...\\nEven though the local storage layer allows you to update live containers, you should\\nnever do this. Instead, you should treat containers asimmutable objectsand never change\\nthem once deployed. For example, if you need to fix or change the configuration of a live\\ncontainer, you should create and test a new container with the changes and then replace\\nthe live container with the new one.\\nTo be clear, applications like databases can change the data they manage. But users\\nand configuration tools should never change the container’sconfiguration, such as its\\nnetwork or application configuration. You should always make changes like these in a\\nnew container and then replace the old container with the new one.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 239}, page_content='15: Volumes and persistent data 233\\nIf your containers don’t create persistent data, this thin writable layer of local storage\\nwill be fine, and you’ll be good to go. However, if your containers create persistent data,\\nyou need to read the next section.\\nContainers with volumes\\nThere are three main reasons you should usevolumes to handle persistent data in\\ncontainers:\\n• Volumes are independent objects that are not tied to the lifecycle of a container\\n• You can map volumes to specialized external storage systems\\n• Multiple containers on different Docker hosts can use volumes to access and share\\nthe same data\\nAt a high level, you create a volume, then create a container, and finally mount the\\nvolume into the container. When you mount it into the volume, you mount it into a\\ndirectory in the container’s filesystem, and anything you write to that directory gets\\nstored in the volume. If you delete the container, the volume and data will still exist.\\nYou’ll even be able to mount the surviving volume into another container.\\nFigure 15.2 shows a Docker volume outside the container as a separate object. The\\nvolume is mounted into the container’s filesystem at/data, and anything you write to\\nthat directory will be stored on the volume and exist after you delete the container.\\nFigure 15.2 High-level view of volumes and containers\\nThe image also shows that you can map the volume to an external storage system\\nor a directory on the Docker host. External storage systems can be cloud services or\\ndedicated storage appliances, but either way, the volume’s lifecycle is decoupled from\\nthe container. All of the container’s other directories use the thin writable layer in the\\nlocal storage area on the Docker host.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 240}, page_content='15: Volumes and persistent data 234\\nCreating and managing Docker volumes\\nVolumes are first-class objects in Docker. This means there’s adocker volume sub-\\ncommand, and avolume resource in the API.\\nRun the following command to create a new volume calledmyvol.\\n$ docker volume create myvol\\nmyvol\\nBy default, Docker creates new volumes with the built-inlocal driver. And, as the name\\nof the driver suggests, these volumes are only available to containers on the same node\\nas the volume. You can use the-d flag to specify a different driver, but you’ll need to\\ninstall the driver first.\\nThird-party drivers20 provide advanced features and access to external storage systems\\nsuch as cloud storage services and on-premises storage systems such as SAN and NAS.\\nFigure 15.3 shows a Docker host connected to an external storage system via a plugin\\n(driver).\\nFigure 15.3 Plugging external storage into Docker\\nOnce you’ve created the volume, you can see it with thedocker volume ls command\\nand inspect it with thedocker volume inspect command.\\n20https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 241}, page_content='15: Volumes and persistent data 235\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal myvol\\n$ docker volume inspect myvol\\n[\\n{\\n\"CreatedAt\": \"2024-05-15T12:23:14Z\",\\n\"Driver\": \"local\",\\n\"Labels\": null,\\n\"Mountpoint\": \"/var/lib/docker/volumes/myvol/_data\",\\n\"Name\": \"myvol\",\\n\"Options\": null,\\n\"Scope\": \"local\"\\n}\\n]\\nNotice that theDriver and Scope fields are both set tolocal. This means you created\\nthe volume with thelocal driver, and it’s only available to containers on this Docker\\nhost. Mountpoint tells you where the volume exists in the Docker host’s filesystem.\\nBy default, Docker gives every volume created with thelocal driver its own directory\\non the host under/var/lib/docker/volumes. This means anyone with access to the\\nDocker host can bypass the container and access the volume’s contents directly in the\\nhost’s filesystem. You saw this in the Docker Compose chapter when we copied a file\\ndirectly into a volume’s directory on the Docker host, and the file immediately appeared\\nin the volume inside the container. However, that’s not a recommended practice.\\nNow that you’ve created a volume, you can create containers to use it. However, before\\nyou do that, there are two ways to delete Docker volumes:\\n• docker volume prune\\n• docker volume rm\\nThe docker volume prune command deletesall volumesnot mounted into a container\\nor service replica, so use it with caution!\\nThe docker volume rm command is more precise and lets you specify which volumes to\\ndelete.\\nNeither command will delete a volume in use by a container or service replica.\\nThe myvol volume you created isn’t used by a container, so you can delete it with either\\ncommand. Be careful if you use theprune command, as it may also delete other volumes.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 242}, page_content='15: Volumes and persistent data 236\\n$ docker volume prune --all\\nWARNING! This will remove all local volumes not used by at least one container.\\nAre you sure you want to continue? [y/N] y\\nDeleted Volumes:\\nmyvol\\nTotal reclaimed space: 0B\\nCongratulations. You’ve created, inspected, and deleted a Docker volume, and none\\nof the actions involved a container. This proves that volumes are decoupled from\\ncontainers.\\nAt this point, you know all the commands to create, list, inspect, and delete Docker\\nvolumes. You’ve even seen how to deploy them via Compose files in the Compose and\\nSwarm stacks chapters. However, you can also deploy volumes via Dockerfiles by using\\nthe VOLUME instruction. The format isVOLUME <container-mount-point>. Interestingly,\\nyou cannot specify a host directory when you define volumes in a Dockerfile. This is\\nbecause host directories can differ depending on your host OS, and you could easily\\nbreak your builds if you specified a directory that doesn’t exist on a host. As a result,\\ndefining a volume in a Dockerfile requires you to specify host directories at deployment\\ntime.\\nUsing volumes with containers\\nLet’s see how to use volumes with containers.\\nRun the following command to create a new standalone container calledvoltainer that\\nmounts a volume calledbizvol.\\n$ docker run -it --name voltainer \\\\\\n--mount source=bizvol,target=/vol \\\\\\nalpine\\nThe command specified the--mount flag, telling Docker to mount a volume called\\nbizvol into the container at/vol. The command completed successfully even though\\nyou didn’t have a volume calledbizvol. This raises an important point:\\n• If you specify a volume that already exists, Docker will use it\\n• If you specify a volume that does not exist, Docker will create it\\nIn our case,bizvol didn’t exist, so Docker created it and mounted it into the container.\\nType Ctrl PQ to return to your local shell, and then list volumes to make sure Docker\\ncreated it.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 243}, page_content='15: Volumes and persistent data 237\\n# <Ctrl-PQ>\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal bizvol\\nEven though volumes are decoupled from containers, Docker won’t let you delete this\\none because it’s in use by thevoltainer container.\\nTry to delete it.\\n$ docker volume rm bizvol\\nError response from daemon: remove bizvol: volume is in use - [b44d3f82...dd2029ca]\\nAs expected, you can’t delete it.\\nThe volume is brand new, so it doesn’t have any data. Let’sexec onto the container and\\nwrite some data to it.\\n$ docker exec -it voltainer sh\\n# echo \"I promise to write a book review on Amazon\" > /vol/file1\\nThe command writes some text to a file calledfile1 in the/vol directory where the\\nvolume is mounted.\\nRun a few commands to make sure the file and data exist.\\n# ls -l /vol\\ntotal 4\\n-rw-r--r-- 1 root root 50 May 23 08:49 file1\\n# cat /vol/file1\\nI promise to write a book review on Amazon\\nType exit to return to your Docker host’s shell, and then delete the container with the\\nfollowing commands.\\n# exit\\n$ docker rm voltainer -f\\nvoltainer\\nCheck that Docker deleted the container but kept the volume.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 244}, page_content='15: Volumes and persistent data 238\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal bizvol\\nAs the volume still exists, you can view its contents in the Docker host’s local filesystem.\\nRemember, though, that it’s not recommended to access volumes directly via the host’s\\nfilesystem. We’re just showing you how to do it for demonstration and educational\\nreasons.\\nRun the following commands from your Docker host terminal. They’ll show the\\ncontents of the volume’s directory on your Docker host. The first command will show\\nthat the file still exists, and the second will show its contents.\\nThis step won’t work on Docker Desktop, as Docker Desktop runs inside a VM. You\\nmay have to prefix the commands withsudo.\\n$ ls -l /var/lib/docker/volumes/bizvol/_data/\\ntotal 4\\n-rw-r--r-- 1 root root 50 Jan 12 14:25 file1\\n$ cat /var/lib/docker/volumes/bizvol/_data/file1\\nI promise to write a book review on Amazon\\nGreat, the volume and the data still exist.\\nLet’s see if you can mount the existingbizvol volume into a new service or container.\\nRun the following command to create a new container callednewctr that mounts bizvol\\nat /vol.\\n$ docker run -it \\\\\\n--name newctr \\\\\\n--mount source=bizvol,target=/vol \\\\\\nalpine sh\\nYour terminal is now attached to thenewctr container. Check to see if the volume and\\ndata are mounted as expected.\\n# cat /vol/file1\\nI promise to write a book review on Amazon\\nCongratulations. You’ve created a volume, written some data to it, deleted the original\\ncontainer, mounted it in a second container, and verified the data still exists.\\nType exit to leave the container and jump over to Amazon to leave the book review you\\npromised to write.\\nIf you left a review, thanks! If you didn’t, I’ll cry, but I’ll live ;-)'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 245}, page_content='15: Volumes and persistent data 239\\nSharing storage across cluster nodes\\nIntegrating Docker withexternal storage systemslets you present shared storage to\\nmultiple nodes so that the containers running on different nodes can share the same\\nvolumes. These external systems can be cloud storage services or enterprise storage\\nsystems in your on-premises data centers. For example, you can present a single storage\\nLUN or NFS share (shared volume) to multiple Docker hosts so that any container on\\nthose hosts can access and share the volume. Figure 15.4 shows an external storage\\nsystem presenting a shared volume to two Docker nodes. The Docker nodes use the\\nappropriate driver for the external system to make the shared volume available to either\\nor both containers.\\nFigure 15.4\\nBuilding a shared setup like this requires a lot of things. You need access to specialized\\nstorage systems and knowledge of how they work. You also need a volume driver/plugin\\nthat works with the external storage system. Finally, you need to know how your\\napplications read and write to the shared storage to avoid potential data corruption.\\nPotential data corruption\\nData corruption is a major concern for any shared storage configuration.\\nAssume the following example based on Figure 15.4.\\nThe application running inctr1 writes an update to the shared volume. However,\\ninstead of directly committing the update, it keeps it in a local cache for faster recall. At\\nthis point, the application inctr1 thinks it’s written data to the volume. However, before\\nctr1 flushes its cache and commits the data to the volume, the app inctr2 updates the\\nsame data with a different value and commits it directly to the volume. At this point,\\nboth applicationsthink they’ve updated the data in the volume, but in reality, only'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 246}, page_content='15: Volumes and persistent data 240\\nthe application inctr2 has. A few seconds later,ctr1 flushes the data to the volume\\nand overwrites the changes made by the application inctr2. However, neither of the\\napplications is aware of the changes the other has made.\\nThis is why you need to design applications that share data to coordinate updates to\\nshared volumes.\\nClean up\\nIf you’ve been following along, you’ll have a container and a volume.\\nRun the following command to delete the container.\\n$ docker rm\\nNow, run this command to delete the volume.\\n$ docker volume rm bizvol\\nVolumes and persistent data – The Commands\\n• docker volume create creates new volumes. By default, it creates them with the\\nlocal driver, but you can use the-d flag to specify a different driver.\\n• docker volume ls lists all volumes on your Docker host.\\n• docker volume inspect shows you detailed volume information. You can use this\\ncommand to see where a volume exists in the Docker host’s filesystem.\\n• docker volume prune deletes all volumes not in use by a container or service\\nreplica. Use with caution!\\n• docker volume rm deletes specific volumes that are not in use.\\nChapter Summary\\nThere are two main types of data:persistent and non-persistent.\\nPersistent data is data you need to keep, and non-persistent data is data you don’t need\\nto keep.\\nBy default, all containers get a layer of writable non-persistent storage that lives and dies\\nwith the container. We sometimes call thislocal storage, and it’s ideal for non-persistent'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 247}, page_content='15: Volumes and persistent data 241\\ndata. However, if your apps create data you need to keep, you should store the data in a\\nDocker volume.\\nDocker volumes are first-class objects in the Docker API, and you manage them\\nindependently of containers using their owndocker volume sub-command. This means\\ndeleting containers doesn’t delete the data in their volumes.\\nA few third-party plugins exist that provide Docker with access to specialized external\\nstorage systems.\\nVolumes are the recommended way to work with persistent data in Docker environ-\\nments.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 248}, page_content='16: Docker security\\nIf security is hard, we’re less likely to implement it. Fortunately, most of the security\\nin Docker is easy and pre-configured with sensible defaults. This means you get a\\nmoderately secureexperience with zero effort. The defaults are not perfect, but they’re a\\ngood starting point.\\nDocker supports all major Linux security technologies and adds some of its own. As\\nsuch, I’ve divided the chapter so we cover the Linux security technologies first and\\nfinish the chapter covering the Docker technologies:\\n• Docker security – The TLDR\\n• Linux security technologies\\n– Kernel namespaces\\n– Control Groups\\n– Capabilities\\n– Mandatory Access Control\\n– seccomp\\n• Docker security technologies\\n– Swarm security\\n– Docker Scout and vulnerability scanning\\n– Docker Content Trust\\n– Docker secrets\\nThe chapter focuses heavily on Linux, but the sections relating to Docker security\\ntechnologies apply to Linux and Windows containers.\\nDocker security – The TLDR\\nGood security is about layers and defence in depth, and more layers is always better.\\nFortunately, Docker offers a lot of security layers, including the ones shown in Figure\\n16.1.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 249}, page_content='16: Docker security 243\\nFigure 16.1\\nAs you can see, Docker leverages the common Linux security and workload isolation\\ntechnologies, includingnamespaces, control groups, capabilities, mandatory access control\\n(MAC), and seccomp. It ships with sensible defaults for each one, but you can customize\\nthem to your specific requirements.\\nDocker also has its own security technologies, includingDocker Scoutand Docker\\nContent Trust.\\nDocker Scout offers class-leadingvulnerability scanningthat scans your images, provides\\ndetailed reports on known vulnerabilities, and recommends solutions. Docker Content\\nTrust (DCT) lets you cryptographically sign and verify images.\\nIf you use Docker Swarm, you’ll also get all of the following that Docker automatically\\nconfigures: cryptographic node IDs, mutual authentication (TLS), automatic CA\\nconfiguration and certificate rotation, secure cluster join tokens, an encrypted cluster\\nstore, encrypted networks, and more.\\nOther security-related technologies also exist, but the important thing to know is that\\nDocker works with the major Linux security technologies and adds a few of its own.\\nSometimes, the Linux security technologies can be complex and challenging to work\\nwith, but the native Docker ones are always easy.\\nKernel Namespaces\\nKernel namespaces,usually shortened tonamespaces, are the main technology for building\\ncontainers.\\nLet’s quickly compare namespaces and containers with hypervisors and virtual ma-\\nchines (VM).'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 250}, page_content='16: Docker security 244\\nNamespaces virtualize operating system constructssuch as process trees and filesystems,\\nwhereas hypervisors virtualize physical resourcessuch as CPUs and disks. In the VM model,\\nhypervisors create virtual machines by grouping virtual CPUs, virtual disks, and virtual\\nnetwork cards so that every VM looks, smells, and feels like a physical machine. In the\\ncontainer model,namespaces create virtual operating systems (containers) by grouping\\nvirtual process trees, virtual filesystems, and virtual network interfaces so that every\\ncontainer looks, smells, and feels exactly like a regular OS.\\nAt a very high level, namespaces provide lightweight isolation but do not provide a\\nstrong security boundary. Compared with VMs, containers are more efficient, but\\nvirtual machines are more secure.\\nDon’t worry, though. Platforms like Docker implement additional security technologies,\\nsuch as cgroups, capabilities, and seccomp, to improve container security.\\nNamespaces are a tried and tested technology that’s existed in the Linux kernel for\\na very long time. However, they were complex and hard to work with until Docker\\ncame along and hid all the complexity behind the simpledocker run command and a\\ndeveloper-friendly API.\\nAt the time of writing, every Docker container gets its own instance of the following\\nnamespaces:\\n• Process ID (pid)\\n• Network (net)\\n• Filesystem/mount (mnt)\\n• Inter-process Communication (ipc)\\n• User (user)\\n• UTS (uts)\\nFigure 16.2 shows a single Docker host running two containers. The host OS has its\\nown collection of namespaces we call theroot namespaces, and each container has its own\\ncollection of equivalent isolated namespaces. Applications in containers think they’re\\nrunning on their own host and are unaware of theroot namespacesor namespaces in\\nother containers.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 251}, page_content='16: Docker security 245\\nFigure 16.2\\nLet’s briefly look at how Docker uses each namespace:\\n• Process ID namespace:Docker uses thepid namespace to give each container\\nits own isolated process tree. This means every container gets its own PID 1 and\\ncannot see or access processes running in other containers. Nor can any container\\nsee or access processes running on the host.\\n• Network namespace:Docker uses thenet namespace to provide each container\\nwith an isolated network stack. This stack includes interfaces, IP addresses,\\nport ranges, and routing tables. For example, every container gets its owneth0\\ninterface with its own unique IP and range of ports.\\n• Mount namespace:Every container has its ownmnt namespace with its own\\nunique isolated root (/) filesystem. This means every container can have its own\\n/etc, /var, /dev, and other important filesystem constructs. Processes inside a\\ncontainer cannot access the host’s filesystem or filesystems in other containers.\\n• Inter-process Communication namespace:Docker uses theipc namespace\\nfor shared memory access within a container. It also isolates the container from\\nshared memory on the host and other containers.\\n• User namespace:Docker gives each container its own users that are only valid\\ninside the container. It also lets you map those users to different users on the\\nDocker host. For example, you can map a container’sroot user to a non-root user\\non the host.\\n• UTS namespace:Docker uses theuts namespace to provide each container with\\nits own hostname.\\nRemember, a container is a collection of namespaces that Docker organizes to look like\\na regular OS. These namespaces provide isolation, but they are not a strong enough'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 252}, page_content='16: Docker security 246\\nsecurity boundary on their own. This is why Docker augments container security with\\nthe technologies we’re about to discuss.\\nControl Groups\\nIf namespaces are aboutisolation, control groups (cgroups) are aboutlimits.\\nThink of containers as similar to rooms in a hotel. While each room might appear to be\\nisolated, they actually share a lot of things such as water supply, electricity supply, air\\nconditioning, swimming pool, gym, elevators, breakfast bar, and more. Containers are\\nsimilar — even though they’re isolated, they share a lot of common resources such as the\\nhost’s CPU, RAM, network I/O, and disk I/O.\\nDocker usescgroups to limit a container’s use of these shared resources and prevent any\\ncontainer from consuming them all and causing a denial of service (DoS) attack.\\nCapabilities\\nThe Linux root user is extremely powerful, and you shouldn’t use it to run apps and\\ncontainers.\\nHowever, it’s not as simple as running them as non-root users, as most non-root users\\nare so powerless that they are practically useless. What’s needed is a way to run apps and\\ncontainers with the exact set of permissions they need — nothing more, nothing less.\\nThis is wherecapabilities come to the rescue.\\nUnder the hood, the Linuxroot user is a combination of a long list ofcapabilities. Some\\nof these capabilities include:\\n• CAP_CHOWN: lets you change file ownership\\n• CAP_NET_BIND_SERVICE: lets you bind a socket to low-numbered network\\nports\\n• CAP_SETUID: lets you elevate the privilege level of a process\\n• CAP_SYS_BOOT: lets you reboot the system.\\nThe list goes on and is long.\\nDocker leverages capabilities so that you can run containers asroot but strip out\\nall the capabilities you don’t need. For example, suppose the only capability your\\ncontainer needs is the ability to bind to low-numbered network ports. In that case,'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 253}, page_content='16: Docker security 247\\nDocker can start the container as root,drop all root capabilities,and then add back the\\nCAP_NET_BIND_SERVICE capability.\\nThis is a good example of implementing the principle ofleast privilegeas you end up\\nwith a container that only has the capabilities it needs. Docker also sets restrictions to\\nprevent containers from re-adding dropped capabilities.\\nDocker ships with sensible out-of-the-box capabilities, but you should configure your\\nown for your production apps and containers. However, configuring your own requires\\nextensive effort and testing.\\nMandatory Access Control systems\\nDocker works with major Linux MAC technologies such as AppArmor and SELinux.\\nDepending on your Linux distribution, Docker applies default AppArmor or SELinux\\nprofiles to all new containers, and according to the Docker documentation, the default\\nprofiles aremoderately protective while providing wide application compatibility.\\nYou can tell Docker to start containers without these policies, and you can configure\\nyour own. However, as withcapabilities, configuring your own policies is very powerful\\nbut requiresa lotof effort and testing.\\nseccomp\\nDocker uses seccomp to limit which syscalls a container can make to the host’s kernel.\\nSyscalls are how applications ask the Linux kernel to perform tasks. At the time of writ-\\ning, Linux has over 300 syscalls and the default Docker profile disables approximately\\n40-50.\\nAs per the Docker security philosophy, all new containers get a default seccomp profile\\nconfigured withsensible defaultsdesigned to providemoderate security without impacting\\napplication compatibility.\\nAs always, you can customize your own seccomp profiles or tell Docker to start\\ncontainers without one. Unfortunately, the Linux syscall table is long, and configuring\\ncustom seccomp policies may be prohibitively complex for some users.\\nFinal thoughts on the Linux security technologies\\nDocker supports most of the important Linux security technologies and ships with\\nsensible defaults that add security without being too restrictive. Figure 16.3 shows how\\nDocker uses them to build adefence in depthsecurity posture with multiple layers.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 254}, page_content='16: Docker security 248\\nFigure 16.3 - Linux security defense in depth\\nSome of these technologies require knowledge of the Linux kernel and can be complex\\nto customize. Fortunately, many platforms, including Docker, ship with defaults that are\\na good place to start.\\nDocker security technologies\\nLet’s switch our focus to some of the security technologies Docker offers.\\nSwarm security\\nDocker Swarm lets you cluster multiple Docker hosts and manage applications declar-\\natively. Every Swarm comprisesmanager nodesand worker nodesthat can be Linux or\\nWindows. Managers host the control plane and are responsible for configuring the\\ncluster and dispatching work tasks. Workers run application containers.\\nFortunately, swarm modeincludes many security features that Docker automatically\\nconfigures with sensible defaults. These include:\\n• Cryptographic node IDs\\n• TLS for mutual authentication'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 255}, page_content='16: Docker security 249\\n• Secure join tokens\\n• CA configuration with automatic certificate rotation\\n• Encrypted cluster store\\n• Encrypted networks\\nLet’s walk through building a secure swarm and configuring some of the security\\naspects.\\nIf you’re following along, you’ll need three Docker hosts that can ping each other by\\nname. The examples use three hosts calledmgr1, mgr2, andwrk1.\\nConfigure a secure Swarm\\nRun the following command from the node you want to be the first manager. We’ll run\\nthe example frommgr1.\\n$ docker swarm init\\nSwarm initialized: current node (7xam...662z) is now a manager.\\nThat’s it! You’ve configured a secure swarm with a cryptographic cluster ID, an en-\\ncrypted cluster store, a certificate authority with a 90-day certificate rotation policy, a\\nset of secure join tokens to use when adding new managers and workers, and configured\\nthe current manager with a client certificate for mutual TLS — all with a single com-\\nmand!\\nFigure 16.4 shows the current swarm configuration. Some of the details may be\\ndifferent in your lab.\\nFigure 16.4\\nLet’s joinmgr2 as an additional manager.\\nJoining new managers is a two-step process:'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 256}, page_content='16: Docker security 250\\n• Extract the secure join token\\n• Execute adocker swarm join command with the join token on the node you’re\\nadding\\nRun the following command frommgr1 to extract the manager join token.\\n$ docker swarm join-token manager\\nTo add a manager to this swarm, run the following command:\\ndocker swarm join --token \\\\\\nSWMTKN-1-1dmtwu...r17stb-2axi5...8p7glz \\\\\\n172.31.5.251:2377\\nThe output gives you the full command and join token to run onmgr2. The join token\\nand IP address will be different in your lab.\\nThe format of the joincommand is:\\n• docker swarm join --token <manager-join-token> <ip-of-existing-\\nmanager>:<swarm-port>\\nThe format of thetoken is:\\n• SWMTKN-1-<hash-of-cluster-certificate>-<manager-join-token>\\nCopy the command and run it onmgr2:\\n$ docker swarm join --token SWMTKN-1-1dmtwu...r17stb-2axi5...8p7glz 172.31.5.251:2377\\nThis node joined a swarm as a manager.\\nList the nodes in your swarm.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\n7xamk...ge662z mgr1 Ready Active Leader\\ni0ue4...zcjm7f * mgr2 Ready Active Reachable\\nYou now have a two-node swarm withmgr1 and mgr2 as managers. Both have access to\\nthe cluster store and are configured with client certificates for mutual TLS.\\nIn the real world, you’ll always run three or five managers for high availability.\\nFigure 16.5 shows the updated swarm with both managers.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 257}, page_content='16: Docker security 251\\nFigure 16.5\\nAdding worker nodes is a similar two-step process — extract the join token and run the\\ncommand on the node.\\nRun the following command on either of the managers to expose the worker join\\ncommand and token.\\n$ docker swarm join-token worker\\nTo add a worker to this swarm, run the following command:\\ndocker swarm join --token \\\\\\nSWMTKN-1-1dmtw...17stb-ehp8g...w738q \\\\\\n172.31.5.251:2377\\nCopy the command and run it onwrk1:\\n$ docker swarm join --token SWMTKN-1-1dmtw...17stb-ehp8g...w738q 172.31.5.251:2377\\nThis node joined a swarm as a worker.\\nRun anotherdocker node ls from either of your managers.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\n7xamk...ge662z * mgr1 Ready Active Leader\\nailrd...ofzv1u wrk1 Ready Active\\ni0ue4...zcjm7f mgr2 Ready Active Reachable\\nYour swarm has two managers and a worker. The managers are configured for high\\navailability (HA) and the cluster store is replicated to both. The worker node is part of\\nthe swarm but cannot access the cluster store. Figure 16.6 shows the final configuration.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 258}, page_content='16: Docker security 252\\nFigure 16.6\\nNow that you’ve built a secure Swarm, let’s examine some of the security aspects.\\nSwarm join tokens\\nThe only requirement for joining managers and workers is possession of the secure join\\ntoken. This means you should keep them safe and never post them on public repos or\\neven internal repos that are not restricted.\\nEvery swarm maintains two distinct join tokens:\\n• Manager token\\n• Worker token\\nEvery join token has four distinct fields separated by dashes (-):\\n• PREFIX - VERSION - SWARM ID - TOKEN\\nThe prefix is alwaysSWMTKN and allows you to pattern-match against it to prevent people\\nfrom accidentally posting it publicly. TheVERSION field indicates the version of the\\nswarm. TheSwarm ID field is a hash of the swarm’s certificate. TheTOKEN field is the\\nworker or manager token.\\nAs you can see in the following table, the manager and worker tokens for any given\\nswarm are identical except for the finalTOKEN field.\\nRole Prefix Version Swarm ID Token\\nManager SWMTKN 1 1dmtwusdc…r17stb 2axi53zjbs45lqxykaw8p7glz\\nWorker SWMTKN 1 1dmtwusdc…r17stb ehp8gltji64jbl45zl6hw738q'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 259}, page_content='16: Docker security 253\\nIf you suspect either of your join tokens are compromised, you can revoke them and\\nissue new ones with a single command. The following example revokes the existing\\nmanager token and issues a new one.\\n$ docker swarm join-token --rotate manager\\nSuccessfully rotated manager join token.\\nExisting managers are unaffected, but you can only add new ones with the new token.\\nAs expected, the last field is the only difference between the old and new tokens.\\nDocker keeps a copy of join tokens in the encrypted cluster store.\\nTLS and mutual authentication\\nDocker issues every manager and worker with a client certificate that they use for\\nmutual authentication. It identifies the node, the swarm it’s a member of, and whether\\nit’s a manager or worker.\\nYou can inspect a node’s client certificate on Linux with the following command.\\n$ sudo openssl x509 \\\\\\n-in /var/lib/docker/swarm/certificates/swarm-node.crt \\\\\\n-text\\nCertificate:\\nData:\\nVersion: 3 (0x2)\\nSerial Number:\\n7c:ec:1c:8f:f0:97:86:a9:1e:2f:4b:a9:0e:7f:ae:6b:7b:b7:e3:d3\\nSignature Algorithm: ecdsa-with-SHA256\\nIssuer: CN = swarm-ca\\nValidity\\nNot Before: May 23 08:23:00 2024 GMT\\nNot After : Aug 21 09:23:00 2024 GMT\\nSubject: O = tcz3w1t7yu0s4wacovn1rtgp4, OU = swarm-manager,\\nCN = 2gxz2h1f0rnmc3atm35qcd1zw\\nSubject Public Key Info:\\n<SNIP>\\nAs shown in Figure 16.7, theSubject field uses the standardO, OU, andCN fields to\\nspecify the Swarm ID, the node’s role, and the node ID:\\n• The Organization (O) field stores the Swarm ID\\n• The Organizational Unit (OU) field stores the node’s role in the swarm'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 260}, page_content='16: Docker security 254\\n• The Canonical Name (CN) field stores the node’s crypto ID.\\nYou can also see the certificate rotation period in theValidity section.\\nFigure 16.7\\nYou can match these values to the corresponding values from adocker info command.\\n$ docker info\\n<SNIP>\\nSwarm: active\\nNodeID: 2gxz2h1f0rnmc3atm35qcd1zw <<---- Relates to the CN field\\nIs Manager: true <<---- Relates to the OU field\\nClusterID: tcz3w1t7yu0s4wacovn1rtgp4 <<---- Relates to the O field\\n<SNIP>\\nCA Configuration:\\nExpiry Duration: 3 months <<---- Relates to the validity block\\nForce Rotate: 0\\nRoot Rotation In Progress: false\\n<SNIP>\\nSwarm CA configuration\\nYou can use thedocker swarm update command to configure the certificate rotation\\nperiod. The following example changes it to 30 days.\\n$ docker swarm update --cert-expiry 720h\\nSwarm allows nodes to renew certificates early so that all nodes don’t update at exactly\\nthe same time.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 261}, page_content='16: Docker security 255\\nYou can configure a new swarm to use an external CA by passing the--external-ca\\nflag todocker swarm init command, and you can use thedocker swarm ca command\\nto manage other CA-related settings.\\n$ docker swarm ca --help\\nUsage: docker swarm ca [OPTIONS]\\nDisplay and rotate the root CA\\nOptions:\\n--ca-cert pem-file Path to the PEM-formatted root CA certificate to use\\nfor the new cluster\\n--ca-key pem-file Path to the PEM-formatted root CA key to use for the\\nnew cluster\\n--cert-expiry duration Validity period for node certificates (ns|us|ms|s|m|h)\\n(default 2160h0m0s)\\n-d, --detach Exit immediately instead of waiting for the root rotation\\nto converge\\n--external-ca external-ca Specifications of one or more certificate signing endpoints\\n-q, --quiet Suppress progress output\\n--rotate Rotate the swarm CA - if no certificate or key are\\nprovided, new ones will be generated\\nThe cluster store\\nThe cluster store is where Docker keeps the configuration and state of a swarm. It’s also\\ncritical to other Docker technologies, such as overlay networks and secrets. This is why\\noverlay networks and many other advanced security features only work in swarm mode.\\nThe cluster store is based on the popularetcd distributed database and is automatically\\nencrypted and replicated to all managers.\\nDocker handles day-to-day maintenance, but you should implement strong backup and\\nrecovery procedures for production clusters.\\nThat’s enough about swarm mode security for now. Let’s look at some Docker security\\ntechnologies that don’t require swarm mode.\\nDocker Scout and vulnerability scanning\\nEvery container runs multiple software packages that are susceptible to bugs and\\nvulnerabilities that malicious actors can exploit.\\nImage scanning analyzes your images and produces a detailed list of all the software\\npackages it uses. We call this list asoftware bill of materials (SBOM),and the image'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 262}, page_content='16: Docker security 256\\nscanning system compares the SBOM against databases of known vulnerabilities and\\nprovides a report of vulnerabilities in your software. Most vulnerability scanners will\\nrank the vulnerabilities and provide advice on fixes.\\nVulnerability scanning is now an integral part of most software supply chains.\\nDocker Scoutis Docker’s native scanning platform and works with Docker Hub,\\nDocker Desktop, the Docker CLI, and even has its own Docker Scout Dashboard.\\nHowever, it’s a subscription-based service.\\nOther scanning platforms are available, but most of these also require some form of\\nsubscription.\\nIf you’re using Docker Desktop, you can run the following command to see an example\\nof Docker Scout.\\n$ docker scout quickview nigelpoulton/tu-demo:latest\\n✓ Provenance obtained from attestation\\n✓ Pulled\\n✓ Image stored for indexing\\n✓ Indexed 66 packages\\nTarget │ nigelpoulton/tu-demo:latest │ 0C 4H 2M 0L\\ndigest │ b4210d0aa52f │\\nBase image │ python:3-alpine │ 0C 2H 1M 0L\\nUpdated base image │ python:3.11-alpine │ 0C 1H 1M 0L\\n│ │\\nThe output shows zero critical vulnerabilities (0C), four high (4H), two medium (2M),\\nand zero low (0L).\\nYou can also run adocker scout cves command to get more detailed information,\\nincluding remediation advice.\\n$ docker scout cves nigelpoulton/tu-demo:latest\\n✓ SBOM of image already cached, 66 packages indexed\\n\\uffff Detected 6 vulnerable packages with a total of 8 vulnerabilities\\n## Overview\\n│ Analyzed Image\\n────────────────────┼────────────────────────────────\\nTarget │ nigelpoulton/tu-demo:latest\\ndigest │ b4210d0aa52f\\nplatform │ linux/arm64\\nvulnerabilities │ 0C 4H 2M 0L\\nsize │ 26 MB\\npackages │ 66'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 263}, page_content='16: Docker security 257\\n## Packages and Vulnerabilities\\n0C 1H 1M 0L expat 2.5.0-r2\\npkg:apk/alpine/expat@2.5.0-r2?os_name=alpine&os_version=3.19\\n\\uffff HIGH CVE-2023-52425\\nhttps://scout.docker.com/v/CVE-2023-52425\\nAffected range : <2.6.0-r0\\nFixed version : 2.6.0-r0\\n\\uffff MEDIUM CVE-2023-52426\\nhttps://scout.docker.com/v/CVE-2023-52426\\nAffected range : <2.6.0-r0\\nFixed version : 2.6.0-r0\\n<Snip>\\nI’ve snipped the output, so it only shows some of the vulnerabilities. However, even\\nfrom the snipped output in the book, you can see:\\n• Scout has scanned 66 packages and detected several vulnerabilities\\n• We’re using version2.5.0-r2 of theexpat package which has one high (1H) and\\none medium (1M) vulnerability\\n• The high vulnerability is listed asCVE-2023-52425 and the medium asCVE-2023-\\n52426\\n• The report includes links to Scout reports containing more info on each vulnera-\\nbility\\n• Scout recommends updating toexpat version 2.6.0-r0 which contains fixes for\\nboth\\nFigure 16.8 shows what it looks like in in Docker Desktop, and you get similar integra-\\ntions and views in Docker Hub.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 264}, page_content='16: Docker security 258\\nFigure 16.8 - Docker Scout integration with Docker Desktop\\nIf you subscribe to Docker Scout, you can use thescout.docker.com portal to configure\\npolicies and integrations with Docker Hub and other registries.\\nAs good as vulnerability scanning is, it only scans images and doesn’t detect security\\nproblems with networks, nodes, or orchestrators. Also, not all image scanners are equal.\\nFor example, the best ones perform deep binary-level scans, whereas others may just\\nlook at package names and do not inspect content closely.\\nIn summary, scanning tools are great for inspecting your images and detecting known\\nvulnerabilities. Beware though, with great knowledge comes great responsibility — once\\nyou’re aware of vulnerabilities, you’re responsible for mitigating or fixing them.\\nSigning and verifying images with Docker Content\\nTrust\\nDocker Content Trust (DCT) makes it simple for you to verify the integrity and pub-\\nlisher of images and is especially important when you’re pulling images over untrusted\\nnetworks such as the internet.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 265}, page_content='16: Docker security 259\\nAt a high level, DCT lets you sign your images when you push them to registries like\\nDocker Hub. It also lets you verify the images you pull and run as containers.\\nFigure 16.9 shows the high-level process.\\nFigure 16.9 - Docker Content Trust image signing and verification\\nYou can also use DCT to providecontext, such as whether or not a developer has signed\\nan image for use in a particular environment such asprod or dev, or whether an image\\nhas been superseded by a newer version and is therefore stale.\\nThe following steps walk you through configuring Docker Content Trust, signing and\\npushing an image, and then pulling the signed image.\\nIf you plan on following along, you’ll need a cryptographic key pair. If you don’t already\\nhave one, you can run the followingdocker trust command to generate one. The\\ncommand generates a new key pair callednigel and loads it to the local trust store ready\\nfor use. It will prompt you to enter a passphrase; don’t forget it :-)\\n$ docker trust key generate nigel\\nGenerating key for nigel...\\nEnter passphrase for new nigel key with ID 1f78609:\\nRepeat passphrase for new nigel key with ID 1f78609:\\nSuccessfully generated and loaded private key.... key available: /Users/nigelpoulton/nigel.pub\\nIf you already have a key pair, you can import and load it withdocker trust key load\\nkey.pem --name nigel.\\nThe next step is associating your key pair with the image repository to which you’ll push\\nsigned images. This example associates thenigel.pub key with thenigelpoulton/ddd-\\ntrust repo on Docker Hub. Your key file and repo will be different, and the repository\\ndoesn’t have to exist before you run the command.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 266}, page_content='16: Docker security 260\\n$ docker trust signer add --key nigel.pub nigel nigelpoulton/ddd-trust\\nAdding signer \"nigel\" to nigelpoulton/dct...\\nInitializing signed repository for nigelpoulton/dct...\\nEnter passphrase for root key with ID aee3314:\\nEnter passphrase for new repository key with ID 1a18dd1:\\nRepeat passphrase for new repository key with ID 1a18dd1:\\nSuccessfully initialized \"nigelpoulton/dct\"\\nSuccessfully added signer: nigel to nigelpoulton/dct\\nNow that you’ve loaded the key pair and associated it with a repository, the final step is\\nto sign an image and push it to the repo.\\nThe following command signs a local image callednigelpoulton/ddd-trust:signed\\nand pushes it to Docker Hub. Your image will have a different name and you’ll push it to\\na different repo.\\n$ docker trust sign nigelpoulton/ddd-trust:signed\\nSigning and pushing trust data for local image nigelpoulton/ddd-trust:signed may...\\nThe push refers to repository [docker.io/nigelpoulton/ddd-trust]\\n6495b414566f: Mounted from nigelpoulton/ddd-book\\n798676f7ef8b: Mounted from nigelpoulton/ddd-book\\nbca4290a9639: Mounted from nigelpoulton/ddd-book\\n28ad2149d870: Mounted from nigelpoulton/ddd-book\\n4f4fb700ef54: Mounted from nigelpoulton/ddd-book\\n5e1fc7f5df34: Mounted from nigelpoulton/ddd-book\\nsigned: digest: sha256:b65f9a1aa4e670bbafd0fbb91281ea95f9cdc5728aa546579e248dfbc0ea4bde\\nSigning and pushing trust metadata\\nEnter passphrase for nigel key with ID 92330ea:\\nSuccessfully signed docker.io/nigelpoulton/ddd-trust:signed\\nThe push operation creates the repo on Docker Hub and then signs and pushes the\\nimage. You can view the repo on Docker Hub, and you can run the following command\\nto inspect its signing data.\\n$ docker trust inspect nigelpoulton/ddd-trust:signed --pretty\\nSignatures for nigelpoulton/ddd-trust:signed\\nSIGNED TAG DIGEST SIGNERS\\nsigned 30e6d35703c578ee...4fcbbcbb0f281 nigel\\nList of signers and their keys for nigelpoulton/ddd-trust:signed\\nSIGNER KEYS\\nnigel 4d6f1bf55702\\nAdministrative keys for nigelpoulton/ddd-trust:signed\\nRepository Key: 5e72e54afafb8444f...6b2744b32010ad22\\nRoot Key: 40418fc47544ca630...69a2cb89028c22092'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 267}, page_content='16: Docker security 261\\nYou can export theDOCKER_CONTENT_TRUST variable with a value of1 to force a Docker\\nhost to sign and verify all images.\\n$ export DOCKER_CONTENT_TRUST=1\\nOnce enabled, you won’t be able to pull and work with unsigned images.\\nTest it by trying to pull an unsigned image.\\n$ docker pull nigelpoulton/ddd-book:web0.2\\nError: remote trust data does not exist for docker.io/nigelpoulton/ddd-book: notary.docker.io\\ndoes not have trust data for docker.io/nigelpoulton/ddd-book\\nYou can no longer pull images without trust data!\\nDelete the local copy of the image you just signed and pushed so that you can try pulling\\nit from Docker Hub. Your image name will be different.\\n$ docker rmi nigelpoulton/ddd-trust:signed\\nUntagged: nigelpoulton/ddd-trust:signed@sha256...\\n<Snip>\\nNow, try pulling the image.\\n$ docker pull nigelpoulton/ddd-trust:signed\\nPull (1 of 1): nigelpoulton/ddd-trust:signed@sha256:30e6...\\ndocker.io/nigelpoulton/ddd-trust@sha256:30e6... Pulling from nigelpoulton/ddd-trust\\n08409d417260: Pull complete\\nDigest: sha256:30e6d35703c578ee703230b9dc87ada2ba958c1928615ac8a674fcbbcbb0f281\\nStatus: Downloaded newer image for nigelpoulton/ddd-trust@sha256:30e6...\\nTagging nigelpoulton/ddd-trust@sha256:30e6d... as nigelpoulton/ddd-trust:signed\\ndocker.io/nigelpoulton/ddd-trust:signed\\nThe pull worked because the image has valid trust data.\\nIn summary, Docker Content Trust is an important technology that helps you verify the\\nintegrity of the images you pull and run. It’s simple to configure in its basic form, but\\nmore advanced features, such ascontext, can be more complex.\\nDocker Secrets\\nMost applications leverage sensitive data such as passwords, certificates, and SSH keys.\\nFortunately, Docker lets you wrap them insidesecrets to keep them secure.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 268}, page_content='16: Docker security 262\\nNote: Secrets only work in swarm mode as they leverage the cluster store.\\nBehind the scenes, Docker encrypts secrets when they’reat restin the cluster store and\\nwhile they’rein flighton the network. It also usesin-memory filesystemsto mount secrets\\ninto containers and operates a least-privilege model, where secrets are only available\\nto services that have been explicitly granted access. There’s even adocker secret\\ncommand.\\nFigure 16.10 shows the high-level workflow of creating a secret and deploying it to\\nservice replicas:\\nFigure 16.10 - Secret workflow\\nLet’s go through the five steps in the diagram. I’ve used a key symbol to show the secret,\\nand it’s only available to the dark containers.\\n1. You create the secret\\n2. Docker stores it in the encrypted cluster store\\n3. You create a service (the dark containers) and grant it access to the secret\\n4. Docker encrypts the secret when sending it over the network to service replicas\\n5. Docker mounts the secret into service replicas as an unencrypted file in an in-\\nmemory filesystem\\nThe light-colored containers are part of a different service and cannot access the secret.\\nAs soon as replicas using the secret terminate, Docker destroys the in-memory filesys-\\ntem and flushes the secret from the node.\\nDocker mounts secrets in their unencrypted form so that applications can use them\\nwithout needing keys to decrypt them.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 269}, page_content='16: Docker security 263\\nYou can create and manage secrets with thedocker secret command and attach them\\nto services by passing the--secret flag to thedocker service create command.\\nClean up\\nIf you’ve followed along, you’ve created a swarm, added a signer, created a new repo\\non Docker Hub, and exported an environment variable to sign and verify images\\nautomatically.\\nRun the following command to disable Docker Content Trust. You’ll need to run it on\\nevery node where you enabled Docker Content Trust.\\n$ unset DOCKER_CONTENT_TRUST\\nRemove the signer from the repository you created. Your signer and repository will have\\ndifferent names.\\n$ docker trust signer remove nigel nigelpoulton/ddd-trust\\nRemoving signer \"nigel\" from nigelpoulton/ddd-trust...\\nall signed tags are currently revoked, use docker trust sign to fix\\nYou may also want to delete the repositories you created on Docker Hub and delete the\\nlocal key files on your system (usually a .pub file in your home directory)\\nDelete the swarm by running the following command on all swarm nodes. You should\\nrun it on the swarm managers last.\\n$ docker swarm leave -f\\nChapter Summary\\nYou can configure Docker to be extremely secure. It supports all of the major Linux\\nsecurity technologies such as kernel namespaces, cgroups, capabilities, MAC, and\\nseccomp. It ships with sensible defaults for all of these, but you can customize and even\\ndisable them.\\nIn addition to the Linux security technologies, Docker includes an extensive set of\\nits own security technologies. Swarms are built on TLS and are secure out of the box.\\nDocker Scout performs binary-level image scans and provides detailed reports of\\nknown vulnerabilities and suggested fixes. Docker Content Trust lets you sign and\\nverify images, and Docker secrets allow you to share sensitive data with swarm services.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 270}, page_content='What next\\nThank you so much for reading my book. You’re on your way to mastering containers!\\nAbout the front cover\\nI love this book’s cover, and I’m grateful to the hundreds of people who voted for the\\ndesign.\\nThe YAML code on the left represents the book’s technical nature. The Docker whale\\nrepresents the main topic. The vertical symbols on the right are container-related icons\\ndone in the style ofdigital rainfrom the Matrix movies. There’s also a hidden message\\nwritten in Klingon.\\nGet involved with the community\\nThere’s a vibrant container community full of helpful people. Get involved with Docker\\ngroups and chats on the internet, and look up your local Docker or cloud-native meetup\\n(search for “Docker meetup near me”).\\nKubernetes\\nNow that you know a thing or two about Docker, a great next step is Kubernetes. It’s a\\nlot like Swarm but has a larger scope and a more active community.\\nIf you liked this book, you’ll love my books on Kubernetes.\\nConnect with me\\nI’d love to connect with you and talk about Kubernetes and other cool tech.\\nYou can reach me at all of the following:\\n• Twitter: twitter.com/nigelpoulton\\n• LinkedIn: linkedin.com/in/nigelpoulton\\n• Mastodon: @nigelpoulton@hachyderm.io\\n• Web: nigelpoulton.com'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 271}, page_content='What next 265\\n• YouTube: youtube.com/nigelpoulton\\n• Delete me\\nFeedback and reviews\\nBooks live and die by Amazon reviews and stars.\\nI’ve spent more than a year of my life writing this book, and I work tirelessly keeping\\nit up-to-date. Soooo… please take a moment to leave a kind review on Amazon or\\nGoodreads.\\nAlso, ping me atddd@nigelpoulton.com if you want to suggest content or fixes for\\nfuture editions.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 272}, page_content='Terminology\\nThis glossary defines some of the most common Docker and container-related terms\\nused in the book.\\nIf you think I’ve missed anything important, ping me atddd@nigelpoulton.com.\\nTerm Definition (according to Nigel)\\nAPI Application Programming Interface. In the\\ncase of Docker, all resources are defined in\\nthe Docker API, which is RESTful and\\nexposed via theDocker Daemon.\\nBase image The first layer of all container images.\\nCreated by the DockerfileFROM instruction\\nand usually contains a minimal set of OS\\nconstructs required by an application.\\nBuild The process of building a new container\\nimage. Dockerbuilds images by stepping\\nthrough a set of instructions defined in a\\nDockerfile.\\nBuild Cloud A subscription service that performs fast and\\nefficient image builds in Docker’s cloud\\ninfrastructure. It allows you to share a\\ncommon build-cache among teams for very\\nfast builds.\\nBuildKit Docker’s build engine that implements\\nadvanced build features such as advanced\\ncaching, multi-stage builds, and\\nmulti-architecture builds.\\nBuildx Docker’s latest and greatest build client that\\nsupports all the latest features of BuildKit,\\nsuch as multi-stage builds and\\nmulti-architecture images. Buildx has been\\nDocker’s default build client since Docker\\nEngine v23.0 and Docker Desktop v4.19.\\nCapability Linux kernel technology used by Docker to\\ncreate user accounts with the precise set of\\nsystem access they need.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 273}, page_content='Terminology 267\\nTerm Definition (according to Nigel)\\nCloud native A loaded term that means different things to\\ndifferent people. Cloud native is a way of\\ndesigning, building, and working with\\nmodern applications and infrastructure. I\\nconsider an application to becloud nativeif it\\ncan self-heal, scale on-demand, perform\\nrolling updates, and versioned rollbacks.\\nCluster store Docker Swarm’s distributed database that\\nholds the state of the cluster and apps. Based\\non theetcd distributed database, it is\\nautomatically encrypted and automatically\\ndistributed across all swarm managers for\\nhigh availability.\\nCompose An open specification for defining, deploying,\\nand managing multi-container microservices\\napps. Docker implements the Compose spec\\nand provides thedocker compose command\\nto make it easy to work with Compose apps.\\nContainer A container is a collection of kernel\\nnamespaces organized to look, smell, and feel\\nlike a regular operating system. Each\\ncontainer runs a single application, and\\ncontainers are smaller, faster, and more\\nportable than virtual machines. We\\nsometimes call themDocker containersor OCI\\ncontainers\\nContainer Network Model Pluggable interface enabling different\\nnetwork topologies and architectures. Third\\nparties provide CNM plugins for overlay\\nnetworks and BGP networks, as well as\\nvarious implementations of each.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 274}, page_content='Terminology 268\\nTerm Definition (according to Nigel)\\nContainer runtime Software running on every Docker node\\nresponsible for pulling container images,\\nstarting containers, stopping containers, and\\nother low-level container operations. Docker\\nuses two runtimes that work together:\\ncontainerd is Docker’s high-level runtime\\nthat manages lifecycle events such as starting\\nand stopping containers, whereasrunc is\\nDocker’s low-level runtime that interfaces\\nwith kernel constructs such as namespaces\\nand cgroups.\\ncontainerd Industry-standard container runtime used by\\nDocker and most Kubernetes clusters.\\nDonated to the CNCF by Docker, Inc.\\nPronounced “container dee”.\\nContainerize The process of packaging an application and\\nall dependencies into a container image.\\nControl Groups (cgroups) Linux kernel feature that Docker uses to limit\\nthe amount of host CPU, RAM, disk, and\\nnetwork resources a container uses.\\nDesired state How your cluster and applications should be.\\nFor example, thedesired stateof an application\\nmicroservice might be five replicas of xyz\\ncontainer listening on port 8080/tcp. Vital to\\nreconciliation.\\nDocker Platform that makes it easy to work with\\ncontainerized apps. It allows you to build\\nimages, as well as run and manage standalone\\ncontainers and multi-container apps.\\nDocker Debug Docker CLI plugin that lets you easily debug\\nslim images and containers that don’t ship\\nwith any debugging tools.\\nDocker Desktop Desktop application for Linux, Mac, and\\nWindows that makes working with Docker\\neasy. It has a slick UI and many advanced\\nfeatures like image management, vulnerability\\nscanning, and Wasm support.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 275}, page_content='Terminology 269\\nTerm Definition (according to Nigel)\\nDocker Hub High-performance OCI-compliant image\\nregistry. Docker Hub has over 57PB of\\nstorage and handles an average of 30K\\nrequests per second.\\nDocker, Inc. US-based technology company making it easy\\nfor developers to build, ship, and run\\ncontainerized applications. The company\\nbehind the Docker platform.\\nDocker init A new Docker CLI plugin that creates\\nhigh-fidelity Dockerfiles and makes it easy to\\nscaffold Compose apps.\\nDocker Scout Docker’s native vulnerability scanning service.\\nScout is a subscription service that integrates\\nwith the Docker CLI, Docker Desktop,\\nDocker Hub, and other image registries.\\nDockerfile Plain text file with instructions telling Docker\\nhow to build an application into a container\\nimage.\\netcd The open-source distributed database used by\\nDocker Swarm.\\nImage Archive containing application code, all\\ndependencies, and the metadata required to\\nstart a single application as a container. We\\nsometimes call themOCI images, container\\nimages, orDocker images.\\nIngress network Hidden network on all Docker Swarm\\nclusters used to publish services to external\\nclients.\\nKernel namespace Feature of the Linux kernel used by Docker to\\nisolate containers from processes running on\\nthe host and in other containers.\\nLayer Image layers contain modifications to the\\nbase image or the layer below them. Docker\\nbuilds images by stacking layers, each\\ncontaining changes to the layer below it. A\\nsimple example is a base layer that has basic\\nOS constructs, followed by a layer with the\\napplication. The two combined layers create\\nthe image with the OS and app.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 276}, page_content='Terminology 270\\nTerm Definition (according to Nigel)\\nlibcontainer A Go library that uses namespaces, cgroups,\\nand capabilities to build containers. Docker\\nuses libcontainer via therunc low-level\\nruntime that is a CLI wrapper around\\nlibcontainer.\\nlibnetwork The Go library used by Docker to create and\\nmanage container networks.\\nMicroservices Design pattern for modern applications.\\nIndividual application features are developed\\nas their own small applications\\n(microservices/containers) and communicate\\nvia APIs. They work together to form a useful\\napplication.\\nMulti-architecture builds (sometimes called\\nmulti-platform builds)\\nAllows you to build images for multiple\\narchitectures and platforms with a single\\ndocker build command. For example, you\\ncan run a singledocker build command on\\nan AMD-based Windows system to build an\\nAMD imageand an ARM image.\\nMulti-state build Allows you to create very small images (slim\\nimages). You build your images in stages and\\nonly carry forward the necessary artifacts for\\neach next stage. Each build stage is\\nrepresented by its ownFROM instruction in\\nyour Dockerfile, and later build stages use the\\nCOPY --from instruction to use artifacts from\\nprevious stages and leave everything else\\nbehind.\\nObserved state Also known ascurrent stateor actual state. The\\nmost up-to-date view of the cluster and\\nrunning applications. Docker Swarm is\\nalways working to makeobserved statematch\\ndesired state.\\nOpen Container Initiative (OCI) Lightweight governance body responsible for\\ncreating and maintaining standards for\\nlow-level container technologies such as\\nimages, runtimes, and registries. Docker\\ncreates OCI-compliant images, implements\\nan OCI-compliant runtime, and Docker Hub\\nis an OCI-compliant registry.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 277}, page_content='Terminology 271\\nTerm Definition (according to Nigel)\\nOrchestrator Software that deploys and manages apps.\\nDocker Swarm and Kubernetes are examples\\nof orchestrators that manage microservices\\napps, keep them healthy, scale them up and\\ndown, and more…\\nOverlay network A large flat layer-2 network that spans\\nmultiple swarm nodes. All containers on the\\nsame overlay network can communicate with\\neach other even if they’re on different Docker\\nhosts that are on different networks. The\\nbuilt-in overlay driver creates overlay\\nnetworks using advanced VXLAN\\ntechnologies. They are only supported by\\nDocker Swarm.\\nPush Upload an image to a registry.\\nPull Download an image from a registry.\\nReconciliation The process of watching the state of an\\napplication and ensuring observed state\\nmatches desired state. Docker Swarm runs\\nreconciliation loops, ensuring applications\\nrun how you want them to.\\nRegistry Central place for storing and retrieving\\nimages. We sometimes call themOCI registries,\\ncontainer registries,or Docker registries.\\nRepository An area of a registry where you store related\\ncontainer images. You can set access controls\\nper repository.\\nSeccomp Secure computing Linux kernel feature used\\nby Docker to restrict the syscalls available to a\\ncontainer.\\nSecret The way Docker Swarm lets you inject\\nsensitive data into a container at run-time.\\nService Capital “S” is a Docker Swarm feature that\\naugments containers with self-healing,\\nscaling, rollouts, and rollbacks.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 278}, page_content='Terminology 272\\nTerm Definition (according to Nigel)\\nSpin Framework that makes it easy to build,\\ndeploy, and run Wasm apps. Docker Desktop\\nships with thespin runtime. Created by\\nFermyon Technologies, Inc.\\nSwarm (also known as Docker Swarm Docker’s native orchestration platform. A\\nlightweight and easy alternative to\\nKubernetes.\\nVolume Where containers store important data they\\nneed to keep. You can create and delete\\nvolumes independently from containers.\\nWasm See WebAssembly.\\nWebAssembly Also known as Wasm. New virtual machine\\narchitecture that is smaller, faster, more\\nportable, and more secure than traditional\\ncontainers. Wasm apps run anywhere with a\\nWasm runtime.\\nYAML Yet Another Markup Language. You write\\nCompose files in YAML. It’s a superset of\\nJSON.'),\n",
              " Document(metadata={'source': '/content/docker-deep-dive-nigel-poulton 2024.pdf', 'page': 279}, page_content='Index\\nplaceholder just to create index')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages = [doc.page_content for doc in documents]"
      ],
      "metadata": {
        "id": "oAxHcWsuL3Vq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agdSBmZ6L3tW",
        "outputId": "7fcd4093-bda8-46a1-a5c9-bf3278941110"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'Docker Deep Dive\\nZero to Docker in a single book!\\nNigel Poulton\\nThis book is for sale athttp://leanpub.com/dockerdeepdive\\nThis version was published on 2024-05-21\\nISBN 9781916585133\\nThis is aLeanpub book. Leanpub empowers authors and publishers with the Lean\\nPublishing process.Lean Publishingis the act of publishing an in-progress ebook using\\nlightweight tools and many iterations to get reader feedback, pivot until you have the\\nright book and build traction once you do.\\n© 2016 - 2024 Nigel Poulton',\n",
              " 'Huge thanks to my wife and kids for putting up with a geek in the house who genuinely thinks\\nhe’s a bunch of software running inside of a container on top of midrange biological hardware. It\\ncan’t be easy living with me!\\nMassive thanks as well to everyone who watches my Pluralsight videos. I love connecting with you\\nand really appreciate all the feedback I’ve gotten over the years. This was one of the major reasons\\nI decided to write this book! I hope it’ll be an amazing tool to help you drive your careers even\\nfurther forward.',\n",
              " 'Contents\\n0: About the book. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1\\nPart 1: The big picture stuff. . . . . . . . . . . . . . . . . . . . .4\\n1: Containers from 30,000 feet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\\nThe bad old days. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nHello VMware! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nVMwarts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nHello Containers!. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\nLinux containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\nHello Docker! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\nDocker and Windows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\nWhat about WebAssembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\nWhat about Kubernetes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2: Docker and container-related standards and projects. . . . . . . . . . . . . . . .10\\nDocker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\nContainer-related standards and projects. . . . . . . . . . . . . . . . . . . . . . . . 12\\n3: Getting Docker. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16\\nDocker Desktop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nInstalling Docker with Multipass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nInstalling Docker on Linux. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4: The big picture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22\\nThe Ops Perspective. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\nThe Dev Perspective. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nPart 2: The technical stuff. . . . . . . . . . . . . . . . . . . . . . .30\\n5: The Docker Engine. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .31',\n",
              " 'CONTENTS\\nDocker Engine – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\nThe Docker Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\nThe influence of the Open Container Initiative (OCI). . . . . . . . . . . . . . . . . 34\\nrunc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\ncontainerd . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\nStarting a new container (example). . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\nWhat’s the shim all about?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\nHow it’s implemented on Linux. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n6: Working with Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40\\nDocker images – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\nIntro to images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nPulling images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\nImage registries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\nImage naming and tagging. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nImages and layers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nPulling images by digest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\nMulti-architecture images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\nVulnerability scanning with Docker Scout. . . . . . . . . . . . . . . . . . . . . . . . 61\\nDeleting Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\nImages – The commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\\n7: Working with containers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\\nContainers – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\nContainers vs VMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\nImages and Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\nCheck Docker is running. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\nStarting a container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\nHow containers start apps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\\nConnecting to a container. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\nInspecting container processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\nThe docker inspect command . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\nWriting data to a container. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\nStopping, restarting, and deleting a container. . . . . . . . . . . . . . . . . . . . . . 80\\nKilling a container’s main process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\nDebugging slim images and containers with Docker Debug. . . . . . . . . . . . . 84\\nSelf-healing containers with restart policies. . . . . . . . . . . . . . . . . . . . . . . 89\\nContainers – The commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n8: Containerizing an app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94\\nContainerizing an app – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nContainerize a single-container app. . . . . . . . . . . . . . . . . . . . . . . . . . . 95\\nMoving to production with multi-stage builds. . . . . . . . . . . . . . . . . . . . .106',\n",
              " 'CONTENTS\\nBuildx, BuildKit, drivers, and Build Cloud. . . . . . . . . . . . . . . . . . . . . . . . 111\\nMulti-architecture builds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .113\\nA few good practices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .116\\nContainerizing an app – The commands. . . . . . . . . . . . . . . . . . . . . . . . .118\\n9: Multi-container apps with Compose. . . . . . . . . . . . . . . . . . . . . . . . . . .120\\nDocker Compose – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .120\\nCompose background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\nInstalling Compose. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\nThe sample app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .122\\nCompose files. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\\nDeploying apps with Compose – The commands. . . . . . . . . . . . . . . . . . . .133\\n10: Docker Swarm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135\\nDocker Swarm – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .135\\nSwarm primer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .136\\nBuild a secure swarm cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\nDocker Swarm – The Commands. . . . . . . . . . . . . . . . . . . . . . . . . . . . .158\\n11: Deploying apps with Docker Stacks. . . . . . . . . . . . . . . . . . . . . . . . . .160\\nDeploying apps with Docker Stacks – The TLDR. . . . . . . . . . . . . . . . . . . .160\\nBuild a Swarm lab. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\\nThe sample app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .162\\nDeploy the app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168\\nManaging the app . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\\nDeploying apps with Docker Stacks – The Commands. . . . . . . . . . . . . . . .176\\n12: Docker and WebAssembly. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .177\\nPre-reqs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .178\\nIntro to Wasm and Wasm containers. . . . . . . . . . . . . . . . . . . . . . . . . . .180\\nWrite a Wasm app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nContainerize a Wasm app. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183\\nRun a Wasm container. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\\nClean up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .186\\nChapter summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .186\\n13: Docker Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .188\\nDocker Networking – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . .189\\nDocker networking theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .189\\nSingle-host bridge networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .193\\nExternal access via port mappings. . . . . . . . . . . . . . . . . . . . . . . . . . . . .200\\nDocker Networking – The Commands. . . . . . . . . . . . . . . . . . . . . . . . . . 214\\n14: Docker overlay networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .216',\n",
              " 'CONTENTS\\nDocker overlay networking – The TLDR. . . . . . . . . . . . . . . . . . . . . . . .216\\nDocker overlay networking history. . . . . . . . . . . . . . . . . . . . . . . . . . . .216\\nBuilding and testing Docker overlay networks. . . . . . . . . . . . . . . . . . . . . 217\\nOverlay networks explained. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\\nDocker overlay networking – The commands. . . . . . . . . . . . . . . . . . . . . .229\\n15: Volumes and persistent data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .231\\nVolumes and persistent data – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . 231\\nContainers without volumes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232\\nContainers with volumes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .233\\nVolumes and persistent data – The Commands. . . . . . . . . . . . . . . . . . . . .240\\n16: Docker security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .242\\nDocker security – The TLDR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .242\\nKernel Namespaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .243\\nControl Groups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .246\\nCapabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .246\\nMandatory Access Control systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\nseccomp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\nDocker security technologies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .248\\nSwarm security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .248\\nDocker Scout and vulnerability scanning. . . . . . . . . . . . . . . . . . . . . . . .255\\nSigning and verifying images with Docker Content Trust. . . . . . . . . . . . . . .258\\nDocker Secrets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\\nWhat next . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .264\\nTerminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .266\\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .273',\n",
              " '0: About the book\\nThis is a book about Docker and containers; no prior knowledge required! In fact, the\\nbook’s motto isZero to Docker in a single book.\\nSo, if you want to work with cloud and cloud-native technologies, this book is dedicated\\nto you.\\nWhy should I read this book or care about Docker?\\nDocker is here, and it’s changed the world. If you want the best jobs working with\\nthe best technologies, you need to know Docker and containers. They’re even central\\nto Kubernetes, and a strong Docker skill set will help you learn Kubernetes. Docker\\nand containers are also well-positioned for emerging cloud technologies such as\\nWebAssembly and AI workloads.\\nWhat if I’m not a developer\\nMost applications, even modern cloud-native microservices, need high-performance\\nproduction-grade infrastructure. If you think traditional developers will take care of\\nthis, think again. To cut a long story short, if you want to thrive in the modern cloud-\\nfirst world, you must know Docker. But don’t stress, this book will give you all the skills\\nyou need.\\nHow I’ve organized the book\\nI’ve divided the book into two main sections:\\n1. The big picture stuff\\n2. The technical stuff\\nThe big picture stuffgets you up to speed with things like what Docker is, why we\\nhave containers, and the fundamental jargon such ascloud-native, microservices,and\\norchestration.\\nThe technical stuffsection covers everything you need to know aboutimages, containers,\\nmulti-container microservices apps,and the increasingly important topic oforchestration.\\nIt even covers WebAssembly, vulnerability scanning with Docker Scout, debugging\\ncontainers, high availability, and more.',\n",
              " '0: About the book 2\\nChapter breakdown\\n• Chapter 1:Summarises the history and potential future of Docker and containers\\n• Chapter 2:Explains the most important container-related standards and projects\\n• Chapter 3:Shows you a few ways to get Docker\\n• Chapter 4:Walks you through a very simple hands-on container workflow\\n• Chapter 5:Explains the architecture of the Docker Engine\\n• Chapter 6:Dives deep into images and image management\\n• Chapter 7:Dives deep into containers and container management\\n• Chapter 8:Walks you through the process of containerizing an app\\n• Chapter 9:Shows you how to build, deploy, and manage multi-container apps\\nwith Compose\\n• Chapter 10:Walks you through building a secure swarm\\n• Chapter 11:Deploys and manages a multi-container app on a secure swarm\\n• Chapter 12:Walks you through building and containerizing a WebAssembly app\\n• Chapter 13:Dives into Docker networking\\n• Chapter 14:Builds and tests Docker overlay networks\\n• Chapter 15:Introduces you to persistent and non-persistent data in Docker\\n• Chapter 16:Covers all the major Linux and Docker security technologies\\nEditions and updates\\nDocker and the cloud-native ecosystem are evolving fast, and a 2-3-year-old book on\\nDocker isn’t valuable. As a result, I’m committed to updating the book every year.\\nIf that sounds excessive, welcome to the new normal.\\nThe book is available in hardback, paperback, and e-book on all good book publishing\\nplatforms.\\nWhen you purchase the Kindle edition, you’re entitled to all future updates. However,\\nKindle doesn’t always download the latest edition.\\nA potential solution is to go tohttp://amzn.to/2l53jdg and chooseQuick Solutions.\\nThen selectDigital Purchases, search for your Docker Deep Dive Kindle edition\\npurchase, and selectContent and Devices. Your purchase should appear in the list with\\na button that saysUpdate Available. Click that button. Delete your old version on your\\nKindle and download the new one.\\nIf this doesn’t work, your only option is to contact Kindle Support.',\n",
              " '0: About the book 3\\nFeedback\\nIf you like the book and it helps your career, share the love by recommending it to a\\nfriend and leaving a review on Amazon or Goodreads.\\nIf you spot a typo or want to make a recommendation, email me atddd@nigelpoulton.com\\nThat’s everything. Let’s get rocking with Docker!',\n",
              " 'Part 1: The big picture stuff',\n",
              " '1: Containers from 30,000 feet\\nContainers have taken over the world!\\nIn this chapter, you’ll learn why we have containers, what they do for us, and where we\\ncan use them.\\nThe bad old days\\nApplications are the powerhouse of every modern business. When applications break,\\nbusinesses break.\\nMost applications run on servers, and in the past, we were limited to running one\\napplication per server. As a result, the story went something like this:\\nEvery time a business needed a new application, it had to buy a new server. Unfor-\\ntunately, we weren’t very good at modeling the performance requirements of new\\napplications, and the IT departments had to guess. This often resulted in businesses\\nbuying very expensive servers with a lot more performance capability than the apps\\nneeded. After all, nobody wanted underpowered servers incapable of handling the app,\\nresulting in unhappy customers and lost revenue. As a result, companies ended up with\\nracks and racks of overpowered servers operating as low as 5-10% of their potential\\ncapacity. This was a tragic waste of company capital and environmental resources!\\nHello VMware!\\nAmid all this, VMware, Inc. gave the world a gift — thevirtual machine (VM).\\nAs soon as VMware came along, the world became much better. We finally had a\\ntechnology that allowed us to safely run multiple business applications on a single\\nserver.\\nIt was a game-changer. Businesses could run new apps on the spare capacity of existing\\nservers, spawning a golden age of maximizing the value of existing assets.\\nVMwarts\\nBut, and there’s always abut! As great as VMs are, they’re far from perfect.',\n",
              " '1: Containers from 30,000 feet 6\\nA feature of the VM model is every VM needing its own dedicated operating system\\n(OS). Unfortunately, this has several drawbacks, including:\\n• Every OS consumes CPU, RAM, and other resources we’d rather use on applica-\\ntions\\n• Every VM and OS needs patching\\n• Every VM and OS needs monitoring\\nVMs are also slow to boot and not very portable.\\nHello Containers!\\nWhile most of us were reaping the benefits of VMs, web scalers like Google had already\\nmoved on from VMs and were using containers.\\nA feature of the container model is that every container shares the OS of the host it’s\\nrunning on. This means a single host can run more containers than VMs. For example,\\na host that can run 10 VMs might be able to run 50 containers, making containers far\\nmore efficient than VMs.\\nContainers are also faster and more portable than VMs.\\nLinux containers\\nModern containers started in the Linux world and are the product of incredible work\\nfrom many people over many years. For example, Google contributed many container-\\nrelated technologies to the Linux kernel. It’s thanks to many contributions like these\\nthat we have containers today.\\nSome of the major technologies behind modern containers include;kernel namespaces,\\ncontrol groups (cgroups), capabilities,and more.\\nHowever, despite all this great work, containers were incredibly complicated, and it\\nwasn’t until Docker came along that they became accessible to the masses.\\nNote: I know that many container-like technologies pre-date Docker and\\nmodern containers. However, none of them changed the world the way\\nDocker has.',\n",
              " '1: Containers from 30,000 feet 7\\nHello Docker!\\nDocker was the magic that made Linux containers easy and brought them to the masses.\\nWe’ll talk a lot more about Docker in the next chapter.\\nDocker and Windows\\nMicrosoft worked hard to bring Docker and container technologies to the Windows\\nplatform.\\nAt the time of writing, Windows desktop and server platforms support both of the\\nfollowing:\\n• Windows containers\\n• Linux containers\\nWindows containersrun Windows apps and require a host system with a Windows kernel.\\nWindows 10, Windows 11, and all modern versions of Windows Server natively support\\nWindows containers.\\nWindows systems can also run Linux containers via theWSL 2 (Windows Subsystem for\\nLinux) subsystem.\\nThis means Windows 10 and Windows 11 are great platforms for developing and testing\\nWindows and Linux containers.\\nHowever, despite all the work developingWindows containers, almost all containers are\\nLinux containers. This is because Linux containers are smaller and faster, and more\\ntooling exists for Linux.\\nAll of the examples in this edition of the book are Linux containers.\\nWindows containers vs Linux containers\\nIt’s vital to understand that containers share the kernel of the host they’re running on.\\nThis means containerized Windows apps need a host with a Windows kernel, whereas\\ncontainerized Linux apps need a host with a Linux kernel. However, as mentioned, you\\ncan run Linux containers on Windows systems that have the WSL 2 backend installed.',\n",
              " '1: Containers from 30,000 feet 8\\nWhat about Mac containers?\\nThere is no such thing as Mac containers. However, Macs are great platforms for\\nworking with containers, and I do all of my daily work with containers on a Mac.\\nThe most popular way of working with containers on a Mac isDocker Desktop. It works\\nby running Docker inside a lightweight Linux VM on your Mac. Other tools, such as\\nPodman and Rancher Desktop, are also great for working with containers on a Mac.\\nWhat about WebAssembly\\nWebAssembly (Wasm) is a modern binary instruction set that builds applications that are\\nsmaller, faster, more secure, and more portable than containers. You write your app in\\nyour favorite language and compile it as a Wasm binary, and it’ll run anywhere you have\\na Wasm runtime.\\nHowever, Wasm apps have many limitations, and we’re still developing many of\\nthe standards. As a result, containers remain the dominant model for cloud-native\\napplications.\\nThe container ecosystem is also much richer and more mature than the Wasm ecosys-\\ntem.\\nAs you’ll see in the Wasm chapter, Docker and the container ecosystem are adapting\\nto work with Wasm apps, and you should expect a future where VMs, containers, and\\nWasm apps run side-by-side in most clouds and applications.\\nThis book is up-to-date with the latest Wasm and container developments.\\nWhat about Kubernetes\\nKubernetes is the industry standard platform for deploying and managing containerized\\napps.\\nTerminology: A containerized appis an application running as a container.\\nWe’ll cover this in a lot of detail later.\\nOlder versions of Kubernetes used Docker to start and stop containers. However, newer\\nversions usecontainerd, which is a stripped-down version of Docker optimized for use\\nby Kubernetes and other platforms.\\nThe important thing to know is that all Docker containers work on Kubernetes.\\nCheck out these resources if you need to learn Kubernetes:',\n",
              " '1: Containers from 30,000 feet 9\\n• Quick Start Kubernetes:This is\\x18100 pages and will get you up-to-speed with\\nKubernetes inone day!\\n• The Kubernetes Book. This is the ultimate book for mastering Kubernetes.\\nI update both books annually to ensure they’re up-to-date with the latest and greatest\\ndevelopments in the cloud native ecosystem, including WebAssembly.\\nChapter Summary\\nWe used to live in a world where every time the business needed a new application,\\nwe had to buy a brand-new server. VMware came along and allowed us to drive more\\nvalue out of new and existing servers. However, following the success of VMware and\\nhypervisors came a newer, more efficient, and portable virtualization technology called\\ncontainers. However, containers were complex and hard to implement until Docker came\\nalong and made them easy. WebAssembly is powering a third wave of cloud computing,\\nbut Docker and the container ecosystem are evolving to work with WebAssembly, and\\nthe book has an entire chapter dedicated to Docker and WebAssembly.',\n",
              " '2: Docker and container-related\\nstandards and projects\\nThis chapter introduces you to Docker and some of the most important standards and\\nprojects shaping the container ecosystem. The goal is to lay some foundations that we’ll\\nbuild on in later chapters.\\nThis chapter has two main parts:\\n• Docker\\n• Container-related standards and projects\\nDocker\\nDocker is at the heart of the container ecosystem. However, the termDocker can mean\\ntwo things:\\n1. The Docker platform\\n2. Docker, Inc.\\nThe Docker platformis a neatly packaged collection of technologies for creating, manag-\\ning, and orchestrating containers.Docker, Inc.is the company that created the Docker\\nplatform and continues to be the driving force behind developing new features.\\nLet’s dive a bit deeper.\\nDocker, Inc.\\nDocker, Inc. is a technology company based out of Palo Alto and founded by French-\\nborn American developer and entrepreneur Solomon Hykes. Solomon is no longer at\\nthe company.\\nThe company started as aplatform as a service (PaaS)provider calleddotCloud. Behind the\\nscenes, dotCloud delivered their services on top of containers and had an in-house to\\nhelp them deploy and manage those containers. They called this in-house toolDocker.\\nThe wordDocker is a British expression meaningdock work____er____ that refers to a\\nperson who loads and unloads cargo from ships.',\n",
              " '2: Docker and container-related standards and projects 11\\nIn 2013, dotCloud dropped the struggling PaaS side of the business, rebranded as\\nDocker, Inc., and focussed on bringing Docker and containers to the world.\\nThe Docker technology\\nThe Docker platform is designed to make it as easy as possible tobuild, ship,and run\\ncontainers.\\nAt a high level, there are two major parts to the Docker platform:\\n• The CLI (client)\\n• The engine (server)\\nThe CLI is the familiardocker command-line tool for deploying and managing contain-\\ners. It converts simple commands into API requests and sends them to the engine.\\nThe engine comprises all the server-side components that run and manage containers.\\nFigure 2.1 shows the high-level architecture. The client and engine can be on the same\\nhost or connected over the network.\\nFigure 2.1 Docker client and engine.\\nIn later chapters, you’ll see that the client and engine are complex and comprise a lot of\\nsmall specialized parts. Figure 2.2 gives you an idea of some of the complexity behind\\nthe engine. However, the client hides all this complexity so you don’t have to care. For\\nexample, you type friendlydocker commands into the CLI, the CLI converts them to\\nAPI requests and sends them to the daemon, and the daemon takes care of everything\\nelse.',\n",
              " '2: Docker and container-related standards and projects 12\\nFigure 2.2 Docker CLI and daemon hiding complexity.\\nLet’s switch focus and briefly look at some standards and governance bodies.\\nContainer-related standards and projects\\nThere are several important standards and governance bodies influencing the develop-\\nment of containers and the container ecosystem. Some of these include:\\n• The OCI\\n• The CNCF\\n• The Moby Project\\nThe Open Container Initiative (OCI)\\nThe Open Container Initiative (OCI)1 is a governance council responsible for low-level\\ncontainer-related standards.\\nIt operates under the umbrella of theLinux Foundation2 and was founded in the early\\ndays of the container ecosystem when some of the people at a company called CoreOS\\ndidn’t like the way Docker was dominating the ecosystem. In response, CoreOS created\\nan open standard calledappc3 that defined specifications for things such as image\\nformat and container runtime. They also created a reference implementation calledrkt\\n(pronounced “rocket”).\\nThe appc standard did things differently from Docker and put the ecosystem in an\\nawkward position with two competingstandards.\\nWhile competition is usually a good thing,competing standardsare generally bad, as they\\ngenerate confusion that slows down user adoption. Fortunately, the main players in the\\n1https://www.opencontainers.org\\n2https://www.linuxfoundation.org/projects\\n3https://github.com/appc/spec/',\n",
              " '2: Docker and container-related standards and projects 13\\necosystem came together and formed the OCI as a vendor-neutral lightweight council\\nto govern container standards. This allowed us to archive the appc project and place all\\nlow-level container-related specifications under the OCI’s governance.\\nAt the time of writing, the OCI maintains three standards calledspecs:\\n• The image-spec4\\n• The runtime-spec5\\n• The distribution-spec6\\nWe often use arail tracksanalogy when explaining the OCI standards:\\nWhen the size and properties of rail tracks were standardized, it gave entrepreneurs\\nin the rail industry confidence the trains, carriages, signaling systems, platforms, and\\nother rail infrastructure they built would work with the standardized tracks — nobody\\nwanted competing standards for track sizes.\\nThe OCI specifications did the same thing for the container ecosystem and it’s flour-\\nished ever since. Docker has also changed a lot since the formation of the OCI, and all\\nmodern versions of Docker implement all three OCI specs. For example:\\n• The Docker builder (BuildKit) createsOCI compliant-images\\n• Docker uses anOCI-compliant runtimeto createOCI-compliant containers\\n• Docker Hub implements the OCI distribution spec and is anOCI-compliant registry\\nDocker, Inc. and many other companies have people on the technical oversight board\\n(TOB) of the OCI.\\nThe CloudNative Computing Foundation (CNCF)\\nThe Cloud Native Computing Foundation (CNCF)7 is another Linux Foundation\\nproject that is influential in the container ecosystem. It was founded in 2015 with the\\ngoal of“…advancing container technologies… and making cloud native computing ubiquitous”.\\nInstead of creating and maintaining container-related specifications, the CNCFhosts\\nimportant projects such as Kubernetes, containerd, Notary, Prometheus, Cilium, and\\nlots more.\\nWhen we say the CNCFhosts these projects, we mean it provides a space, structure, and\\nsupport for projects to grow and mature. For example, all CNCF projects pass through\\nthe following three phases or stages:\\n4https://github.com/opencontainers/image-spec\\n5https://github.com/opencontainers/runtime-spec\\n6https://github.com/opencontainers/distribution-spec\\n7https://www.cncf.io/',\n",
              " '2: Docker and container-related standards and projects 14\\n• Sandbox\\n• Incubating\\n• Graduated\\nEach phase increases a project’s maturity level by requiring higher standards of gov-\\nernance, documentation, auditing, contribution tracking, marketing, community\\nengagement, and more. For example, new projects accepted as sandbox projects may\\nhave great ideas and great technology but need help and resources to create strong\\ngovernance, etc. The CNCF helps with all of that.\\nGraduated projects are consideredready for productionand are guaranteed to have strong\\ngovernance and implement good practices.\\nIf you look back to Figure 2.2, you’ll see that Docker uses at least two CNCF technolo-\\ngies — containerd and Notary.\\nThe Moby Project\\nDocker created theMoby projectas a community-led place for developers to build\\nspecialized tools for building container platforms.\\nPlatform builders can pick the specific Moby tools they need to build their container\\nplatform. They can even compose their platforms from a mix of Moby tools, in-house\\ntools, and tools from other projects.\\nDocker, Inc. originally created the Moby project, but it now has members including\\nMicrosoft, Mirantis, and Nvidia.\\nThe Docker platform is built using tools from various projects, including the Moby\\nproject, the CNCF, and the OCI.\\nChapter summary\\nThis chapter introduced you to Docker and some of the major influences in the\\ncontainer ecosystem.\\nDocker, Inc., is a technology company based in Palo Alto that is changing how we do\\nsoftware. They were thefirst moversand instigators of the modern container revolution.\\nThe Docker platform focuses on running and managing application containers. It runs\\non Linux and Windows, can be installed almost anywhere, and offers a variety of free\\nand paid-for products.\\nThe Open Container Initiative (OCI) governs low-level container standards and\\nmaintains specifications for runtimes, image format, and registries.',\n",
              " '2: Docker and container-related standards and projects 15\\nThe CNCF hosts important cloud-native projects and helps them mature into produc-\\ntion-grade tools.\\nThe Moby project hosts low-level tools developers can use to build container platforms.',\n",
              " '3: Getting Docker\\nThere are lots of ways to get Docker and work with containers. This chapter will show\\nyou the following ways:\\n• Docker Desktop\\n• Multipass\\n• Server installs on Linux\\nDocker Desktop is the best way to work with Docker and gets you the complete Docker\\nexperience on your laptop with all the latest tools, plugins, and extensions. I use it every\\nday, and I recommend it to everyone.\\nWe’ll also show you how to install Docker on your laptop with Multipass and a super-\\nsimple Linux installation. However, you should only consider these if you can’t use\\nDocker Desktop, as they offer fewer features.\\nDocker Desktop\\nDocker Desktop is a desktop app from Docker, Inc. and is the best way to work with\\ncontainers. You get the Docker Engine, a slick UI, all the latest plugins and features, and\\nan extension system with a marketplace. You even get Docker Compose and a single-\\nnode Kubernetes cluster if you want to learn Kubernetes.\\nIt’s free for personal use and education, but you’ll have to pay a license fee if you use it\\nfor work and your company has over 250 employees or does more than $10M in annual\\nrevenue.\\nDocker Desktop on Windows 10 and Windows 11 Professional and Enterprise editions\\nsupports Windows containersand Linux containers.Docker Desktop on Mac, Linux, and\\nHome editions of Windows only supportLinux containers.All of the examples in the\\nbook and almost all of the containers in the real world are Linux containers.\\nLet’s install Docker Desktop on Windows and MacOS.\\nWindows prereqs\\nDocker Desktop on Windows requires all of the following:',\n",
              " '3: Getting Docker 17\\n• 64-bit version of Windows 10/11\\n• Hardware virtualization support must be enabled in your system’s BIOS\\n• WSL 2\\nBe very careful changing anything in your system’s BIOS.\\nInstalling Docker Desktop on Windows 10 and 11\\nSearch the internet for _“install Docker Desktop on Windows”. This will take you\\nto the relevant download page, where you can download the installer and follow\\nthe instructions. When prompted, you should install and enable the WSL 2 backend\\n(Windows Subsystem for Linux).\\nOnce the installation is complete, you need to manually start Docker Desktop from the\\nWindows Start menu. It may take a minute to start, but you can watch the start progress\\nvia the animated whale icon on the Windows taskbar at the bottom of the screen.\\nOnce it’s running, you can open a terminal and type some simpledocker commands.\\n$ docker version\\n<Snip>\\nServer: Docker Desktop 4.30.0 (149282)\\nEngine:\\nVersion: 26.1.1\\nAPI version: 1.45 (minimum version 1.24)\\nGo version: go1.21.9\\nBuilt: Tue Apr 30 11:48:28 2024\\nOS/Arch: linux/amd64\\n<Snip>\\nNotice how theServer output showsOS/Arch: linux/amd64. This is because a default\\ninstallation assumes you’ll be working with Linux containers.\\nSome versions of Windows let you switch toWindows containersby right-clicking the\\nDocker whale icon in the Windows notifications tray and selectingSwitch to Windows\\ncontainers…. Doing this keeps existing Linux containers running in the background,\\nbut you won’t be able to see or manage them until you switch back to Linux containers\\nmode.\\nCongratulations. You now have a working installation of Docker on your Windows\\nmachine.\\nMake sure you’re running inLinux containers modeso you can follow along with the\\nexamples later in the book.',\n",
              " '3: Getting Docker 18\\nInstalling Docker Desktop on Mac\\nDocker Desktop for Mac is like Docker Desktop for Windows — a packaged product\\nwith a slick UI that gets you the full Docker experience on your laptop. You can also\\nenable the built-in single-node Kubernetes cluster.\\nBefore proceeding with the installation, you need to know that Docker Desktop on\\nMac installs the daemon and server-side components inside a lightweight Linux VM\\nthat seamlessly exposes the API to your local Mac environment. This means you can\\nopen a terminal on your Mac and rundocker commands without ever knowing it’s all\\nrunning in a hidden VM. This is also why Mac versions of Docker Desktop only work\\nwith Linux containers — everything’s running inside a Linux VM.\\nFigure 3.1 shows the high-level architecture for Docker Desktop on Mac.\\nFigure 3.1\\nThe simplest way to install Docker Desktop on your Mac is to search the web for“install\\nDocker Desktop on MacOS”, follow the links to the download, and then complete the\\nsimple installer.\\nWhen the installer finishes, you’ll have to start Docker Desktop from the MacOS\\nLaunchpad manually. It may take a minute to start, but you can watch the animated\\nDocker whale icon in the status bar at the top of your screen. Once it’s started, you can\\nclick the whale icon to manage Docker Desktop.\\nOpen a terminal window and run some regular Docker commands. Try the following.',\n",
              " '3: Getting Docker 19\\n$ docker version\\nClient:\\nCloud integration: v1.0.35+desktop.13\\nVersion: 26.1.1\\nAPI version: 1.45\\nOS/Arch: darwin/arm64\\n<Snip>\\nServer: Docker Desktop 4.30.0 (149282)\\nEngine:\\nVersion: 26.1.1\\nAPI version: 1.45 (minimum version 1.24)\\nOS/Arch: linux/arm64\\ncontainerd:\\nVersion: 1.6.31\\nrunc:\\nVersion: 1.1.12\\ndocker-init:\\nVersion: 0.19.0\\n<Snip>\\nNotice that theOS/Arch: for theServer component shows aslinux/amd64 or lin-\\nux/arm64. This is because the daemon runs inside the Linux VM mentioned earlier. The\\nClient component is a native Mac application and runs directly on the Mac OS Darwin\\nkernel. This is why it shows asdarwin/amd64 or darwin/arm64.\\nYou can now use Docker on your Mac.\\nInstalling Docker with Multipass\\nOnly consider this section if you can’t use Docker Desktop, as Multipass installations\\ndon’t ship with out of the box support fordocker scout, docker debug, ordocker\\ninit.\\nMultipass is a free tool for creating cloud-style Linux VMs on your Linux, Mac, or\\nWindows machine and is incredibly easy to install and use. It’s an easy way to create\\nmulti-node production-like Docker clusters.\\nGo tohttps://multipass.run/install and install the right edition for your hardware\\nand OS.\\nOnce installed, you only need three commands:\\n$ multipass launch\\n$ multipass ls\\n$ multipass shell',\n",
              " '3: Getting Docker 20\\nLet’s see how to launch and connect to a new VM with Docker pre-installed.\\nRun the following command to create a new VM callednode1 based on thedocker\\nimage. The docker image has Docker pre-installed and ready to go.\\n$ multipass launch docker --name node1\\nIt’ll take a minute or two to download the image and launch the VM.\\nList VMs to make sure yours launched properly.\\n$ multipass ls\\nName State IPv4 Image\\nnode1 Running 192.168.64.37 Ubuntu 22.04 LTS\\n172.17.0.1\\n172.18.0.1\\nYou’ll use the192 IP address when working with the examples later in the book.\\nConnect to the VM with the following command.\\n$ multipass shell node1\\nOnce connected, you can run the following commands to check your Docker version\\nand list installed CLI plugins.\\n$ docker --version\\nDocker version 26.1.0, build 9714adc\\n$ docker info\\nClient: Docker Engine - Community\\nVersion: 26.1.0\\nContext: default\\nDebug Mode: false\\nPlugins:\\nbuildx: Docker Buildx (Docker Inc.)\\nVersion: v0.14.0\\nPath: /usr/libexec/docker/cli-plugins/docker-buildx\\ncompose: Docker Compose (Docker Inc.)\\nVersion: v2.26.1\\nPath: /usr/libexec/docker/cli-plugins/docker-compose\\n<Snip>\\nThe installation in the example only has thebuildx and compose CLI plugins. You’ll\\nhave to manually install the relevant plugins if you want to follow the Docker Scout,\\nDocker Init, and Docker Debug examples later in the book.\\nYou can typeexit to log out of the VM, andmultipass shell node1 to log back in.\\nYou can also typemultipass delete node1 and thenmultipass purge to delete it.',\n",
              " '3: Getting Docker 21\\nInstalling Docker on Linux\\nOnly consider this section if you can’t use Docker Desktop. This installation doesn’t\\ninstall thedocker scout, docker debug, ordocker init CLI plugins that we’ll use in\\nsome of the later chapters.\\nThese instructions show you how to install Docker on Ubuntu Linux 22.04 and are just\\nfor guidance purposes. Lots of other installation methods exist, and you should search\\nthe web for the latest instructions.\\n$ sudo snap install docker\\ndocker 24.0.5 from Canonical ✓ installed\\nRun some commands to test the installation. You’ll have to prefix them withsudo.\\n$ sudo docker --version\\nDocker version 24.0.5, build ced0996\\n$ sudo docker info\\n<Snip>\\nServer:\\nContainers: 0\\nRunning: 0\\nPaused: 0\\nStopped: 0\\nImages: 0\\nServer Version: 24.0.5\\n<Snip>\\nChapter Summary\\nYou can run Docker almost anywhere, and installing it’s easier than ever.\\nDocker Desktop gives you a fully functional Docker environment on your Linux, Mac,\\nor Windows machine and is the best way to get a Docker development environment\\non your local machine. It’s easy to install, includes the Docker Engine, has a slick UI,\\nand has a marketplace with lots of extensions to extend its capabilities. It works with\\ndocker scout, docker debug, anddocker init, and it even lets you spin up a single-\\nnode Kubernetes cluster.\\nMultipass is a great way to spin up a local VM running Docker, and there are lots of\\nways to install Docker on Linux servers. These give you access to most of the free\\nDocker features but lack some of the features of Docker Desktop.',\n",
              " '4: The big picture\\nThis chapter will give you some hands-on experience and a high-level view of images\\nand containers. The goal is to prepare you for more detail in the upcoming chapters.\\nWe’ll break this chapter into two parts:\\n• The Ops perspective\\n• The Dev perspective\\nThe ops perspective focuses on starting, stopping, deleting containers, and executing\\ncommands inside them.\\nThe dev perspective focuses more on the application side of things and runs through\\ntaking application source code, building it into a container image, and running it as a\\ncontainer.\\nI recommend you read both sections and follow the examples, as this will give you the\\ndev and ops perspectives. DevOps anyone?\\nThe Ops Perspective\\nIn this section, you’ll complete all of the following:\\n• Check Docker is working\\n• Download an image\\n• Start a container from the image\\n• Execute a command inside the container\\n• Delete the container\\nA typical Docker installation installs the client and the engine on the same machine and\\nconfigures them to talk to each other.\\nRun adocker version command to ensure both are installed and running.',\n",
              " '4: The big picture 23\\n$ docker version\\nClient: <<---- Start of client response\\nCloud integration: v1.0.35+desktop.13 -----┐\\nVersion: 26.1.1 |\\nAPI version: 1.45 | Client info block\\nGo version: go1.21.9 |\\nOS/Arch: darwin/arm64 |\\nContext: desktop-linux -----┘\\nServer: Docker Desktop 4.30.0 (149282) <<---- Start of server response\\nEngine: -----┐\\nVersion: 26.1.1 |\\nAPI version: 1.45 (minimum version 1.24) |\\nGo version: go1.21.9 |\\nOS/Arch: linux/arm64 |\\ncontainerd: | Server block\\nVersion: 1.6.31 |\\nrunc: |\\nVersion: 1.1.12 |\\ndocker-init: |\\nVersion: 0.19.0 -----┘\\nIf your response from the clientand server looks like the output in the book, everything\\nis working as expected.\\nIf you’re on Linux and get apermission denied while trying to connect to the\\nDocker daemon... error, try again withsudo in front of the command —sudo docker\\nversion. If it works withsudo, you’ll need to prefixall future docker commands with\\nsudo.\\nDownload an image\\nImages are objects that contain everything an app needs to run. This includes an OS\\nfilesystem, the application, and all dependencies. If you work in operations, they’re\\nsimilar to VM templates. If you’re a developer, they’re similar toclasses.\\nRun adocker images command.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nIf you are working from a clean installation, you’ll have no images, and your output\\nwill be the same as the book. If you’re working with Multipass, you might see an image\\ncalled protainer/protainer-ce.\\nCopying new images onto your Docker host is calledpulling. Pull theubuntu:latest\\nimage.',\n",
              " '4: The big picture 24\\n$ docker pull ubuntu:latest\\nlatest: Pulling from library/ubuntu\\nb91d8878f844: Download complete\\nDigest: sha256:e9569c25505f33ff72e88b2990887c9dcf230f23259da296eb814fc2b41af999\\nStatus: Downloaded newer image for ubuntu:latest\\ndocker.io/library/ubuntu:latest\\nRun anotherdocker images to confirm yourpull command worked.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nubuntu latest e9569c25505f 10 days ago 106MB\\nWe’ll discuss the details of where the image is stored and what’s inside it in later\\nchapters. For now, all you need to know is that images contain enough of an operating\\nsystem (OS) and all the code and dependencies required to run the application. The\\nUbuntu image you pulled includes a stripped-down version of the Ubuntu Linux\\nfilesystem and a few of the standard Linux utilities that ship with Ubuntu.\\nIf you pull anapplication container,such asnginx:latest, you’ll get an image with a\\nminimal OSand the code to run the NGINX app.\\nStart a container from the image\\nIf you’ve been following along, you’ll have a copy of theubuntu:latest image on your\\nDocker host, and you can use thedocker run command to start a container from it.\\nRun the followingdocker run command to start a new container calledtest from the\\nubuntu:latest image.\\n$ docker run --name test -it ubuntu:latest bash\\nroot@bbd2e5ad1817:/#\\nNotice how your shell prompt has changed. This is because the container is already\\nrunning and your shell is attached to it.\\nLet’s quickly examine thatdocker run command.\\ndocker run tells Docker to start a new container. The--name flag told Docker to call\\nthis containertest. Next, the-it flags told Docker to make the container interactive and\\nto attach your shell to the container’s terminal. After that, the command told Docker to\\nbase the container on theubuntu:latest image. Finally, it told Docker to start a Bash\\nshell as the container’s main app.\\nRun aps command from inside the container to list all running processes.',\n",
              " '4: The big picture 25\\nroot@bbd2e5ad1817:/# ps -elf\\nF S UID PID PPID NI ADDR SZ WCHAN STIME TTY TIME CMD\\n4 S root 1 0 0 - 4560 - 13:38 pts/0 00:00:00 /bin/bash\\n0 R root 9 1 0 - 8606 - 13:38 pts/0 00:00:00 ps -elf\\nThere are only two processes:\\n• PID 1 is the Bash process that we told the container to run\\n• PID 9 is theps -elf command we ran to list the running processes\\nThe presence of theps -elf process in the output can be a bit misleading as it is a short-\\nlived process that exits as soon as theps command completes. This means the only long-\\nrunning process inside the container is the/bin/bash process.\\nPress Ctrl-PQ to exit the container without terminating it. This will land your shell back\\nat your local terminal, and your shell prompt will revert.\\nRun the following command to verify yourtest container is still running.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\nbbd2e5ad1817 ubuntu:latest \"/bin/bash\" 7 mins Up 7 min test\\nThe output shows yourtest container, and you can see it was created 7 minutes ago and\\nhas been running for 7 minutes.\\nExecute a command inside the container\\nYou can use thedocker attach command to attach your shell to the container’s main\\nprocess.\\nRun the following command to attach your shell to the Bash process inside the con-\\ntainer.\\n$ docker attach test\\nroot@bbd2e5ad1817:/#\\nMake sure your shell prompt changes, indicating you successfully connected to the\\ncontainer.\\nExit the container again by typingCtrl-PQ and verify your shell prompt reverts to your\\nlocal machine.\\nDelete the container\\nRun anotherdocker ps to verify your container is still running.',\n",
              " '4: The big picture 26\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\nbbd2e5ad1817 ubuntu:latest \"/bin/bash\" 9 mins Up 9 min test\\nStop and kill the container using thedocker stop and docker rm commands.\\n$ docker stop test\\ntest\\nIt can take a few seconds for the container to stop.\\n$ docker rm test\\ntest\\nVerify the container was successfully deleted by running thedocker ps command with\\nthe -a flag to list all containers, even those in the stopped state.\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\nCongratulations, you’ve pulled a Docker image, started a container from it, logged in to\\nit, executed a command inside it, stopped it, and deleted it.\\nThe Dev Perspective\\nContainers are all about applications.\\nYou’ll complete all of the following steps in this section:\\n• Clone an app from a GitHub repo\\n• Inspect the app’sDockerfile\\n• Containerize the app\\n• Run the app as a container\\nRun the following command to make a local clone of the repo. This will copy the\\napplication code to your machine so you can containerize it in a future step. You’ll need\\nthe git CLI for this to work.',\n",
              " '4: The big picture 27\\n$ git clone https://github.com/nigelpoulton/psweb.git\\nCloning into \\'psweb\\'...\\nremote: Enumerating objects: 63, done.\\nremote: Counting objects: 100% (34/34), done.\\nremote: Compressing objects: 100% (22/22), done.\\nremote: Total 63 (delta 13), reused 25 (delta 9), pack-reused 29\\nReceiving objects: 100% (63/63), 13.29 KiB | 4.43 MiB/s, done.\\nResolving deltas: 100% (21/21), done.\\nChange into thepsweb directory and list its contents.\\n$ cd psweb\\n$ ls -l\\ntotal 32\\n-rw-r--r--@ 1 nigelpoulton staff 324 5 Feb 12:31 Dockerfile\\n-rw-r--r-- 1 nigelpoulton staff 378 5 Feb 12:31 README.md\\n-rw-r--r-- 1 nigelpoulton staff 341 5 Feb 12:31 app.js\\n-rw-r--r--@ 1 nigelpoulton staff 355 5 Feb 12:47 package.json\\ndrwxr-xr-x 3 nigelpoulton staff 96 5 Feb 12:31 views\\nThe app is a simple Node.js web app running some static HTML.\\nInspect the app’s Dockerfile\\nThe Dockerfile is a plain-text document that tells Docker how to build the app and\\ndependencies into an image.\\nList the contents of the application’s Dockerfile.\\n$ cat Dockerfile\\nFROM alpine\\nLABEL maintainer=\"nigelpoulton@hotmail.com\"\\nRUN apk add --update nodejs npm curl\\nCOPY . /src\\nWORKDIR /src\\nRUN npm install\\nEXPOSE 8080\\nENTRYPOINT [\"node\", \"./app.js\"]\\nYou’ll learn more about Dockerfiles later in the book. Right now, all you need to know is\\nthat each line represents an instruction Docker executes to build the app into an image.\\nIf you’ve been following along, you’ve pulled some application code from a remote Git\\nrepo and looked at the application’s Dockerfile.',\n",
              " '4: The big picture 28\\nContainerize the app\\nRun the followingdocker build command to create a new image based on the instruc-\\ntions in the Dockerfile. It will create a new Docker image calledtest:latest.\\nBe sure to run the command from within thepsweb directory.\\n$ docker build -t test:latest .\\n[+] Building 36.2s (11/11) FINISHED\\n=> [internal] load .dockerignore 0.0s\\n=> => transferring context: 2B 0.0s\\n=> [internal] load build definition from Dockerfile 0.0s\\n<Snip>\\n=> => naming to docker.io/library/test:latest 0.0s\\n=> => unpacking to docker.io/library/test:latest 0.7s\\nOnce the build is complete, check that you have an image calledtest:latest.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\ntest latest 0435f2738cf6 21 seconds ago 160MB\\nCongratulations, you’vecontainerized the app. That’s jargon for building it into a\\ncontainer image that contains the app and dependencies.\\nRun the app as a container\\nRun the following command to start a container calledweb1 from the image. If you’re\\non a Windows machine, you’ll need to replace the backslashes with backticks or run the\\ncommand on a single line without the backslashes.\\n$ docker run -d \\\\\\n--name web1 \\\\\\n--publish 8080:8080 \\\\\\ntest:latest\\nOpen a web browser and navigate to the DNS name or IP address of your Docker host\\non port8080. If you’re following along on Docker Desktop, connect tolocalhost:8080\\nor 127.0.0.1:8080. If you’re following along on Multipass, connect to your Multipass\\nVM’s 192 address on port8080. Run anip a | grep 192 command from within the\\nMultipass VM, or run amultipass ls from your local machine to find the address.\\nYou will see the following web page.',\n",
              " '4: The big picture 29\\nFigure 4.1\\nCongratulations. You’ve copied some application code from a remote Git repo, built it\\ninto a Docker image, and run it as a container. We call thiscontainerizing an app.\\nClean up\\nRun the following commands to terminate the container and delete the image.\\n$ docker rm web1 -f\\nweb1\\n$ docker rmi test:latest\\nUntagged: test:latest\\nDeleted: sha256:0435f27...cac8e2b\\nChapter Summary\\nIn the Ops section of the chapter, you downloaded a Docker image, launched a container\\nfrom it, logged into the container, executed a command inside of it, and then stopped\\nand deleted the container.\\nIn the Dev section, you containerized a simple application by pulling source code from\\nGitHub and building it into an image using instructions in a Dockerfile. You then ran\\nthe app as a container.\\nThe things you’ve learned in this chapter will help you in the upcoming chapters.',\n",
              " 'Part 2: The technical stuff',\n",
              " '5: The Docker Engine\\nIn this chapter, we’ll look under the hood of the Docker Engine.\\nThis chapter has a strongoperations focus, and you can use Docker without knowing\\neverything you’re about to learn. However, to truly master something, you need to\\nunderstand what’s going on under the hood. So, if you want tomaster Docker, you\\nshould read this chapter.\\nI’ve divided the chapter into the following sections:\\n• Docker Engine – The TLDR\\n• The Docker Engine\\n• The influence of the Open Container Initiative (OCI)\\n• runc\\n• containerd\\n• Starting a new container (example)\\n• What’s the shim all about\\n• How it’s implemented on Linux\\nLet’s learn about the Docker Engine.\\nDocker Engine – The TLDR\\nDocker Engineis jargon for the server-side components of Docker that run and manage\\ncontainers. If you’ve ever worked with VMware, the Docker Engine is similar to ESXi.\\nThe Docker Engine is modular and built from many small specialized components\\npulled from projects such as the OCI, the CNCF, and the Moby project.\\nIn many ways, the Docker Engine is like a car engine:\\n• A car engine is made from many specialized parts that work together to make a car\\ndrive — intake manifolds, throttle bodies, cylinders, pistons, spark plugs, exhaust\\nmanifolds, and more.\\n• The Docker Engine is made from many specialized tools that work together to\\ncreate and run containers — the API, image builder, high-level runtime, low-level\\nruntime, shims, etc.',\n",
              " '5: The Docker Engine 32\\nFigure 5.1 shows the components of the Docker Engine that create and run containers.\\nOther components exist, but this simplified diagram focuses on the components that\\nstart and run containers.\\nFigure 5.1\\nThroughout the book, we’ll refer torunc and containerd with lowercase “r” and “c”, which\\nis how they’re both written in the official project documentation. This means sentences\\nstarting with eitherrunc or containerd will not begin with a capital letter.\\nThe Docker Engine\\nWhen Docker was first released, the Docker Engine had two major components:\\n• The Docker daemon (sometimes referred to as just “the daemon”)\\n• LXC\\nThe daemon was a monolithic binary containing all the code for the API, image builders,\\ncontainer execution, volumes, networking, and more.\\nLXC did the hard work of interfacing with the Linux kernel and constructing the\\nrequired namespaces and cgroups to build and start containers.\\nReplacing LXC\\nRelying on LXC posed several problems for the Docker project.\\nFirst, LXC is Linux-specific, and Docker had aspirations of being multi-platform.\\nSecond, Docker was evolving fast, and there was no way of ensuring LXC evolved in the\\nways Docker needed.',\n",
              " '5: The Docker Engine 33\\nTo improve the experience and help the project evolve more quickly, Docker replaced\\nLXC with its own tool,libcontainer. The goal of libcontainer was to be a platform-\\nagnostic tool that gave Docker access to the fundamental container building blocks in\\nthe host kernel.\\nLibcontainer replaced LXC in Docker a very long time ago.\\nBreaking up the monolithic Docker daemon\\nAs previously mentioned, the Docker Engine was originally a monolith with almost all\\nfunctionality coded into thedaemon. However, as time passed, this became more and\\nmore problematic for the following reasons:\\n1. It got slower\\n2. It wasn’t what the ecosystem wanted\\n3. It’s hard to innovate on monolithic software\\nThe project recognized these challenges and began a long-running program to break\\napart and refactor the Engine so that every feature became its own small specialized tool.\\nPlatform builders could then re-use these tools to build other platforms.\\nThis work of breaking apart the Docker daemon is an ongoing process, and all of the\\ncode for building images and executing containers has been removed and refactored\\ninto small, specialized tools. Notable examples include removing the high-level and\\nlow-level runtime functionality and re-implementing them in separate tools called\\ncontainerd and runc, both of which are used by many different projects including\\nDocker, Kubernetes, Firecracker, and Fargate. More recently (starting with Docker\\nDesktop 4.27.0), Docker has removed image management from the daemon and now\\nuses containerd’s image management capabilities.\\nFigure 5.2 shows another view of the main components of the Docker Engine that are\\nused to run containers and lists the primary responsibilities of each component.',\n",
              " '5: The Docker Engine 34\\nFigure 5.2 - Engine components and responsibilities\\nOther engine components exist.\\nThe influence of the Open Container Initiative (OCI)\\nAround the same time that Docker, Inc. was refactoring the Engine, theOCI8 was in the\\nprocess of defining two container-related standards:\\n1. Image Specification(image-spec)9\\n2. Runtime Specification(runtime-spec)10\\nBoth specifications were released as version 1.0 in July 2017 and are still vital today.\\nThey’ve even added a third specification called the Distribution Specification (distribu-\\ntion-spec) governing how images are distributed via registries.\\nAt the time of writing, the runtime-spec is at version 1.1.0, the image-spec is at version\\n1.0.2, and the distribution-spec is at 1.0.1. However, plans are underway for version 1.1\\nof both the image-spec and distribution-spec. We mention this to highlight the slow-\\nand-steady nature of these low-level specifications that are heavily relied upon by so\\nmany other projects — stability is the name of the game for low-level OCI specs.\\n8https://www.opencontainers.org/\\n9https://github.com/opencontainers/image-spec\\n10https://github.com/opencontainers/runtime-spec',\n",
              " '5: The Docker Engine 35\\nDocker, Inc. was a founding member of the OCI and was heavily involved in defining\\nthe specifications. It continues to be involved by contributing code and helping guide\\nthe specifications.\\nAll versions of Docker since 2016 have implemented the OCI specifications. For\\nexample, Docker usesrunc, the reference implementation of the OCI runtime-spec, to\\ncreate OCI-compliant containers (runtime-spec). It also uses BuildKit to build OCI-\\ncompliant images (image-spec), and Docker Hub is an OCI-compliant registry (registry-\\nspec).\\nrunc\\nAs previously mentioned,runc11 (pronounced “run see” and always written with a\\nlowercase “r”) is the reference implementation of the OCI runtime-spec. Docker, Inc.\\nwas heavily involved in defining the spec and contributed the initial code for runc.\\nrunc is a lightweight CLI wrapper for libcontainer that you can download and use\\nto manage OCI-compliant containers. However, it’s a very low-level tool and lacks\\nalmost all of the features and add-ons you get with the Docker Engine. Fortunately, as\\npreviously shown in Figure 5.2, Docker uses runc as its low-level runtime. This means\\nyou get OCI-compliant containersand the feature-rich Docker user experience.\\nOn the jargon front, we sometimes say that runc operates at theOCI layer, and we often\\nrefer to it as alow-level runtime.\\nDocker and Kubernetes both use runc as their default low-level runtime, and both pair it\\nwith the containerd high-level runtime:\\n• containerd operates as the high-level runtimemanaging lifecycle events\\n• runc operates as the low-level runtimeexecuting lifecycle events by interfacing\\nwith the kernel to do the work of actually building containers and deleting them\\nYou can see the latest releases here:\\n• https://github.com/opencontainers/runc/releases\\ncontainerd\\ncontainerd (pronounced “container dee” and always written with a lowercase “c”) is\\nanother tool that Docker created while stripping functionality out of the daemon.\\n11https://github.com/opencontainers/runc',\n",
              " '5: The Docker Engine 36\\nWe refer to containerd as ahigh-level runtimeas itmanages lifecycle events such as\\nstarting, stopping, and deleting containers. However, it needs a low-level runtime to\\nperform the actual work. Most of the time, containerd is paired with runc as its low-\\nlevel runtime. However, as you saw in Figure 5.3, it usesshims that make it possible to\\nreplace runc with other low-level runtimes. We’ll go into more detail in the WebAssem-\\nbly chapter when you’ll see how to use Docker to run WebAssembly apps.\\nThe original plan was for containerd to be a small specialized tool for managing\\ncontainer lifecycle events. However, it has since grown to include the ability to manage\\nimages, networks, and volumes.\\nOne reason for adding more functionality is for projects such as Kubernetes that want\\ncontainerd to be able to push and pull images. Fortunately, this extra functionality is\\nmodular, meaning projects like Kubernetes can include containerd but only take the\\npieces they need.\\ncontainerd was originally developed by Docker, Inc. and donated to the Cloud Native\\nComputing Foundation (CNCF). At the time of writing, containerd is a graduated\\nCNCF project, meaning it’s stable and production-ready. You can see the latest releases\\nhere:\\n• https://github.com/containerd/containerd/releases\\nStarting a new container (example)\\nNow that you have a view of the big picture, let’s see how to use Docker to create a new\\ncontainer.\\nThe most common way of starting containers is using the Docker CLI. Feel free to run\\nthe following command to start a new container calledctr1 based on thenginx image.\\n$ docker run -d --name ctr1 nginx\\nRun adocker ps command to see if the container is running.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\n9cfb0c9aacb2 nginx \"/docker-entrypoint.…\" 9 seconds ago Up 9 seconds 80/tcp ctr1\\nWhen you run commands like this, the Docker client converts them into API requests\\nand sends them to the API exposed by the daemon.\\nThe daemon can expose the API on a local socket or over the network. On Linux, the\\nlocal socket is/var/run/docker.sock and on Windows it’s\\\\pipe\\\\docker_engine.',\n",
              " '5: The Docker Engine 37\\nThe daemon receives the request, interprets it as a request to create a new container,\\nand passes it to containerd. Remember that the daemon no longer contains any code\\nto create containers.\\nThe daemon communicates with containerd via a CRUD-style API overgRPC12.\\nDespite its name, even containerd cannot create containers. It converts the required\\nDocker image into anOCI bundleand tellsrunc to use this to create a new container.\\nrunc interfaces with the OS kernel to pull together all the constructs necessary to create\\na container (namespaces, cgroups, etc.). The container is started as a child process of\\nrunc, and as soon as the container starts, runc exits.\\nFigure 5.3 summarizes the process.\\nFigure 5.3\\nDecoupling the container creation and management from the Docker daemon and\\nimplementing it in containerd and runc makes it possible to stop, restart, and even\\nupdate the daemon without impacting running containers. We sometimes call this\\ndaemonless containers.\\n12https://grpc.io/',\n",
              " '5: The Docker Engine 38\\nIf you started the NGINX container earlier, you should delete it using the following\\ncommand.\\n$ docker rm ctr1 -f\\nWhat’s the shim all about?\\nSome of the diagrams in the chapter have shown ashim component.\\nShims are a popular software engineering pattern, and the Docker Engine uses them in\\nbetween containerd and the OCI layer, bringing the following benefits:\\n• Daemonless containers\\n• Improves efficiency\\n• Makes the OCI layer pluggable\\nWe’ve already mentioned daemonless containers.\\nOn the efficiency front, containerd forks a shim and a runc process for every new\\ncontainer. However, each runc process exits as soon as the container starts running,\\nleaving the shim process as the container’s parent process. The shim is lightweight\\nand sits between containerd and the container. It reports on the container’s status and\\nperforms low-level tasks such as keeping the container’s STDIN and STDOUT streams\\nopen.\\nShims also make it possible to replace runc with other low-level runtimes.\\nHow it’s implemented on Linux\\nOn a Linux system, Docker implements the components we’ve discussed as the follow-\\ning separate binaries:\\n• /usr/bin/dockerd (the Docker daemon)\\n• /usr/bin/containerd\\n• /usr/bin/containerd-shim-runc-v2\\n• /usr/bin/runc\\nYou can see all of these on a Linux-based Docker host by running aps command. Some\\nof the processes will only be present when the system has running containers, and you\\ncan’t see them if you’re using Docker Desktop on a Mac because the Docker Engine is\\nrunning inside a VM.',\n",
              " '5: The Docker Engine 39\\nDo we still need the daemon\\nAt the time of writing, Docker has stripped most of the functionality out of the daemon,\\nleaving the daemon to focus on serving the API.\\nChapter summary\\nThe Docker Engine is a platform that makes it easy to build, ship, and run containers. It\\nimplements the OCI standards and is a modular app comprising many small, specialized\\ncomponents.\\nThe Docker daemonimplements the Docker API, but most other functionality has been\\nstripped out and implemented as standalone composable tools such as containerd and\\nrunc.\\ncontainerd performs image management tasks and oversees container lifecycle manage-\\nment, such as starting, stopping, and deleting containers. Docker, Inc. originally wrote it\\nand then contributed to the CNCF. It’s classed as a high-level runtime and used by many\\nother projects, including Kubernetes, Firecracker, and Fargate.\\ncontainerd relies on a low-level runtime calledrunc to interface with the host kernel and\\nbuild containers. runc is the reference implementation of the OCI runtime-spec and\\nexpects to start containers from OCI-compliant bundles. containerd talks to runc and\\nensures Docker images are presented to runc as OCI-compliant bundles.\\nrunc is based on code from libcontainer, you can run it as a standalone CLI tool to\\ncreate containers, and it’s used almost everywhere that containerd is used.\\nShims make it possible to use containerd with other low-level runtimes.',\n",
              " '6: Working with Images\\nIn this chapter, we’ll dive deep into Docker images. You’ll learn what images are, how to\\nwork with them, and how they work under the hood. You’ll learn how to build your own\\nin Chapter 8: Containerizing an application.\\nI’ve arranged the chapter as follows:\\n• Docker images – The TLDR\\n• Intro to images\\n• Pulling images\\n• Image registries\\n• Image naming and tagging\\n• Images and layers\\n• Pulling images by digest\\n• Multi-architecture images\\n• Vulnerability scanning with Docker Scout\\n• Deleting images\\nDocker images – The TLDR\\nBefore getting started, all of the following terms mean the same thing, and we’ll use\\nthem interchangeably:Image, Docker image, container image, andOCI image.\\nAn image is a read-only package containing everything you need to run an application.\\nThis means they include application code, dependencies, a minimal set of OS constructs,\\nand metadata. You can start multiple containers from a single image.\\nIf you’re familiar with VMware, images are a bit like VM templates — a VM template is\\nlike a stopped VM, whereas an image is like a stopped container. If you’re a developer,\\nimages are similar toclasses — you can create one or more objects from a class, whereas\\nyou can create one or more containers from an image.\\nThe easiest way to get an image is topull one from aregistry. Docker Hub13 is the most\\ncommon registry, andpulling an image downloads it to your local machine where\\n13https://hub.docker.com',\n",
              " '6: Working with Images 41\\nDocker can use it to start one or more containers. Other registries exist, and Docker\\nworks with them all.\\nImages are made by stacking independentlayers and representing them as a single\\nunified object. One layer might have the OS components, another layer might have\\napplication dependencies, and another layer might have the application. Docker stacks\\nthese layers and makes them look like a unified system.\\nImages are usually small. For example, the official NGINX image is around 60MB, and\\nthe official Redis image is around 40MB. However, Windows images can be huge.\\nThat’s the elevator pitch. Let’s dig a little deeper.\\nIntro to images\\nWe’ve already said that images are like stopped containers. You can even stop a con-\\ntainer and create a new image from it. With this in mind, images arebuild-time con-\\nstructs, whereas containers arerun-time constructs. Figure 6.1 shows thebuild and run\\nnature of each and that you can start multiple containers from a single image.\\nFigure 6.1\\nThe docker run command is the most common way to start a container from an image.\\nOnce the container is running, the image and the container are bound, and you cannot\\ndelete the image until you stop and delete the container. If multiple containers use the\\nsame image, you can only delete the image after you’ve deleted all the containers using\\nit.\\nContainers are designed to run a single application or microservice. As such, they\\nshould only contain application code and dependencies. You should not include non-\\nessentials such as build tools or troubleshooting tools.',\n",
              " '6: Working with Images 42\\nFor example, the official Alpine Linux image is currently about 3MB. This is because it\\ndoesn’t ship with six different shells, three different package managers, and a bunch of\\ntools you “might” need once every ten years. In fact, it’s increasingly common for images\\nto ship without a shell or a package manager — if the application doesn’t need it at run-\\ntime, the image doesn’t include it. We call theseslim images.\\nAnother thing that keeps images small is the lack of an OS kernel. This is because con-\\ntainers use the kernel of the host they’re running on. The only OS-related components\\nin most images are filesystem objects, and you’ll sometimes hear people say images\\ncontain just enough OS.\\nUnfortunately, Windows images can be huge. For example, some Windows-based\\nimages can be gigabytes in size and take a long time to push and pull.\\nPulling images\\nA clean Docker installation has an emptylocal repository.\\nLocal repositoryis jargon for an area on your local machine where Docker stores images\\nfor more convenient access. We sometimes call it theimage cache, and on Linux it’s\\nusually located in/var/lib/docker/<storage-driver>. However, it will be inside the\\nDocker VM if you’re using Docker Desktop.\\nRun the following command to inspect the contents of your local repository. This\\nexample has three images relating to three Docker Desktop extensions I’m running.\\nYours will be different and may be empty.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\ndocker/disk-usage-extension 0.2.9 f4c95478a537 26 hours ago 3.64MB\\ndocker/logs-explorer-extension 0.2.6 417dd9a8f96d 26 hours ago 17.9MB\\nportainer/portainer-docker-extension 2.19.4 908d04d20e86 2 months ago 364MB\\nThe process of getting images is calledpulling.\\nRun the following commands to pull theredis image and verify it exists in your local\\nrepository.\\nNote: If you are following along on Linux and haven’t added your user\\naccount to the localdocker Unix group, you may need to addsudo to the\\nbeginning of all the following commands.',\n",
              " \"6: Working with Images 43\\n$ docker pull redis\\nUsing default tag: latest <<---- Assume the 'latest' tag\\nlatest: Pulling from library/redis <<---- Assume you want to pull from Docker Hub\\n08df40659127: Download complete <<---- Pulling layer\\n4f4fb700ef54: Already exists <<---- Pulling layer (local copy must exist)\\n4fe7fa4aab04: Download complete <<---- Pulling layer\\n57dea0f129a5: Download complete <<---- Pulling layer\\nf546e941f15b: Download complete <<---- Pulling layer\\nf7f7da262cdb: Download complete <<---- Pulling layer\\nf45ab649e450: Download complete <<---- Pulling layer\\n983f900bbc88: Download complete <<---- Pulling layer\\nDigest: sha256:76d5908f5e19fcdd73daf956a38826f790336ee4707d9028f32b24ad9ac72c08\\nStatus: Downloaded newer image for redis:latest\\ndocker.io/library/redis:latest <<---- docker.io = Docker Hub\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nredis latest 11c3e418c296 2 weeks ago 223MB\\n<Snip>\\nThe image now exists in your local repository. However, I’ve annotated a few interesting\\nlines from thedocker pull output. We’ll cover them in more detail later in the chapter\\nbut they’re worth a quick mention now.\\nDocker is opinionated and made two assumptions when pulling the image:\\n1. It assumed you wanted to pull the image tagged aslatest\\n2. It assumed you wanted to pull the image from Docker Hub\\nYou can override both, but Docker will use these as defaults if you don’t override them.\\nThe Redis image has eight layers. However, Docker only pulled seven layers because it\\nalready had a local copy of one of them. This is because my system runs thePortainer\\nDocker Desktop extension, which is based on an image that shares a common layer\\nwith the Redis image. You’ll learn about this very soon, but images can share layers, and\\nDocker is clever enough only to pull the layers it doesn’t already have.\\nWe’ll talk more about these points later in the chapter.\\nImage registries\\nWe store images in centralized places calledregistries. The job of a registry is to securely\\nstore images and make them easy to access from different environments.\\nFigure 6.2 shows the central nature of registries in the build > ship > run pipeline.\",\n",
              " '6: Working with Images 44\\nFigure 6.2\\nMost modern registries implement the OCI distribution-spec, and we sometimes call\\nthem OCI registries. Most registries also implement the Docker Registry v2 API, meaning\\nyou can use the Docker CLI and other API tools to query them and work with them in\\nstandard ways. Some offer advanced features such as image scanning and integration\\nwith build pipelines.\\nThe most common registry is Docker Hub, but others exist, including 3rd-party\\ninternet-based registries and secure on-premises registries. However, as previously\\nmentioned, Docker is opinionated and will default to Docker Hub unless you tell it\\nthe name of a different registry. We’ll use Docker Hub for the rest of the book, but the\\nprinciples apply to other registries.\\nImage registries contain one or moreimage repositories, and image repositories contain\\none or more images. Figure 6.3 shows an image registry with three repositories, each\\nwith one or more images.',\n",
              " '6: Working with Images 45\\nFigure 6.3 - Registry architecture\\nOfficial repositories\\nDocker Hub has the concept ofofficial repositoriesthat are home to images vetted and\\ncurated by Docker and the application vendor. This means theyshould contain up-to-\\ndate high-quality code that is secure, well-documented, and follows good practices.\\nMost of the popular applications and operating systems haveofficial repositorieson\\nDocker Hub, and they’re easy to identify because they live at the top level of the Docker\\nHub namespace and have a greenDocker Official Imagebadge. The following list shows\\na few official repositories and their URLs that exist at the top level of the Docker Hub\\nnamespace:\\n• nginx: https://hub.docker.com/_/nginx/\\n• busybox: https://hub.docker.com/_/busybox/\\n• redis: https://hub.docker.com/_/redis/\\n• mongo: https://hub.docker.com/_/mongo/\\nFigure 6.4 shows the official Alpine and NGINX repositories on Docker Hub. Both have\\nthe greenDocker Official Imagebadge and have over a billion pulls each. Also, notice how\\nboth are available for a wide range of CPU architectures.',\n",
              " '6: Working with Images 46\\nFigure 6.4 - Official repos on Docker Hub\\nUnofficial repositories\\nThe following list shows two of my personal repositories in the wild west ofunofficial\\nrepositories that you should not trust.\\n• nigelpoulton/gsd — https://hub.docker.com/r/nigelpoulton/gsd/\\n• nigelpoulton/k8sbook — https://hub.docker.com/r/nigelpoulton/k8sbook/\\nNotice how they exist below thenigelpoulton second-level namespace. This is one of\\nseveral indications they are not official repositories.\\nWhile there are lots of great images in unofficial repositories, you should always start\\nwith the assumption that anything from an unofficial repository isunsafe. This is based\\non the good practice of never trusting software from the internet. In fact, you should\\nalso exercise caution when downloading and using Docker Official Images.\\nImage naming and tagging\\nMost of the time, you’ll work with images based on their names, and you can learn\\na lot about an image from its name. Figure 6.5 shows a fully qualified image name,\\nincluding the registry name, user/organization name, repository name, and tag. Docker\\nautomatically populates the registry and tag values if you don’t specify them.',\n",
              " \"6: Working with Images 47\\nFigure 6.5 - Fully qualified image name\\nAddressing images from official repositories is easy. All you need to supply is the\\nrepository name and image name separated by a colon. Sometimes we call the image\\nname thetag. The format for adocker pull command pulling an image from an official\\nrepository is:\\n$ docker pull <repository>:<tag>\\nThe example from earlier pulled the Redis image with the following command. It pulled\\nthe image tagged aslatest from the top-levelredis repository.\\n$ docker pull redis:latest\\nThe following examples show how to pull a few different official images.\\n$ docker pull mongo:7.0.5\\n//Pulls the image tagged as '7.0.5' from the official 'mongo' repository.\\n$ docker pull busybox:glibc\\n//Pulls the image tagged as 'glibc' from the official 'busybox' repository.\\n$ docker pull alpine\\n//Pulls the image tagged as 'latest' from the official 'alpine' repository.\\nA couple of things are worth noting.\\n• As previously mentioned, if youdon’t specify an image tag after the repository\\nname, Docker assumes you want the image tagged aslatest. The command will\\nfail if the repository has no image tagged aslatest.\\n• Images tagged aslatest are not guaranteed to be the most up-to-date in the\\nrepository.\\nPulling images fromunofficial repositoriesis almost the same as pulling from official\\nrepositories — you just need to add a Docker Hub username or organization name\\nbefore the repository name. The following example shows how to pull thev2 image\\nfrom thetu-demo repository owned by a not-to-be-trusted person whose Docker Hub\\nID isnigelpoulton.\",\n",
              " '6: Working with Images 48\\n$ docker pull nigelpoulton/tu-demo:v2\\nTo pull an image from a different registry, you just add the registry’s DNS name before\\nthe repository name. For example, the following command pulls thelatest image from\\nBrandon Mitchell’sregclient/regsync repo on GitHub Container Registry (ghcr.io).\\n$ docker pull ghcr.io/regclient/regsync:latest\\nlatest: Pulling from regclient/regsync\\n6f14f2b64ccf: Download complete\\n7746d6728537: Download complete\\n685af2c79c31: Download complete\\n4c377311167a: Download complete\\n662e9541e042: Download complete\\nDigest: sha256:149a95d47d6beed2a1404d7c3b00dddfa583a94836587ba8e3b4fe59853c1ece\\nStatus: Downloaded newer image for ghcr.io/regclient/regsync:latest\\nghcr.io/regclient/regsync:latest\\nNotice how the pull looks the same as it did with Docker Hub. This is because GHCR\\nsupports the OCI registry-spec and implements the Docker Registry v2 API.\\nImages with multiple tags\\nYou can give a single image as many tags as you want.\\nAt first glance, the following output might look like it’s listing three images. However,\\non closer inspection it’s just two — theb4210d0aa52f image is tagged aslatest and v1.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/tu-demo latest b4210d0aa52f 2 days ago 115MB\\nnigelpoulton/tu-demo v1 b4210d0aa52f 2 days ago 115MB\\nnigelpoulton/tu-demo v2 6ba12825d092 12 minutes ago 115MB\\nThis is a great example of thelatest tag not relating to the newest image in the repo.\\nIn this example, thelatest tag refers to the same image as thev1 tag, which is actually\\nolder than thev2 image.\\nImages and layers\\nAs already mentioned, images are a collection of loosely connected read-only layers\\nwhere each layer comprises one or more files.\\nFigure 6.6 shows an image with four layers. Docker takes care of stacking them and\\nrepresenting them as a single unified image.',\n",
              " '6: Working with Images 49\\nFigure 6.6 - Image and stacked layers\\nYou’re about to look at all of the following ways to inspect layer information:\\n• Pull operations\\n• The docker inspect command\\n• The docker history command\\nRun the following command to pull thenode:latest image and observe it pulling the\\nindividual layers. Some newer versions may have more or less layers, but the principle is\\nthe same.\\n$ docker pull node:latest\\nlatest: Pulling from library/ubuntu\\n952132ac251a: Pull complete\\n82659f8f1b76: Pull complete\\nc19118ca682d: Pull complete\\n8296858250fe: Pull complete\\n24e0251a0e2c: Pull complete\\nDigest: sha256:f4691c96e6bbaa99d...28ae95a60369c506dd6e6f6ab\\nStatus: Downloaded newer image for node:latest\\ndocker.io/node:latest\\nEach line ending withPull completerepresents a layer that Docker pulled. This image has\\nfive layers and is shown in Figure 6.7 with layer IDs.',\n",
              " '6: Working with Images 50\\nFigure 6.7 - Image layers and IDs\\nAnother way to see image layers is to inspect the image with thedocker inspect\\ncommand. The following example inspects the samenode:latest image pulled in the\\nprevious step.\\n$ docker inspect node:latest\\n[\\n{\\n\"Id\": \"sha256:bd3d4369ae.......fa2645f5699037d7d8c6b415a10\",\\n\"RepoTags\": [\\n\"node:latest\"\\n<Snip>\\n\"RootFS\": {\\n\"Type\": \"layers\",\\n\"Layers\": [\\n\"sha256:c8a75145fc...894129005e461a43875a094b93412\",\\n\"sha256:c6f2b330b6...7214ed6aac305dd03f70b95cdc610\",\\n\"sha256:055757a193...3a9565d78962c7f368d5ac5984998\",\\n\"sha256:4837348061...12695f548406ea77feb5074e195e3\",\\n\"sha256:0cad5e07ba...4bae4cfc66b376265e16c32a0aae9\"\\n]\\n}\\n}\\n]\\nThe trimmed output shows the five layers. However, it shows their SHA256 hashes,\\nwhich are different from the short IDs shown in thedocker pull output.\\nThe docker inspect command is great for getting detailed image information.\\nYou can also use thedocker history command to inspect an image and see its layer\\ndata. However, this command shows the build history of an image and isnot a strict list',\n",
              " '6: Working with Images 51\\nof layers in the final image. For example, some Dockerfile instructions (ENV, EXPOSE, CMD,\\nand ENTRYPOINT) only add metadata and don’t create layers.\\nBase layers\\nAll Docker images start with abase layer, and every time you add new content, Docker\\nadds a new layer.\\nConsider the following oversimplified example of building a simple Python application.\\nYour corporate policy mandates all applications be built on top of the official Ubuntu\\n24:04 image. This means the official Ubuntu 24:04 image will be the base layer for this\\napp. Installing Python will add a second layer, and your application source code will add\\na third. The final image will have three layers, as shown in Figure 6.8. Remember, this is\\nan oversimplified example for demonstration purposes.\\nFigure 6.8\\nIt’s important to understand that animage is the combination of all layers stacked in the\\norder they were built. Figure 6.9 shows an image with two layers. Each layer has three\\nfiles, meaning the image has six files.\\nIt also shows that thelayers are stored as independent objects, and theimage is just\\nmetadata identifying the required layers and explaining how to stack them.',\n",
              " '6: Working with Images 52\\nFigure 6.9\\nIn the slightly more complex example of the three-layer image in Figure 6.10, the overall\\nimage only presents six files in the unified view. This is becauseFile 7in the top layer\\nis an updated version ofFile 5directly below (inline). In this situation, the file in the\\nhigher layer obscures the file directly below it. This means you can update the file in an\\nimage by adding new layers.\\nFigure 6.10 - Stacking layers\\nUnder the hood, Docker usesstorage driversto stack layers and present them as a\\nunified filesystem and image. Almost all Docker setups use theoverlay2 driver, butzfs,\\nbtrfs, andvfs are alternative options. However, whichever storage driver you use, the\\ndeveloper and user experience are always the same.\\nFigure 6.11 shows how the three-layer image from Figure 6.10 will appear on the system\\n— all three layers stacked and merged into a single unified view.',\n",
              " '6: Working with Images 53\\nFigure 6.11 - Unified view of multi-layer image\\nSharing image layers\\nAs previously mentioned, images can share layers, leading to efficiencies in space and\\nperformance.\\nOne of the earlierdocker pull commands generated anAlready existsmessage for one\\nof the layers it pulled. This occurred because one of my Docker Desktop extensions had\\nalready pulled an image that used the exact same layer. As a result, Docker skipped that\\nlayer as it already had a local copy.\\nHere’s the code from earlier, and Figure 6.12 shows two images sharing the same layer.\\n$ docker pull redis:latest\\nlatest: Pulling from library/redis\\n25d3892798f8: Download complete\\ne5d458cf0bea: Download complete\\n4f4fb700ef54: Already exists <<---- This line\\n<Snip>',\n",
              " '6: Working with Images 54\\nFigure 6.12 - Two images sharing a layer\\nLayers are also shared on the registry side. This means you can store lots of similar\\nimages in a registry, and the registry will save space by never storing more than a single\\ncopy of any layer.\\nPulling images by digest\\nSo far, you’ve seen how to pull and work with images using names (tags). While this\\nis the most common method, it has a problem — tags are mutable. This means it’s\\npossible to tag an image incorrectly or give a new image the same tag as an older one. An\\nextremely common example is thelatest tag. For example, pulling thealpine:latest\\ntag a year ago will not pull the same image as pulling the same tag today.\\nConsider a quick example outlining one potential implication of trusting mutable tags.\\nImagine you have an image calledgolftrack:1.5 and you get a warning that it has a\\ncritical vulnerability. You build a new image containing the fix and push the new image\\nto the same repository with thesame tag.\\nTake a moment to consider what just happened and the implications.\\nYou have an image calledgolftrack:1.5 that’s being used by lots of containers in your\\nproduction environment, and it has a critical bug. You create a new version containing\\nthe fix. So far, so good, but then you make the mistake. You push the new image to\\nthe same repository with thesame tag as the vulnerable image. This overwrites the\\noriginal image and leaves you without a great way of knowing which of your production',\n",
              " '6: Working with Images 55\\ncontainers are using the vulnerable image and which are using the fixed image — both\\nimages have the same tag!\\nThis is whereimage digestscome to the rescue.\\nDocker uses acontent addressable storagemodel where every image gets a cryptographic\\ncontent hashthat we usually call thedigest. As these are hashes of an image’s contents,\\nit’s impossible for two different images to have the same digest. It’s also impossible to\\nchange an image without creating a new unique digest. Fortunately, Docker lets you\\nwork with image digests instead of just names.\\nIf you’ve already pulled an image by name, you can see its digest by running adocker\\nimages command with the--digests flag as shown.\\n$ docker images --digests alpine\\nREPOSITORY TAG DIGEST IMAGE ID CREATED SIZE\\nalpine latest sha256:c5b1261d...8e1ad6b c5b1261d6d3e 2 weeks ago 11.8MB\\nIf you want to find an image’s digestbefore pullingit, you can use thedocker buildx\\nimagetools command. The following example retrieves the image digest for the\\nnigelpoulton/k8sbook/latest image on Docker Hub.\\n$ docker buildx imagetools inspect nigelpoulton/k8sbook:latest\\nName: docker.io/nigelpoulton/k8sbook:latest\\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json\\nDigest: sha256:13dd59a0c74e9a147800039b1ff4d61201375c008b96a29c5bd17244bce2e14b\\n<Snip>\\nYou can now use the digest to pull the image. I’ve trimmed the command and the output\\nfor readability.\\n$ docker pull nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b\\ndocker.io/nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b: Pulling from nigelpoulton/k8sbook\\n59f1664fb787: Download complete\\na052f1888b3e: Download complete\\n94a9f4dfa0e5: Download complete\\nbb7e600677fa: Download complete\\nedfb0c26f1fb: Download complete\\n5b1423465504: Download complete\\n2f232a362cd9: Download complete\\nDigest: sha256:13dd59a0...bce2e14b\\nStatus: Downloaded newer image for nigelpoulton/k8sbook@sha256:13dd59a0...bce2e14b\\ndocker.io/nigelpoulton/k8sbook:latest@sha256:13dd59a0...bce2e14b\\nIt’s also possible to directly query the registry API for image data, including digest. The\\nfollowing curl command queries Docker Hub for the digest of the same image.',\n",
              " '6: Working with Images 56\\n$ curl \"https://hub.docker.com/v2/repositories/nigelpoulton/k8sbook/tags/?name=latest\" \\\\\\n|jq \\'.results[].digest\\'\\n\"sha256:13dd59a0c74e9a147800039b1ff4d61201375c008b96a29c5bd17244bce2e14b\"\\nImage hashes and layer hashes\\nYou already know that images are just a loose collection of independent layers. This\\nmeans animage is just a manifest file with some metadata and a list of layers. The\\nactual application and all its dependencies live in thelayers. However, layers are fully\\nindependent and have no concept of being part of an image.\\nWith this in mind, images and layers have their own digests as follows:\\n• Images digests are a crypto hash of the image manifest file\\n• Layer digests are a crypto hash of the layer’s contents\\nThis means all changes to layers or image manifests result in new hashes, giving us an\\neasy and reliable way to know if changes have been made.\\nContent hashes vs distribution hashes\\nDocker compares hashes before and after every push and pull to ensure no tampering\\nhas occurred. However, it also compresses images during push and pull operations to\\nsave network bandwidth and storage space on the registry. As a result of this compres-\\nsion, the before and after hashes won’t match.\\nTo get around this, each layer gets two hashes:\\n• Content hash (uncompressed)\\n• Distribution hash (compressed)\\nEvery time Docker pushes or pulls a layer from a registry, it includes the layer’sdistribu-\\ntion hashand uses this to verify no tapering occurred. This is one reason why the hashes\\nin different CLI and registry outputs don’t always match — sometimes you’re looking at\\nthe content hash, and other times you’re looking at the distribution hash.',\n",
              " '6: Working with Images 57\\nMulti-architecture images\\nOne of the best things about Docker is its simplicity. However, as technologies grow,\\nthey inevitably get more complex. This happened for Docker when it started supporting\\ndifferent platforms and architectures, such as Windows and Linux on variations of\\nARM, x64, PowerPC, s390x and more. Suddenly, there were multiple versions of\\nthe same image for all the different architectures, and developers and users had to\\nput in significant extra work to get the right version. This broke the smooth Docker\\nexperience.\\nMulti-architecture images to the rescue!\\nFortunately, Docker and the registry API adapted and became clever enough to hide\\nimages for multiple architectures behind a single tag. This means you can do adocker\\npull alpine on any architecture and get the correct version of the image. For example,\\nif you’re on an AMD64 machine, you’ll get the AMD64 image.\\nTo make this happen, the Registry API supports two important constructs:\\n• Manifest lists\\n• Manifests\\nThe manifest listis exactly what it sounds like — a list of architectures supported by an\\nimage tag. Each supported architecture then has its ownmanifest that lists the layers\\nused to build it.\\nRun the following command to see the different architectures supported behind the\\nalpine:latest tag.\\n$ docker buildx imagetools inspect alpine\\nName: docker.io/library/alpine:latest\\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json\\nDigest: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\\nManifests:\\nName: docker.io/library/alpine:latest@sha256:6457d53f...628977d0\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/amd64\\nName: docker.io/library/alpine:latest@sha256:b229a851...d144c1d8\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/arm/v6\\nName: docker.io/library/alpine:latest@sha256:ec299a7b...33b4c6fe\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/arm/v7',\n",
              " '6: Working with Images 58\\nName: docker.io/library/alpine:latest@sha256:a0264d60...93467a46\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/arm64/v8\\nName: docker.io/library/alpine:latest@sha256:15c46ced...ab073171\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/386\\nName: docker.io/library/alpine:latest@sha256:b12b826d...ba52a3a2\\nMediaType: application/vnd.docker.distribution.manifest.v2+json\\nPlatform: linux/ppc64le\\nIf you look closely, you’ll see a singlemanifest listpointing to sixmanifests.\\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json is the\\nmanifest list.\\nEach MediaType: application/vnd.docker.distribution.manifest.v2+json line\\nrefers to a manifest for each specific architecture.\\nFigure 6.13 shows how manifest lists and manifests are related. On the left, you can see\\na manifest list with entries for the different architectures supported by the image. The\\narrows show that each entry in the manifest list points to a manifest defining the image\\nconfig and the list of layers making up the image for that architecture.\\nFigure 6.13 - Manifest lists and manifests',\n",
              " '6: Working with Images 59\\nLet’s step through a quick example.\\nAssume you’re using Docker Desktop on an M3 Mac where Docker runs inside a\\nlinux/arm VM. You ask Docker to pull an image, and Docker makes the relevant calls to\\nthe Registry API to request the appropriate manifest list. Assuming it exists, Docker then\\nparses it alinux/arm entry. Iflinux/arm entry exists, Docker retrieves its manifest,\\nparses it for the crypto IDs of its layers, pulls each individual layer, and assembles them\\ninto the image.\\nLet’s see it in action.\\nThe following examples are from Docker Desktop on an ARM-based Mac and Docker\\nDesktop on an AMD-based Windows machine running inWindows containers mode.\\nBoth start a new container based the officialgolang image and execute thego version\\ncommand. The outputs show the version of Go and the host’s platform and CPU\\narchitecture. Notice how both commands are exactly the same, and Docker takes care\\nof pulling the correct image.\\nBoth images are large and may take a while to download. You do not need to complete\\nthese commands yourself.\\nLinux on arm64 example:\\n$ docker run --rm golang go version\\n<Snip>\\ngo version go1.22.0 linux/arm64\\nWindows on x64 example:\\n> docker run --rm golang go version\\n<Snip>\\ngo version go1.20.4 windows/amd64\\nYou’ve already seen how to use thedocker buildx imagetools command to see the\\nmanifest list and manifests for an image. You can get similar information from the\\ndocker manifest command. The following example inspects the manifest list for\\nthe officialgolang image on Docker Hub. You can see it has images for Linux and\\nWindows on a variety of CPU architectures. You can run the same command without\\nthe grep filter to see the full JSON manifest list. Windows users should replace thegrep\\ncommand withSelect-String architecture,os',\n",
              " '6: Working with Images 60\\n$ docker manifest inspect golang | grep \\'architecture\\\\|os\\'\\n\"architecture\": \"amd64\",\\n\"os\": \"linux\"\\n\"architecture\": \"arm\",\\n\"os\": \"linux\",\\n\"architecture\": \"arm64\",\\n\"os\": \"linux\",\\n\"architecture\": \"386\",\\n\"os\": \"linux\"\\n\"architecture\": \"mips64le\",\\n\"os\": \"linux\"\\n\"architecture\": \"ppc64le\",\\n\"os\": \"linux\"\\n\"architecture\": \"s390x\",\\n\"os\": \"linux\"\\n\"architecture\": \"amd64\",\\n\"os\": \"windows\",\\n\"os.version\": \"10.0.20348.2227\"\\n\"architecture\": \"amd64\",\\n\"os\": \"windows\",\\n\"os.version\": \"10.0.17763.5329\"\\nPulling the right image for your system is one thing, but what aboutbuilding images for\\nall these different architectures?\\nThe docker buildx command makes it easy to create multi-architecture images. For\\nexample, you can use Docker Desktop onlinux/arm to build images forlinux/amd\\nand possibly other architectures. We’ll perform builds like these in future chapters, but\\ndocker buildx offers two ways to create multi-architecture images:\\n• Emulation\\n• Build Cloud\\nEmulation modeperforms builds for different architectures on your local machine by\\nrunning the build inside a QEMU virtual machine emulating the target architecture. It\\nworks most of the time but is slow and doesn’t have a shared cache.\\nBuild Cloudis a new service from Docker, Inc. that performs builds in the cloud on\\nnative hardware without requiring emulation. It’s very fast, lets you share a common\\nbuild cache with teammates, and is seamlessly integrated into Docker Desktop and any\\nversion of the Docker Engine using a version of buildx supporting the cloud driver.\\nIt also integrates with GitHub actions and other CI solutions. At the time of writing,\\nDocker Build Cloud is a subscription service you have to pay for.\\nWe’ll use both in future chapters, but I ran the following command to build AMD and\\nARM versions of thenigelpoulton/tu-demo image using Docker Build Cloud.',\n",
              " '6: Working with Images 61\\n$ docker buildx build \\\\\\n--builder=cloud-nigelpoulton-ddd-cloud \\\\\\n--platform=linux/amd64,linux/arm64 \\\\\\n-t nigelpoulton/tu-demo:latest --push .\\nVulnerability scanning with Docker Scout\\nLots of tools and plugins exist that scan images for known vulnerabilities.\\nWe’ll look at Docker Scout, as it’s built into almost every level of Docker, including the\\nCLI, Docker Desktop, Docker Hub, and thescout.docker.com portal. It’s a very slick\\nservice, but it requires a paid subscription. Other similar products and services exist, but\\nmost require paid subscriptions.\\nRecent versions of Docker Desktop have the Scout CLI plugin pre-installed and ready\\nto go. If you’re running a different version of Docker, you may be able to install the CLI\\nplugin from theGitHub repo14.\\nYou can use thedocker scout quickview command to get a quick vulnerability\\noverview of an image. The following command analyses thenigelpoulton/tu-\\ndemo:latest image. If a local copy doesn’t exist, it pulls it from Docker Hub and\\nperforms the analysis locally.\\n$ docker scout quickview nigelpoulton/tu-demo:latest\\n✓ SBOM of image already cached, 66 packages indexed\\nTarget │ nigelpoulton/tu-demo:latest │ 0C 1H 1M 0L\\ndigest │ b4210d0aa52f │\\nBase image │ python:3-alpine │ 0C 1H 1M 0L\\nUpdated base image │ python:3.11-alpine │ 0C 1H 1M 0L\\n│ │\\nThe output shows zero critical vulnerabilities (0C), one high (1H), one medium (1M),\\nand zero low (0L).\\nYou can use thedocker scout cves command to get more detailed information,\\nincluding remediation advice.\\n14https://github.com/docker/scout-cli',\n",
              " '6: Working with Images 62\\n$ docker scout cves nigelpoulton/tu-demo:latest\\n✓ SBOM of image already cached, 66 packages indexed\\n\\uffff Detected 1 vulnerable package with 2 vulnerabilities\\n## Overview\\n│ Analyzed Image\\n────────────────────┼────────────────────────────────\\nTarget │ nigelpoulton/tu-demo:latest\\ndigest │ b4210d0aa52f\\nplatform │ linux/arm64/v8\\nvulnerabilities │ 0C 1H 1M 0L\\nsize │ 26 MB\\npackages │ 66\\n## Packages and Vulnerabilities\\n0C 1H 1M 0L expat 2.5.0-r2\\npkg:apk/alpine/expat@2.5.0-r2?os_name=alpine&os_version=3.19\\n\\uffff HIGH CVE-2023-52425\\nhttps://scout.docker.com/v/CVE-2023-52425\\nAffected range : <2.6.0-r0\\nFixed version : 2.6.0-r0\\n<Snip>\\nI’ve snipped the output so it only shows the critical and high vulnerabilities, but several\\nthings are clear:\\n• It has detected one vulnerable package containing two vulnerabilities\\n• The affected package is calledexpat and the vulnerable version we’re running is\\n2.5.0-r2\\n• It lists the vulnerability asCVE-2023-52425\\n• It includes a link to a Scout report containing more info\\n• It suggests we update to version2.6.0-r0 which contains the fix\\nFigure 6.14 shows how this looks in Docker Desktop, and you get similar integrations\\nand views in Docker Hub.',\n",
              " '6: Working with Images 63\\nFigure 6.14 - Docker Scout integration with Docker Desktop\\nThe scout.docker.com portal provides an overview dashboard, allows you to configure\\npolicies, and lets you set up integrations with Docker Hub and other registries to\\nremotely scan and monitor multiple repositories.\\nDeleting Images\\nYou can delete images using thedocker rmi command. rmi is short forremove image.\\nDeleting images removes them from your local repository and they’ll no longer show up\\nin yourdocker images commands. The operation also deletes all directories on your\\nlocal filesystem containing layer data. However, Docker won’t delete layers shared by\\nmultiple images until you delete all images that reference them.\\nYou can delete images by name, short ID, or SHA. You can also delete multiple images\\nwith the same command.\\nThe following command deletes three images — one by name, one by short ID, and one\\nby SHA. I’ve trimmed the output for easier reading.',\n",
              " '6: Working with Images 64\\n$ docker rmi redis:latest af111729d35a sha256:c5b1261d...f8e1ad6b\\nUntagged: redis:latest\\nDeleted: sha256:76d5908f5e19fcdd73daf956a38826f790336ee4707d9028f32b24ad9ac72c08\\nUntagged: nigelpoulton/tu-demo:v2\\nDeleted: sha256:af111729d35a09fd24c25607ec045184bb8d76e37714dfc2d9e55d13b3ebbc67\\nUntagged: alpine:latest\\nDeleted: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\\nDocker will prevent the delete operation if the image is being used by a container or\\nreferenced by more than one tag. However, you can force the operation with the-f\\nflag, but you should do so with caution, as forcing Docker to delete an image in use by\\na container will untag the image and leave it on the system as adangling image.\\nA handy way todelete all imagesis to pass a list of all local image IDs to thedocker rmi\\ncommand. You should use this command with caution, and if you’re following along on\\nWindows, it will only work in a PowerShell terminal.\\n$ docker rmi $(docker images -q) -f\\nTo understand how this works, download a couple of images and then rundocker\\nimages -q.\\n$ docker pull alpine\\n<Snip>\\n$ docker pull ubuntu\\n<Snip>\\n$ docker images -q\\n44dd6f223004\\n3f5ef9003cef\\nSee how thedocker images -q returns a list of local image IDs. Passing this list to\\ndocker rmi will delete all images on the system as shown next.\\n$ docker rmi $(docker images -q) -f\\nUntagged: alpine:latest\\nUntagged: alpine@sha256:02bb6f428431fb...a33cb1af4444c9b11\\nDeleted: sha256:44dd6f2230041...09399391535c0c0183b\\nDeleted: sha256:94dd7d531fa56...97252ba88da87169c3f\\nUntagged: ubuntu:latest\\nUntagged: ubuntu@sha256:dfd64a3b4296d8...9ee20739e8eb54fbf\\nDeleted: sha256:3f5ef9003cefb...79cb530c29298550b92\\nDeleted: sha256:b49483f6a0e69...f3075564c10349774c3\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nLet’s remind ourselves of some of the commands we’ve used.',\n",
              " '6: Working with Images 65\\nImages – The commands\\n• docker pull is the command to download images from remote registries. It\\ndefaults to Docker Hub but works with other registries. The following command\\nwill pull the image tagged aslatest from thealpine repository on Docker Hub:\\ndocker pull alpine:latest.\\n• docker images lists all of the images in your Docker host’s local repository (image\\ncache). You can add the--digests flag to see the SHA256 hashes.\\n• docker inspect gives you a wealth of image-related metadata in a nicely format-\\nted view.\\n• docker manifest inspect lets you inspect the manifest list of images stored\\nin registries. The following command will show the manifest list for theregctl\\nimage on GitHub Container Registry (GHCR):docker manifest inspect\\nghcr.io/regclient/regctl.\\n• docker buildx is a Docker CLI plugin that works with Docker’s latest build\\nengine features. You saw how to use theimagetools sub-command to query\\nmanifest-related data from images.\\n• docker scout is a Docker CLI plugin that integrates with the Docker Scout\\nbackend to perform image vulnerability scanning. It scans images, provides\\nreports on vulnerabilities, and even suggests remediation actions.\\n• docker rmi is the command to delete images. It deletes all layer data stored in the\\nlocal filesystem, and you cannot delete images associated with containers in the\\nrunning (Up) or stopped (Exited) states.\\nChapter summary\\nThis chapter taught you the important theory and fundamentals of images.\\nYou learned that images contain everything needed to run an application as a container.\\nThis includes just enough OS, source code, dependencies, and metadata.\\nYou can start one or more containers from a single image.\\nUnder the hood, Docker constructs images by stacking one or more read-only layers\\nand presenting them as a unified object. Every image has a manifest that lists the layers\\nthat make up the image and how to stack them.\\nYou learned that image names are also called tags, they’re mutable, and they don’t always\\npull the same image. For example, pulling thealpine:latest tag today will not pull\\nthe same image as it will a year from now. Fortunately, every image gets an immutable\\ndigest that you can use to guarantee you always pull the same image.',\n",
              " '6: Working with Images 66\\nDocker Hub has the notion of curatedofficial imagesthat should be safer to use than\\nunofficial images. However, you should always exercise caution when downloading\\nsoftware from the Internet, even official images from Docker Hub.\\nImages can share layers for efficiency, and Docker makes it easy to build and pull images\\nfor lots of different CPU architectures, such as ARM and AMD.\\nDocker Scout scans images for known vulnerabilities and provides remediation\\nrecommendations. It requires a Docker subscription and is integrated into thedocker\\nCLI, Docker Hub, and Docker Desktop.\\nIn the next chapter, we’ll take a similar tour of containers — the run-time sibling of\\nimages.',\n",
              " '7: Working with containers\\nDocker implements the Open Container Initiative (OCI) specifications. This means\\nsome of the things you’ll learn in this chapter will apply to other container runtimes and\\nplatforms that implement the OCI specifications.\\nI’ve divided the chapter into the following sections:\\n• Container – The TLDR\\n• Containers vs VMs\\n• Images and containers\\n• Check Docker is running\\n• Starting a container\\n• How containers start apps\\n• Connecting to a container\\n• Inspecting container processes\\n• The docker inspect command\\n• Writing data to a container\\n• Stopping, restarting, and deleting a container\\n• Killing a container’s main process\\n• Debugging slim images and containers with Docker Debug\\n• Self-healing containers with restart policies\\n• The commands\\nContainers – The TLDR\\nContainers are run-time instances of images, and you can start one or more containers\\nfrom a single image.\\nFigure 7.1 shows multiple containers started from a single image. The shared image is\\nread-only, but you can write to the containers.',\n",
              " '7: Working with containers 68\\nFigure 7.1\\nYou can start, stop, restart, and delete containers just like you can with VMs. However,\\ncontainers are smaller, faster, and more portable than VMs. They’re also designed to\\nbe stateless and ephemeral, whereas VMs are designed to be long-running and can be\\nmigrated with their state and data.\\nContainers are also designed to beimmutable. This means you shouldn’t change them\\nafter you’ve deployed them — if a container fails, you replace it with a new one instead\\nof connecting to it and making the fix in the live instance.\\nContainers should only run a single process and we use them to build microservices\\napps. For example, an application with four features (microservices), such as a web\\nserver, auth, catalog, and store, will have four containers — one running the web server,\\none running the auth service, one running the catalog, and another running the store.\\nContainers vs VMs\\nContainers and VMs are both virtualization technologies for running applications. They\\nboth work on your laptop, bare metal servers, in the cloud, and more. However, the\\nways theyvirtualize are very different:\\n• VMs virtualize hardware\\n• Containers virtualize operating systems\\nIn the VM model, you power on a server and a hypervisor boots. When the hypervisor\\nboots, it claims all hardware resources such as CPU, RAM, storage, and network\\nadapters. To deploy an app, you ask the hypervisor to create a virtual machine. It does\\nthis by carving up the hardware resources into virtual versions, such as virtual CPUs\\nand Virtual RAM, and packaging them into a VM that looks exactly like a physical\\nserver. Once you have the VM, you install an OS and then an app.',\n",
              " '7: Working with containers 69\\nIn the container model, you power on the same server and an OS boots and claims all\\nhardware resources. You then install a container engine such as Docker. To deploy\\nan app, you ask Docker to create a container. It does this by carving up OS resources\\nsuch as process trees and filesystems into virtual versions and then packaging them as a\\ncontainer that looks exactly like a regular OS. You then tell Docker to run the app in the\\ncontainer.\\nFigure 7.2 shows the two models side by side and attempts to demonstrate the more\\nefficient nature of containers with the same server running 3x more containers than\\nVMs.\\nFigure 7.2\\nIn summary, hypervisors performhardware virtualizationwhere they divide hardware\\nresources into virtual versions and package them as VMs. Container engines perform\\nOS virtualizationwhere they divide OS resources into virtual versions and package them\\nas containers. VMs look and feel exactly like physical servers. Containers look and feel\\nexactly like regular operating systems.\\nThe VM tax\\nOne of the biggest problems with the virtual machine model is that you need to\\ninstall an OS on every VM — every OS consumes CPU, RAM, and storage and takes a\\nrelatively long time to boot.\\nContainers get around all of this by sharing a single OS on the host they’re running on.\\nThis gives containers all of the following benefits over VMs:',\n",
              " '7: Working with containers 70\\n• Containers are smaller and more portable\\n• You can run more containers on your infrastructure\\n• Containers start faster\\n• Containers reduce the number of operating systems you need to manage (patch,\\nupdate, etc.)\\n• Containers present a smaller attack surface\\nLet’s briefly expand on each point.\\nContainers are smaller than VMs because they only contain application code and a\\nminimal set of OS-related constructs, such as essential filesystem objects. Because of\\nthis, they’re typically only a few megabytes in size. On the other hand, every VM needs a\\nfull OS, meaning they’re usually hundreds or thousands of megabytes.\\nBecause containers don’t contain an OS, you can run a lot more containers than VMs.\\nFor example, deploying 100 applications as VMs will require 100 operating systems,\\neach consuming CPU, memory, and storage, and each needing to be patched and\\nmanaged. However, deploying the same 100 applications as containers requires no\\nadditional operating systems. This drastically reduces your OS management overhead\\nand allows you to allocate more system resources to applications instead of operating\\nsystems.\\nContainers also start faster than VMs because they use the host’s OS which is already\\nbooted. On the other hand, VMs need to go through a full OS bootstrapping process\\nbefore starting the app.\\nOne of the early concerns about containers centered around the shared kernel model\\nwhere all containers on the same host share the host’s kernel. While this offers perfor-\\nmance and portability benefits, it’s less secure than the VM model where every VM has\\nits own dedicated kernel. For example, a rogue container that exploits a vulnerability\\nin the host’s kernel might be able to impact every other container on the same host.\\nFortunately, this is much less of a concern now that container platforms have matured\\nand ship with class-leading tools that can make them more secure than non-container\\nplatforms. For example, most container engines and platforms implementsensible\\ndefaults for security-related technologies such asSELinux, AppArmor, seccomp, capabilities,\\nand more. You can even configure these to make containers more secure than VMs.\\nOther technologies, such as image vulnerability scanning, give you more control over\\nthe security of your software than you ever had before.\\nAt the time of writing, in 2024, containers are the go-to solution for the vast majority of\\nnew applications.',\n",
              " '7: Working with containers 71\\nPre-reqs\\nYou’ll need a working Docker environment to follow along with the examples, and I\\nrecommend Docker Desktop. Other Docker setups should work, but you may have\\nto manually install the Docker Debug plugin if you want to follow along with those\\nexamples.\\nImages and Containers\\nAs previously mentioned, you can start multiple containers from a single image. The\\nimage is read-only in this relationship, but each container is read-write. As shown\\nin Figure 7.3, Docker accomplishes this by creating a thin read-write layer for each\\ncontainer and placing it on top of the shared image.\\nFigure 7.3 - Container R/W layers\\nIn this example, each container has its own thin R/W layer but shares the same image.\\nThe containers can see and access the files and apps in the imagethrough their own R/W\\nlayer, and if they make any changes, these get written to their R/W layer. When you stop\\na container, Docker keeps the R/W layer and restores it when you restart the container.\\nHowever, when you delete a container, Docker deletes its R/W layer. This way, each\\ncontainer can make and keep its own changes without changing the shared image.',\n",
              " '7: Working with containers 72\\nCheck Docker is running\\nRun adocker version to check Docker is running. It’s a good command because it\\nchecks the CLI and engine components.\\n$ docker version\\nClient:\\nCloud integration: v1.0.35+desktop.13\\nVersion: 26.1.1\\nAPI version: 1.45\\nOS/Arch: darwin/arm64\\n<Snip>\\nServer: Docker Desktop 4.30.0 (149282)\\nEngine:\\nVersion: 26.1.1\\nAPI version: 1.45 (minimum version 1.24)\\nOS/Arch: linux/arm64\\n<Snip>\\nAs long as you get a response from theClient and Server, you’re good to go and can\\nskip to the next section.\\nIf you get an error code in theServer section, this usually means your Docker daemon\\n(server) isn’t running or your user account doesn’t have permission to access it. If\\nyou’re running on Linux, you’ll need to ensure your user account is a member of the\\nlocal docker Unix group. If it isn’t, you can add it by runningusermod -aG docker\\n<username> and restarting your shell. Alternatively, you can prefix alldocker commands\\nwith sudo.\\nYour account needs to be a member of thedocker group so it can access the API which\\nis exposed on a privileged local Unix socket at/var/run/docker.sock. It’s also possible\\nto expose the API over the network.\\nIf your user account is already a member of the localdocker group and you still get an\\nerror from the daemon, there’s a good chance the Docker daemon isn’t running. Run\\none of the following commands to check the status of the daemon.\\nLinux systems not using Systemd.\\n$ service docker status\\ndocker start/running, process 29393\\nLinux systems using Systemd.',\n",
              " \"7: Working with containers 73\\n$ systemctl is-active docker\\nactive\\nIf the daemon isn’t running, start it with the appropriate command for your system.\\nStarting a container\\nThe docker run command is the simplest and most common way to start a new\\ncontainer.\\nRun the following command to start a new container calledwebserver.\\n$ docker run -d --name webserver -p 5005:8080 nigelpoulton/ddd-book:web0.1\\nUnable to find image 'nigelpoulton/ddd-book:web0.1' locally\\nweb0.1: Pulling from nigelpoulton/ddd-book\\n4f4fb700ef54: Already exists\\ncf2a607f33f7: Download complete\\n0a1f0c111e9a: Download complete\\nc1af4b5db242: Download complete\\nDigest: sha256:3f5b281b914b1e39df8a1fbc189270a5672ff9e98bfac03193b42d1c02c43ef0\\nStatus: Downloaded newer image for nigelpoulton/ddd-book:web0.1\\nb5594b3b8b3fdce544d2ca048e4340d176bce9f5dc430812a20f1852c395e96b\\nLet’s take a closer look at the command and the output.\\ndocker run tells Docker to run a new container\\nThe -d flag tells Docker to run it in the background as adaemon processand detached\\nfrom your local terminal\\nThe name flag tells Docker to name this containerwebserver.\\nThe -p 5005:8080 flag maps port5005 on your local system to port8080 inside the\\ncontainer. This works because the container’s web server is listening on port8080.\\nThe nigelpoulton/ddd-book:web0.1 argument tells Docker which image to use to start\\nthe container.\\nWhen you hitReturn, the Docker client converted the command into an API request\\nand posted it to the Docker API exposed by the Docker daemon. The Docker daemon\\naccepted the command and searched its local image repository for a copy of the\\nnigelpoulton/ddd-book:web0.1 image. It didn’t find one, so it searched Docker Hub.\\nIn the example, it found one on Docker Hub and pulled a local copy.\\nOnce it had a local copy of the image, the daemon made a request to containerd asking\\nfor a new container. containerd then instructed runc to create the container and start\\nthe app. It also performed the port mapping.\",\n",
              " '7: Working with containers 74\\nRun the following commands to verify the image was pulled locally and a new container\\ncalled webserver is running.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book web0.1 3f5b281b914b 12 minutes ago 159MB\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND STATUS PORTS NAMES\\nb5594b3b8b3f nigelpoulton... \"node ./app.js\" Up 2 mins 0.0.0.0:80->8080/tcp webserver\\nYou can also connect a browser to port5005 on your Docker host to test the app. If\\nyou’re using Docker Desktop, point your browser tolocalhost:5005. If you’re not\\nrunning Docker Desktop, you may need to substitutelocalhost with the name or IP\\nof the host Docker is running on.\\nFigure 7.4 - Web app running on container\\nCongratulations. Docker pulled a local copy of the image and started a container\\nrunning the app defined in the image.\\nHow containers start apps\\nIn the previous section, you created a container running a web app. But how did the\\ncontainer know which app to start and how to start it?\\nThere are three ways you can tell Docker how to start an app in a container:',\n",
              " '7: Working with containers 75\\n1. An Entrypoint instruction in the image\\n2. A Cmd instruction in the image\\n3. A CLI argument\\nYou’ll learn more about these in the next chapter, but theEntrypoint and Cmd instruc-\\ntions are optional image metadata that store the command Docker uses to start the\\ndefault app. Then, whenever you start a container from the image, Docker checks the\\nEntrypoint or Cmd instruction and executes the stored command.\\nEntrypoint instructions cannot be overridden on the CLI, and anything you pass in via\\nthe CLI will be appended to the Entrypoint instruction as an argument.\\nCmd instructions can be overridden by CLI arguments.\\nRun the following command to see if thenigelpoulton/ddd-book:web0.1 image has\\nan Entrypoint instruction. The command searches the image metadata and returns\\nany lines containing the word“Entrypoint” and the three lines immediately following it.\\nWindows users will need to replace thegrep command withSelect-String -Pattern\\n\\'Entrypoint\\' -Context 0,3.\\n$ docker inspect nigelpoulton/ddd-book:web0.1 | grep Entrypoint -A 3\\n<Snip>\\n\"Entrypoint\": [\\n\"node\",\\n\"./app.js\"\\n],\\nThis image has an Entrypoint instruction that translates into the following command\\n— node ./app.js. If you’re not familiar with Node.js, it’s a simple command telling the\\nNode.js runtime to execute the code in theapp.js file.\\nIf an image doesn’t have an Entrypoint instruction, you can search for the presence of a\\nCmd instruction.\\nIf an image doesn’t have either, you’ll need to pass an argument on the CLI.\\nThe format of thedocker run command is:\\ndocker run <arguments> <image> <command>\\nAs mentioned, the<command> is optional; you don’t need it if the image has an En-\\ntrypoint or Cmd instruction. If you specify a <command>, it will override a Cmd\\ninstruction but will be appended to an Entrypoint instruction.\\nThe following command starts a new background container based on theAlpine image\\nand tells it to run thesleep 20 command, causing it to run for 20 seconds and then exit.\\nThe --rm flag cleans up the exited container so you don’t have to delete it manually.',\n",
              " '7: Working with containers 76\\n$ docker run --rm -d alpine sleep 20\\nIf you run adocker ps command before the 20-second sleep timer expires, you’ll see\\nthe container in the output. If you run it after 20 seconds, the container will be gone.\\nThe --rm argument automatically cleans up the exited container.\\nMost production images will specify an Entrypoint or Cmd instruction.\\nConnecting to a container\\nYou can use thedocker exec command to execute commands in running containers,\\nand it has two modes:\\n• Interactive\\n• Remote execution\\nInteractive exec sessions connect your terminal to a shell process in the container and\\nbehave like remote SSH sessions. Remote execution mode lets you send commands to a\\nrunning container and prints the output to your local terminal.\\nRun the following command to start an interactive exec session by creating a new shell\\nprocess (sh) inside thewebserver container and connecting your terminal to it. The-\\nit flag makes it aninteractive exec session, and thesh argument starts a newsh process\\ninside the container.sh is a minimal shell program installed in the container.\\n$ docker exec -it webserver sh\\n/src #\\nNotice how your shell prompt changed. This proves your terminal is connected to the\\nshell process inside the container.\\nTry executing a few common Linux commands. Some will work, and some won’t. This\\nis because container images are usually optimized to be lightweight and don’t have all of\\nthe normal commands and packages installed. The following example shows a couple of\\ncommands — one succeeds, and the other one fails.\\nThe examples list the contents of your current directory and try to edit theapp.py file\\nwith thevim editor.',\n",
              " '7: Working with containers 77\\n/src # ls -l\\ntotal 100\\n-rw-r--r-- 1 root root 324 Feb 20 12:35 Dockerfile\\n-rw-r--r-- 1 root root 377 Feb 20 12:35 README.md\\n-rw-r--r-- 1 root root 341 Feb 20 12:35 app.js\\ndrwxr-xr-x 183 root root 4096 Feb 20 12:41 node_modules\\n-rw-r--r-- 1 root root 74342 Feb 20 12:41 package-lock.json\\n-rw-r--r-- 1 root root 404 Feb 20 12:38 package.json\\ndrwxr-xr-x 2 root root 4096 Feb 20 12:35 views\\n<Snip>\\n/src # vim app.js\\nsh: vim: not found\\nThe vim command fails because it isn’t installed in the container.\\nInspecting container processes\\nMost containers only run a single process. This is the container’s main app process and\\nis always PID 1.\\nRun aps command to see the processes running in your container. You’ll need to be\\nconnected to the exec session from the previous section for these commands to work.\\n/src # ps\\nPID USER TIME COMMAND\\n1 root 0:00 node ./app.js\\n13 root 0:00 sh\\n22 root 0:00 ps\\nThe output shows three processes:\\n• PID 1 is the main application process running the Node.js web app\\n• PID 13 is the shell process your interactive exec session is connected to\\n• PID 22 is theps command you just ran\\nThe ps process terminated as soon as it displayed the output, and thesh process will\\nterminate when you exit theexec session. This means the only long-running process is\\nPID 1 running the Node app.\\nIf you kill the container’s main process (PID 1), you’ll also kill the container. This is\\nbecause containers only run while their main process is running — when that process\\nis no longer running, there’s no reason for the container to run. We’ll demonstrate this\\nlater.',\n",
              " '7: Working with containers 78\\nType exit to quit the exec session and return to your local terminal.\\nRun anotherdocker exec command without specifying the-it flags. This will\\nremotely execute the command without creating an interactive session. The format of\\nthe command isdocker exec <container> <command>, and it will only work if the\\ncommand you’re executing is installed in the container.\\n$ docker exec webserver ps\\nPID USER TIME COMMAND\\n1 root 0:00 node ./app.js\\n42 root 0:00 ps\\nThis time, only two processes are running because you terminated thesh process when\\nyou typedexit to quit the previous interactive exec session.\\nThe docker inspect command\\nYou’ll absolutely love thedocker inspect command as it’s a treasure trove of detailed\\ninformation about images and containers.\\nThe following command retrieves full details of the runningwebserver container, and\\nI’ve snipped the output to highlight a few interesting things. However, I recommend you\\nrun the command on your own system and study the full output, as you’ll learn a lot.\\n$ docker inspect webserver\\n<Snip>\\n\"State\": {\\n\"Status\": \"running\"\\n},\\n\"Name\": \"/webserver\",\\n\"PortBindings\": {\\n\"8080/tcp\": [\\n{\\n\"HostIp\": \"\",\\n\"HostPort\": \"5005\"\\n}\\n]\\n},\\n\"RestartPolicy\": {\\n\"Name\": \"no\",\\n\"MaximumRetryCount\": 0\\n\"Image\": \"nigelpoulton/ddd-book:web0.1\",\\n\"WorkingDir\": \"/src\",\\n\"Entrypoint\": [\\n\"node\",\\n\"./app.js\"',\n",
              " '7: Working with containers 79\\n],\\n}\\n<Snip>\\nThe snipped output shows the container isrunning, is calledwebserver, is binding port\\n8080 in the container to5005 on the host, has no restart policy, and is based on the\\nnigelpoulton/ddd-book:web0.1 image. TheEntrypoint block lists the command that\\nautomatically runs every time the container starts.\\nWe’ll cover this in more detail later, but this container inherited its Entrypoint instruc-\\ntion from the image you started it from. You can verify this by running the following\\ndocker inspect command against the image. I’ve snipped the output to highlight the\\nrelevant section.\\n$ docker inspect nigelpoulton/ddd-book:web0.1\\n<Snip>\\n\"Config\": {\\n\"WorkingDir\": \"/src\",\\n\"Entrypoint\": [\\n\"node\",\\n\"./app.js\"\\n],\\n<Snip>\\nI recommend you take time to investigate the output ofdocker inspect commands.\\nYou’ll learn a lot.\\nWriting data to a container\\nIn this section, you’ll exec onto thewebserver container and edit the web server\\nconfiguration to display a new message on the home page. In the next section, you’ll\\nstop and restart the container and verify your changes aren’t lost.\\nWARNING: This section is for demonstration purposes only. In the real\\nworld, you shouldn’t change live containers like this. Any time you need to\\nchange a live container, you should create and test a new container with the\\nrequired changes and then replace the existing container with the new one.\\nOpen a new interactive exec session to thewebserver container with the following\\ncommand.',\n",
              " '7: Working with containers 80\\n$ docker exec -it webserver sh\\n/src #\\nThe container runs a simple Node.js web app that uses theviews/home.pug file to build\\nthe app’s home page.\\nRun the following command to open thehome.pug file in thevi editor. Windows users\\ncan use Notepad or another editor.\\n% vi views/home.pug\\nIf you know how to usevi, you can go ahead and change the text on line 8 after theh1\\ntag to anything you like and save your changes.\\nCarefully follow these steps if you’re not comfortable withvi:\\n1. Press thei key to putvi into insert mode\\n2. Use the arrow keys to navigate to line 8\\n3. Use yourdelete key to delete the textafter the h1 tag on line 8\\n4. Type a new message of your choice\\n5. Press theescape key to exitinsert modeand return tocommand mode\\n6. type :wq and pressenter save your changes and exit (:wq is short for write and\\nquit)\\nOnce you’ve saved your changes, refresh your browser to see the updates.\\nType exit to quit the exec session and return to your local terminal.\\nCongratulations, you’ve updated the web server config.\\nStopping, restarting, and deleting a container\\nIn this section, you’ll execute the typical container lifecycle events and see how they\\nimpact the changes you’ve made to the container.\\nThe following commands will only work if you’ve quit the interactive exec session.\\nCheck your container is still running.',\n",
              " '7: Working with containers 81\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND STATUS PORTS NAMES\\nb5594b3b8b3f nigelpoulton... \"node ./app.js\" Up 51 mins 0.0.0.0:80->8080 webserver\\nStop it with thedocker stop command. It will take up to 10 seconds to stop gracefully.\\n$ docker stop webserver\\nwebserver\\nRun anotherdocker ps command.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\nThe container no longer shows in the list of running containers. However, you can see it\\nif you run the same command with the-a flag to showall containers, including stopped\\nones.\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND STATUS NAMES\\nb5594b3b8b3f nigelpou... \"node ./app.js\" Exited (137) About a minute ago webserver\\nAs you can see in the output, it still exists but is in theExited state. Restart it with the\\nfollowing command.\\n$ docker restart webserver\\nwebserver\\nIf you run anotherdocker ps, you’ll see it in theUp state.\\nRefresh your browser to see if Docker has saved your changes to the home page or\\nreverted to the original.\\nThe container has saved your changes!\\nYou can also run the following command to return the exact contents of the file directly\\nfrom within the container’s filesystem.',\n",
              " \"7: Working with containers 82\\n$ docker exec webserver cat views/home.pug\\nhtml\\nhead\\ntitle='Docker FTW'\\nlink(rel='stylesheet', href='https://stackpath.bootstrapcdn.com/....\\nbody\\ndiv.container\\ndiv.jumbotron\\nh1 Everybody loves containers! <<---- I changed this line\\n<Snip>\\nSo far, you’ve seen that starting and stopping containers doesn’t lose changes. You also\\nsaw that restarting them is very fast.\\nRun the following command to delete the container. The-f flag forces the operation\\nand doesn’t allow the app the usual 10-second grace period to flush buffers and grace-\\nfully quit. Be careful forcing operations like this, as Docker doesn’t ask you to confirm.\\n$ docker rm webserver -f\\nwebserver\\nRun adocker ps -a to see if there’s any sign of the container.\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\\nAll signs of the container are gone and you cannot restart it. You can start a new\\ninstance by executing anotherdocker run command and specifying the same image,\\nbut it won’t have the changes you made.\\nWARNING: As previously mentioned, changing live containers like this is an\\nanti-pattern and you shouldn’t do it. We only showed it here to demonstrate\\nhow containers work and how changes to the container’s filesystem (made to\\nthe container’s own thin R/W layer) persist across restarts.\\nKilling a container’s main process\\nEarlier in the chapter, we learned that containers are designed to run a single process,\\nand killing this process also kills the container.\\nLet’s test if that’s true.\\nRun the following command to start a new interactive container calledddd-ctr based on\\nthe Ubuntu image and tell it to run a Bash shell as its main process.\",\n",
              " '7: Working with containers 83\\n$ docker run --name ddd-ctr -it ubuntu:24.04 bash\\nUnable to find image \\'ubuntu:24.04\\' locally\\n24.04: Pulling from library/ubuntu\\n51ae9e2de052: Download complete\\nDigest: sha256:ff0b5139e774bb0dee9ca8b572b4d69eaec2795deb8dc47c8c829becd67de41e\\nStatus: Downloaded newer image for ubuntu:24.04\\nroot@d3c892ad0eb3:/#\\nThe command pulls the Ubuntu image and attaches your terminal to the container’s\\nBash shell process.\\nRun aps command to list all running processes.\\nroot@d3c892ad0eb3:/# ps\\nPID TTY TIME CMD\\n1 pts/0 00:00:00 bash\\n9 pts/0 00:00:00 ps\\nPID 1 is the container’s main process and is the Bash shell you told the container to run.\\nThe other one is theps command and has already exited. This means the Bash process is\\nthe only process running in the container.\\nIf you typeexit, you’ll terminate the Bash processand kill the container. This is because\\ncontainers only run while their main process executes.\\nTest this by typingexit to return to your local terminal and then running adocker ps\\n-a command to see if the container terminated.\\nroot@d3c892ad0eb3:/# exit\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND STATUS NAMES\\nd3c892ad0eb3 ubuntu:24.04 \"bash\" Exited (0) 3 secs ago ddd-ctr\\nAs expected, the container is in theexited state and not running. However, you can run\\nthe following two commands to restart it and attach your shell to its main process.\\n$ docker restart ddd-ctr\\nddd-ctr\\n$ docker attach ddd-ctr\\nroot@d3c892ad0eb3:/#\\nYour terminal is once again attached to the Bash shell in the container.\\nYou can typeCtrl PQ to exit a container without killing the process you’re attached to.\\nType Ctrl PQ to exit the container and run anotherdocker ps command to verify the\\ncontainer is still running this time.',\n",
              " '7: Working with containers 84\\nroot@d3c892ad0eb3:/# <Ctrl PQ>\\nread escape sequence\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND STATUS NAMES\\nd3c892ad0eb3 ubuntu:24.04 \"bash\" Up 27 seconds ddd-ctr\\nThe container is still up.\\nNow that you know how to exit containers without killing them, let’s switch focus and\\nsee how to useDocker Debugto debug slim containers and images.\\nDebugging slim images and containers with Docker\\nDebug\\nAt the time of writing, Docker Debug is a new tool and requires a Pro, Team, or Business\\nsubscription.\\nIt’s a widely accepted good practice to deploy small images containing only app code\\nand dependencies. This means no shell or debugging tools and is a big part of making\\nimages and containers small and secure. However, it also makes it difficult to debug\\nthem when things go wrong.\\nThis is whereDocker Debug comes to the rescue by allowing you to get shell access\\nto images and containers that don’t include a shell and seamlessly injecting powerful\\ndebugging tools into them.\\nAt a high level, Docker Debug works by attaching a shell to a container and mounting a\\ntoolbox loaded with debugging tools. Thistoolbox is mounted as a directory called/nix\\nand is available during your debugging session but is never visible to the container. As\\nsoon as you exit the Docker Debug session, the/nix directory is removed. If you’re\\ndebugging a running container, any changes you make are immediately visible to the\\ncontainer and persist across container restarts. For example, updating anindex.html\\nduring a Docker Debug session will immediately update the running web app, and the\\nchanges will persist if the container is stopped and restarted. If you’re debugging an\\nimage or stopped container, the Docker Debug session creates a debug sandbox and\\nadds it to the image as a R/W layer to make it feel like a running container. However,\\nchanges you make while debugging an image or stopped container arenot persisted and\\nare lost as soon as you quit the debug session.\\nIf you’ve been following along, you’ll have a running container calledddd-ctr. If you\\ndon’t, you can start one by runningdocker run --name ddd-ctr -it ubuntu:24.04\\nbash.',\n",
              " '7: Working with containers 85\\nRun the following commands to attach to the container and see if it has any debugging\\ntools. The followingdocker attach command is similar to thedocker exec commands\\nyou learned earlier but automatically connects to a container’s main process. You don’t\\nneed to run thedocker attach command if you’re already connected to the container.\\n$ docker attach ddd-ctr\\nroot@d3c892ad0eb3:/#\\nroot@d3c892ad0eb3:/# ping nigelpoulton.com\\nbash: ping: command not found\\nroot@d3c892ad0eb3:/# nslookup nigelpoulton.com\\nbash: nslookup: command not found\\nroot@d3c892ad0eb3:/# vim\\nbash: vim: command not found\\nThe commands all failed because none of the tools are installed in this container. This\\nwould make debugging this container difficult without Docker Debug.\\nType Ctrl PQ to gracefully disconnect from the container without killing the Bash\\nprocess.\\nIn the following steps, you’ll use Docker Debug to get a shell session to the container\\nand run commands that aren’t installed in the container. You can even use Docker\\nDebug to get shell access to containers and images that don’t include a shell.\\nYou need to log in to Docker to use Docker Debug, and it only works if you have a Pro,\\nTeam, or Business license.\\n$ docker login\\nAuthenticating with existing credentials...\\nLogin Succeeded\\nRun the following command to check if you have the Docker Debug CLI plugin. All\\nmodern versions of Docker Desktop include this by default. Other Docker installations\\nmay not have it, and you’ll have to install it manually.',\n",
              " '7: Working with containers 86\\n$ docker info\\nClient:\\nVersion: 26.1.1\\nContext: desktop-linux\\nDebug Mode: false\\nPlugins:\\ndebug: Get a shell into any image or container. (Docker Inc.)\\nVersion: 0.0.29\\nPath: /Users/nigelpoulton/.docker/cli-plugins/docker-debug\\n<Snip>\\nOnce you’re logged in and have the plugin installed, you’re ready to use Docker Debug.\\nThe format of the command isdocker debug <image>|<container>. We’ll open a\\nDocker Debug session to the running container calledddd-ctr.\\n$ docker debug ddd-ctr\\nThis is an attach shell, i.e.:\\n- Any changes to the container filesystem are visible to the container directly.\\n- The /nix directory is invisible to the actual container.\\nVersion: 0.0.29 (BETA)\\nroot@d3c892ad0eb3 / [ddd-ctr]\\ndocker >\\nYou’ve successfully connected to the running container and got a new shell prompt\\n(docker >). You also got some helpful info displaying the short ID and name of the\\ncontainer you’re debugging, as well as a reminder that any changes you make will be\\nvisible to the container.\\nTry running theping, nslookup, andvim commands that failed in the previous section.\\nIf you get stuck in thevim session, just type:q and pressEnter.',\n",
              " \"7: Working with containers 87\\ndocker > ping nigelpoulton.com\\nPING nigelpoulton.com (192.124.249.126) 56(84) bytes of data.\\n64 bytes from cloudproxy10126.sucuri.net (192.124.249.126): icmp_seq=1 ttl=63 time=211 ms\\n64 bytes from cloudproxy10126.sucuri.net (192.124.249.126): icmp_seq=2 ttl=63 time=58.3 ms\\n^C\\ndocker > nslookup nigelpoulton.com\\nzsh: command not found: nslookup\\ndocker > vim\\n~ VIM - Vi IMproved\\n~ version 9.0.1441\\n~ by Bram Moolenaar et al.\\n~ Vim is open source and freely distributable\\n<Snip>\\n:q\\nThe ping and vim commands worked, but thenslookup still failed. This is because the\\ndefault Docker Debugtoolbox includes ping and vim but doesn’t includenslookup. Don’t\\nworry, though. You can use Docker Debug’s built-ininstall command to add any\\npackage listed onsearch.nixos.org.\\nRun the following command to install thebind package (which includes thenslookup\\ntool), and then run thenslookup command again.\\ndocker > install bind\\nTip: You can install any package available at: https://search.nixos.org/packages.\\ninstalling 'bind-9.18.19'\\n<Snip>\\ndocker > nslookup nigelpoulton.com\\nServer: 192.168.65.7\\nAddress: 192.168.65.7#53\\nNon-authoritative answer:\\nName: nigelpoulton.com\\nAddress: 192.124.249.126\\nThe command worked, andnslookup is now installed in yourtoolbox and will be\\navailable in future Docker Debug sessions.\\nCongratulations, you’ve used Docker Debug to attach to a running container and run\\ntroubleshooting commands that aren’t part of the container. You’ve also seen how to\\ninstall additional tools to your Docker Debug toolbox. Remember, any changes you\\nmake to running containers are immediately visible to the container and persist after\\nyou close the session.\\nType exit to terminate the debug session and return to your local shell.\",\n",
              " '7: Working with containers 88\\nRun the following command to create a new Docker Debug session that debugs the\\nnigelpoulton/ddd-book:web0.1 image. Docker will automatically pull the image from\\nDocker Hub if you don’t have a local copy.\\n$ docker debug nigelpoulton/ddd-book:web0.1\\nNote: This is a sandbox shell. All changes will not affect the actual image.\\nVersion: 0.0.29 (BETA)\\nroot@3f5b281b914b /src [nigelpoulton/ddd-book:web0.1]\\ndocker >\\nNotice the different message this time. Debugging images creates asandbox shelland\\nchanges won’t affect the actual image. This reminds you that debugging images and\\nstopped containers behaves differently from debugging running containers:\\n• Changes made while debugging a live container are persisted\\n• Changes made while debugging images or stopped containers are deleted when\\nyou quit the debug session\\nRun annslookup command to prove the tool is saved to your toolbox and available for\\nuse without re-installing.\\ndocker > nslookup craigalanson.com\\nServer: 192.168.65.7\\nAddress: 192.168.65.7#53\\nNon-authoritative answer:\\nName: craigalanson.com\\nAddress: 198.185.159.144\\n<Snip>\\nDocker Debug has a built-inentrypoint command that lets you print, lint, and test an\\nimage or container’sEntrypoint or Cmd command. These are the commands Docker\\nexecutes to start the container’s app.\\nRun the followingentrypoint command to reveal the default command this container\\nwill run when it starts.',\n",
              " '7: Working with containers 89\\ndocker > entrypoint --print\\nnode ./app.js\\nThe entrypoint command is clever enough to look for Entrypointand Cmd instruc-\\ntions.\\nType exit to quit the debug session.\\nIn summary, Docker Debug is a new tool for debuggingslim images and containers. It\\ngets you shell access to containers and images that don’t include a shell, and you can\\nrun troubleshooting tools that aren’t available in the container or image. Any changes\\nyou make torunning containerstake immediate effect and persist across stop and restart\\noperations. However, changes made while debuggingimages and stopped containersare\\nlost when you close the session. In all cases, the tools you install and use are never part\\nof the container or image.\\nSelf-healing containers with restart policies\\nContainer restart policiesare a simple form of self-healing that allows the local Docker\\nEngine to automatically restart failed containers.\\nYou applyrestart policiesper container, and Docker supports the following four policies:\\n• no (default)\\n• on-failure\\n• always\\n• unless-stopped\\nThe following table shows how each policy reacts to different scenarios. AY indicates\\nthe policy will attempt a container restart, whereas anN indicates it won’t.\\nRestart\\nRestart Non-zero Zero docker stop when Daemon\\npolicy exit code exit code command restarts\\nno N N N N\\non-failure Y N N Y\\nalways Y Y N Y\\nunless-stopped Y Y N N\\nNon-zero exit codes indicate a failure occurred. Zero exit codes indicate the container\\nexited normally without an error.\\nWe’ll demo some examples, but you should also do your own testing.',\n",
              " '7: Working with containers 90\\nLet’s demonstrate thealways policy by starting a new interactive container with the--\\nrestart always flag and telling it to run a shell process. We’ll then typeexit to kill the\\nshell processand the container to see what happens.\\nRun the following command to start an interactive container calledneversaydie with\\nthe always restart policy.\\n$ docker run --name neversaydie -it --restart always alpine sh\\n/#\\nYour terminal will automatically connect to the shell process inside the container.\\nType exit to kill the shell process and return to your local terminal. This will cause the\\ncontainer to exit with a zero exit code, indicating a normal exit without any failures.\\nAccording to the previous table, thealways restart policy should automatically restart\\nthe container.\\nRun adocker ps command to see if this happened.\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\n1933623830bb alpine \"sh\" 15 seconds ago Up 2 seconds neversaydie\\nThe container is running as expected. However, you can see it was created 15 seconds\\nago but has only been running for 2 seconds. This is because you forced it to exit\\nwhen you killed the shell process, and then Docker automatically restarted it. It’s also\\nimportant to know that Docker restarted the same container and didn’t create a new\\none. In fact, if you run adocker inspect against it, you’ll see theRestartCount has\\nbeen incremented to 1. Remember to replacegrep with to replacegrep with Select-\\nString -Pattern \\'RestartCount\\' if you’re on Windows using PowerShell.\\n$ docker inspect neversaydie | grep RestartCount\\n\"RestartCount\": 1,\\nAn interesting feature of the--restart always policy is that if you stop a container\\nwith docker stop and then restart the Docker daemon, Docker will restart the con-\\ntainer when the daemon comes up. To be clear:\\n1. You start a new container with the--restart always policy\\n2. You manually stop it with thedocker stop command\\n3. You restart Docker (or an event causes Docker to restart)\\n4. When Docker comes back up, it starts thestopped container\\nIf you don’t want this behavior, you should try theunless-stopped policy.\\nIf you are working with Docker Compose or Docker Stacks, you can apply restart\\npolicies toservices as follows. We’ll cover these in more detail in later chapters.',\n",
              " '7: Working with containers 91\\nservices:\\nmyservice:\\n<Snip>\\nrestart_policy:\\ncondition: always | unless-stopped | on-failure\\nClean up\\nYou can rundocker images and docker ps -a commands to see the images you pulled\\nand the containers you created as part of this chapter. Your output will be similar to this.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book web0.1 3f5b281b914b 4 days ago 159MB\\nubuntu 24.04 ff0b5139e774 13 days ago 138MB\\nalpine latest c5b1261d6d3e 4 weeks ago 11.8MB\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAMES\\nac165419214f alpine \"sh\" 33 secs ago Up 24 seconds neversaydie\\n5bd3741185fa ubuntu:24.04 \"bash\" 3 mins ago Exited (0) ~1min ago ddd-ctr\\nYou can delete individual containers with thedocker rm <container> -f command\\nand images with thedocker rmi command, and you should always delete containers\\nbefore images.\\nYou can also deleteall containersand all imageswith the following two commands. Be\\nwarned though, they don’t prompt you for confirmation.\\n$ docker rm $(docker ps -aq) -f\\nac165419214f\\n5bd3741185fa\\n$ docker rmi $(docker images -q)\\nUntagged: nigelpoulton/ddd-book:web0.1\\nDeleted: sha256:3f5b281b914b1e39df8a1fbc189270a5672ff9e98bfac03193b42d1c02c43ef0\\nUntagged: ubuntu:24.04\\nDeleted: sha256:ff0b5139e774bb0dee9ca8b572b4d69eaec2795deb8dc47c8c829becd67de41e\\nUntagged: alpine:latest\\nDeleted: sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\\nBoth commands work by passing a list of all container/image IDs to the delete com-\\nmand.',\n",
              " '7: Working with containers 92\\nContainers – The commands\\n• docker run is the command to start new containers. You give it the name of an\\nimage and it starts a container from it. This example starts an interactive container\\nfrom the Ubuntu image and tells it to run the Bash shell:docker run -it ubuntu\\nbash.\\n• Ctrl-PQ is how you detach from a container without killing the process you’re\\nattached to. You’ll use it frequently to detach from running containers without\\nkilling them.\\n• docker ps lists all running containers, and you can add the-a flag to also see\\ncontainers in the stopped(Exited) state.\\n• docker exec allows you to run commands inside containers. The following\\ncommand will start a new Bash shell inside a running container and connect your\\nterminal to it:docker exec -it <container-name> bash. For this to work, the\\ncontainer must include the Bash shell. This command runs aps command inside\\na running container without opening an interactive shell session:docker exec\\n<container-name> ps.\\n• docker stop stops a running container and puts it in theExited (137) state. It\\nissues aSIGTERM to the container’s PID 1 process and allows the container 10\\nseconds to gracefully quit. If the process hasn’t cleaned up and stopped within 10\\nseconds, it sends aSIGKILL to force the container to immediately terminate.\\n• docker restart restarts a stopped container.\\n• docker rm deletes a stopped container. You can add the-f flag to delete the\\ncontainer without having to stop it first.\\n• docker inspect shows you detailed configuration and run-time information\\nabout a container.\\n• docker debug attaches a debug shell to a container or image and lets you run\\ncommands that aren’t available inside the container or image. It requires a Pro,\\nTeam, or Business Docker subscription.\\nChapter summary\\nIn this chapter, you learned some of the major differences between VMs and containers,\\nincluding that containers are smaller, faster, and more portable.\\nYou learned how to start, stop, and restart containers with thedocker CLI, and you saw\\nthat changes to a container’s filesystem persist across restarts.\\nYou learned that containers run a single process and cannot run if this process is killed.\\nYou also saw the three ways of telling a container which app to run and how to start it —\\nvia Entrypoint or Cmd instructions in the image metadata or via thedocker run CLI.',\n",
              " '7: Working with containers 93\\nYou learned about the new Docker Debug tool and how it allows you to get a shell to\\nslim containers and run troubleshooting commands that don’t exist in the container.\\nFinally, you learned how to attach restart policies to containers and how the different\\nrestart policies work.',\n",
              " '8: Containerizing an app\\nDocker makes it easy to package applications as images and run them as containers.\\nWe call this processcontainerization, and this chapter will walk you through the entire\\nprocess.\\nI’ve divided the chapter as follows:\\n• Containerizing an app – The TLDR\\n• Containerize a single-container app\\n• Moving to production with multi-stage-builds\\n• Buildx, BuildKit, drivers, and Build Cloud\\n• Multi-architecture builds\\n• A few good practices\\nContainerizing an app – The TLDR\\nDocker aims to make it easy tobuild, ship,and run applications. We call thiscontaineriza-\\ntion and the process looks like this:\\n1. Write your applications and create the list of dependencies\\n2. Create aDockerfile that tells Docker how to build and run the app\\n3. Build the app into an image\\n4. Push the image to a registry (optional)\\n5. Run a container from the image\\nYou can see these five steps in Figure 8.1.',\n",
              " '8: Containerizing an app 95\\nFigure 8.1 - Basic flow of containerizing an app\\nContainerize a single-container app\\nIn this section, you’ll complete the following steps to containerize a simple Node.js app:\\n• Get the application code from GitHub\\n• Create the Dockerfile\\n• Containerize the app\\n• Run the app\\n• Test the app\\n• Look a bit closer\\nI recommend you follow along with Docker Desktop. This is because we’ll be using the\\nnew docker init command, which might not be installed on other versions of Docker.\\nDon’t worry if you don’t have access to a Docker installation with thedocker init\\nplugin, you can manually copy the Dockerfile and follow the rest of the examples.',\n",
              " \"8: Containerizing an app 96\\nGet the application code\\nThe application we’ll use is a simple Node.js web app that serves a web page on port\\n8080.\\nIf you still need to clone the repo, run the following command to clone it. You’ll need\\ngit installed, and it will create a new directory calledddd-book.\\n$ git clone https://github.com/nigelpoulton/ddd-book.git\\nCloning into 'ddd-book'...\\nremote: Enumerating objects: 47, done.\\nremote: Counting objects: 100% (47/47), done.\\nremote: Compressing objects: 100% (32/32), done.\\nremote: Total 47 (delta 11), reused 44 (delta 11), pack-reused 0\\nReceiving objects: 100% (47/47), 167.30 KiB | 1.66 MiB/s, done.\\nResolving deltas: 100% (11/11), done.\\nChange into theddd-book/node-app directory and list its contents.\\n$ cd ddd-book/node-app\\n$ ls -l\\ntotal 98\\n-rw-r--r--@ 1 nigelpoulton staff 341 20 Feb 12:35 app.js\\ndrwxr-xr-x 103 nigelpoulton staff 3296 12 Mar 16:18 node_modules\\n-rw-r--r-- 1 nigelpoulton staff 39975 12 Mar 16:18 package-lock.json\\n-rw-r--r--@ 1 nigelpoulton staff 355 8 Mar 10:10 package.json\\ndrwxr-xr-x 3 nigelpoulton staff 96 20 Feb 12:35 views\\nThis directory is yourbuild contextbecause it contains the application source code and\\nthe files listing dependencies.\\nLet’s create the Dockerfile.\\nCreate the Dockerfile\\nIn the past, you had to create Dockerfiles manually. Fortunately, newer versions of\\nDocker support thedocker init command that analyses applications and automatically\\ncreates Dockerfiles that implement good practices.\\nRun the following command to create a Dockerfile for the app. If your Docker installa-\\ntion doesn’t have thedocker init plugin, you’ll have to skip this step.\\nFeel free to accept a newer version of Node.js, but complete all other prompts as shown.\\nYou’ll need to run it from thenode-app directory.\",\n",
              " '8: Containerizing an app 97\\n$ docker init\\nWelcome to the Docker Init CLI!\\n<Snip>\\n? What application platform does your project use? Node\\n? What version of Node do you want to use? 20.8.0 <<---- Newer versions are OK\\n? Which package manager do you want to use? npm\\n? What command do you want to use to start the app? node app.js\\n? What port does your server listen on? 8080\\nCREATED: .dockerignore\\nCREATED: Dockerfile\\nCREATED: compose.yaml\\nCREATED: README.Docker.md\\n\\uffff Your Docker files are ready!\\nThe process created a new Dockerfile and placed it in your current directory. It looks\\nlike this.\\n1. ARG NODE_VERSION=20.8.0\\n2. FROM node:${NODE_VERSION}-alpine\\n3. ENV NODE_ENV production\\n4. WORKDIR /usr/src/app\\n5. RUN --mount=type=bind,source=package.json,target=package.json \\\\\\n--mount=type=bind,source=package-lock.json,target=package-lock.json \\\\\\n--mount=type=cache,target=/root/.npm \\\\\\nnpm ci --omit=dev\\n6. USER node\\n7. COPY . .\\n8. EXPOSE 8080\\n9. CMD node app.js\\nLines 1 and 2 tell Docker to pull thenode:20.8.0-alpine image and use it as the base\\nfor the new image.\\nLine 3 tells Node to run inproduction mode. This is a Node.js optimization that increases\\nperformance while minimizing logging and other common development features.\\nLine 4 sets the working directory for the remaining steps. For example, theRUN and COPY\\ninstructions on lines 5 and 7 will run against theWORKDIR directory, as will thenode\\napp.js command on line 9.\\nLine 5 bind mounts the dependency files and installs them with thenpm ci --omit-dev\\ncommand.\\nLine 6 ensures Node.js runs the app as a non-root user.\\nLine 7 copies the application’s source code from your build context (the first period)\\ninto theWORKDIR directory (the second period) inside the image.',\n",
              " '8: Containerizing an app 98\\nLine 8 documents the application’s network port.\\nLine 9 is the command Docker will execute whenever it starts a container from the\\nimage.\\nYou now have everything Docker needs to build the application into a container image\\n— source code, dependencies, and a Dockerfile.\\nContainerize the app\\nIn this section, you’ll build the application into a container image.\\nIf your Docker installation doesn’t have thedocker init plugin and you didn’t follow\\nthe previous step, you’ll need to rename thesample-Dockerfile to Dockerfile before\\ncontinuing.\\nRun the following command to build a new image calledddd-book:ch8.node. Be sure to\\ninclude the trailing period (.) as this tells Docker to use your current working directory\\nas thebuild context. Remember, thebuild contextis the directory where your app files live.\\n$ docker build -t ddd-book:ch8.node .\\n[+] Building 16.2s (12/12) FINISHED\\n=> [internal] load build definition from Dockerfile 0.0s\\n=> => transferring dockerfile: 1.21kB 0.0s\\n=> => transferring context: 659B 0.0s\\n=> [stage-0 1/4] FROM docker.io/library/node:20.8.0-alpine 3s <<---- Base layer\\n=> [stage-0 2/4] WORKDIR /usr/src/app 0.2s <<---- New layer\\n=> [stage-0 3/4] RUN --mount=type=bind,source=package... 1.1s <<---- New layer\\n=> [stage-0 4/4] COPY . . 0.1s <<---- New layer\\n=> exporting to image 0.2s\\n=> => exporting layers 0.2s\\n=> => writing image sha256:f282569b8bd0f0...016cc1adafc91 0.0s\\n=> => naming to docker.io/library/ddd-book:ch8.node\\nI’ve snipped the output, but you can see four numbered steps that created four image\\nlayers.\\nCheck that the image exists in your Docker host’s local repository.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\nddd-book ch8.node 24dd040fa06b 18 minutes ago 268MB\\nCongratulations, you’ve containerized the app as an OCI image!\\nRun adocker inspect ddd-book:ch8.node command to verify the image and see the\\nsettings from the Dockerfile. You should be able to see the image layers and metadata\\nsuch as theExposed Ports, WorkingDir, andEntrypoint values.',\n",
              " '8: Containerizing an app 99\\n$ docker inspect ddd-book:ch8.node\\n[\\n{\\n\"Id\": \"sha256:24dd040fa06baf6e40144c5a59f99a749159a932ecebb737751f7f862963527a\",\\n\"RepoTags\": [\\n\"ddd-book:ch8.node\"\\n<Snip>\\n\"ExposedPorts\": {\\n\"8080/tcp\": {}\\n\"WorkingDir\": \"/usr/src/app\",\\n\"Cmd\": [\\n\"/bin/sh\",\\n\"-c\",\\n\"node app.js\"\\n],\\n<Snip>\\n\"Layers\": [\\n\"sha256:5f4d9fc4d98de91820d2a9c81e501c8cc6429bc8758b43fcb2cd50f4cab9a324\",\\n\"sha256:6b20c4e93dbab9786f96268bbe32c208d385f2c4490a278ad3b1e55cc79480e4\",\\n\"sha256:012c308a78ec993a47fdb7c4c6d17b53d8ce2649a463be28ae5c48ab1af2e039\",\\n\"sha256:35a839ac7cc922afd896a0297e692141c77ed6e03eff6a70db13bb23f6cd4f8f\",\\n\"sha256:918caa8070410ccfb2c5b3b4d62ca66742c46bf21fe0bd433738b7796c530e68\",\\n\"sha256:a48b3b3d0c5a693840e7e4abd7971f130b4447573483628bcb996091e1e8e8b8\",\\n\"sha256:ea2d4594dbbef4009441a33dd1dd4c5076d7fe09a171381a6b7583605569dd11\"\\n]\\n<Snip>\\nYou might wonder why the image has seven layers when only four Dockerfile instruc-\\ntions created layers. This is because thenode:20.8.0-alpine base image already had\\nfour layers. Therefore, theFROM instruction pulled a base image with four layers, and\\nthen theWORKDIR, RUN and COPY instructions added three more layers. You can see this\\nin Figure 8.2.',\n",
              " '8: Containerizing an app 100\\nFigure 8.2 - Dockerfile and image layers\\nPush the image to Docker Hub\\nThis is an optional section, and you’ll need a Docker Hub account to follow along. Go to\\nhub.docker.com and sign up for one now; they’re free.\\nYou’ll complete the following steps:\\n1. Login to Docker Hub\\n2. Re-tag the image\\n3. Push the image\\nAfter creating images, you’ll normally push them to a registry where you can keep them\\nsafe and make them accessible to teammates and clients. Lots of registries exist, but\\nDocker Hub is the most common public registry and is where Docker pushes images\\nby default.\\nLog in to Docker Hub.',\n",
              " '8: Containerizing an app 101\\n$ docker login\\nLogin with your Docker ID to push and pull images from Docker Hub.\\nUsername: nigelpoulton\\nPassword:\\nWARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\\nConfigure a credential helper to remove this warning.\\nOnce logged in, you need to re-tag the image. This is because Docker uses information\\nfrom the image tag to determine which registry and repository to push it to.\\nIf you run adocker images command, you’ll see an image tagged asddd-book:ch8.node.\\nIf you push this image, Docker will try to push it to a repository calledddd-book on\\nDocker Hub. However, no such repository exists, and the command will fail.\\nRun the following command to re-tag the image to include your Docker ID. The format\\nof the command isdocker tag <current-tag> <new-tag>, and it creates an additional\\ntag for the same image.\\n$ docker tag ddd-book:ch8.node nigelpoulton/ddd-book:ch8.node\\nRun anotherdocker images command to see the image with both tags. Notice how\\neverything is identical except theREPO column. This is because it’s the same image with\\ndifferent names.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book ch8.node 24dd040fa06b 38 minutes ago 268MB\\nddd-book ch8.node 24dd040fa06b 38 minutes ago 268MB\\nPush it to Docker Hub. You’ll need to be logged in with your Docker ID for this to work.\\n$ docker push nigelpoulton/ddd-book:ch8.node\\nThe push refers to repository [docker.io/nigelpoulton/ddd-book]\\ne4ef261755c8: Pushed\\nd25f74b85615: Pushed\\n7e1aebde141d: Pushed\\n7b3f8039e3c4: Pushed\\n2a2799ae89a2: Mounted from library/node\\n4927cb899c33: Mounted from library/node\\n579b34f0a95b: Pushed\\nced319b3ffb5: Pushed\\nch8.node: digest: sha256:24dd040fa06baf...1f7f862963527a size: 856\\nFigure 8.3 shows how Docker figured out where to push the image.',\n",
              " '8: Containerizing an app 102\\nFigure 8.3\\nNow that you’ve pushed the image to a registry, you can access it from anywhere with\\nan internet connection. You can also grant other people access to pull it and push\\nchanges.\\nRun the app\\nAs previously mentioned, the application is a web server that listens on port8080.\\nRun the following command to start it as a container. You’ll have to delete the\\nnigelpoulton image prefix or replace it with your ID.\\n$ docker run -d --name c1 \\\\\\n-p 5005:8080 \\\\\\nnigelpoulton/ddd-book:ch8.node\\nThe -d flag runs the container in the background, and the--name flag calls itc1. The\\n-p 5005:8080 maps port5005 on your Docker host to port8080 inside the container,\\nwhich means you’ll be able to point a browser to port5005 and reach the app. The last\\nline tells Docker to base the container on thenigelpoulton/ddd-book:ch8.node image\\nyou just built.\\nDocker will use the local copy of the image from the previous steps. It only pulls a copy\\nfrom Docker Hub if it doesn’t have a local copy.\\nCheck the container is running and verify the port mapping.\\n$ docker ps\\nID IMAGE COMMAND STATUS PORTS NAMES\\n49.. ddd-book:ch8.node \"node ./app.js\" UP 6 secs 0.0.0.0:5005->8080/tcp c1\\nI’ve snipped the output for readability, but the container is running, and port5005 is\\nmapped on all the Docker host’s interfaces (0.0.0.0:5005).',\n",
              " '8: Containerizing an app 103\\nTest the app\\nOpen a web browser and point it to the DNS name or IP address of your Docker host\\non port5005. If you’re using Docker Desktop or a similar local environment, you can\\nconnect tolocalhost:5005. Otherwise, use the IP or DNS of the Docker host on port\\n5005.\\nYou should see the app as shown in Figure 8.4.\\nFigure 8.4\\nYou can try the following if it doesn’t work:\\n1. Run adocker ps command to ensure thec1 container is running\\n2. Check port mapping is correct —0.0.0.0:5005->8080/tcp\\n3. Check that firewall and other network security settings aren’t blocking traffic to\\nyour Docker host on port5005\\nCongratulations, the application is containerized and running as a container!\\nLooking a bit closer\\nNow that you’ve containerized the application let’s take a closer look at how some of the\\nmachinery works.\\nThe docker build command parses the Dockerfile one line at a time, starting from the\\ntop.\\nYou can insert comments by starting a line with the# character, and the builder will\\nignore them.',\n",
              " '8: Containerizing an app 104\\nAll non-comment lines are calledinstructions or steps and take the format<INSTRUCTION>\\n<arguments>. Instruction names are not case-sensitive, but it’s common to write them in\\nUPPERCASE to make them easier to read.\\nSome instructions create new layers, whereas others add metadata.\\nExamples of instructions that create new layers areFROM, RUN, COPY and WORKDIR.\\nExamples that create metadata includeEXPOSE, ENV, CMD, andENTRYPOINT. The premise\\nis this:\\n• Instructions that addcontent, such as files and programs, create new layers\\n• Instructions that don’t add content don’t add layers and only create metadata\\nYou can run adocker history command against any image to see the instructions that\\ncreated it.\\n$ docker history ddd-book:ch8.node\\nIMAGE CREATED BY SIZE\\n24dd...a06b CMD [\"/bin/sh\" \"-c\" \"node app.js\"] 0B buildkit.dockerfile.v0\\n<missing> EXPOSE map[8080/tcp:{}] 0B buildkit.dockerfile.v0\\n<missing> COPY . . # buildkit 86kB buildkit.dockerfile.v0\\n<missing> USER node 0B buildkit.dockerfile.v0\\n<missing> RUN /bin/sh -c npm ci --omit=dev # buildkit 12.8MB buildkit.dockerfile.v0\\n<missing> WORKDIR /usr/src/app 16.4kB buildkit.dockerfile.v0\\n<missing> ENV NODE_ENV=production 0B buildkit.dockerfile.v0\\n<missing> /bin/sh -c #(nop) CMD [\"node\"] 0B\\n<Snip>\\n<missing> /bin/sh -c #(nop) ADD file:ff3112828967e8… 8.35MB\\nA few things are worth noting from the output.\\nThe bottom few lines that I’ve snipped from the book related to the history of the\\nnode:20.8.0-alpine base image that was pulled by theFROM instruction.\\nAll lines ending withbuildkit.dockerfile.v0 relate to instructions from the Docker-\\nfile used to build the image.\\nThe CREATED BY column lists the exact Dockerfile instruction that created the layer or\\nmetadata.\\nLines with a non-zero value in theSIZE column created new layers, whereas the lines\\nwith 0B only added metadata. In this example, three lines/instructions created layers.\\nRun adocker inspect to see the list of image layers.',\n",
              " '8: Containerizing an app 105\\n$ docker inspect ddd-book:ch8.node\\n<Snip>\\n},\\n\"RootFS\": {\\n\"Type\": \"layers\",\\n\"Layers\": [\\n\"sha256:5f4d9fc4d98de91820d2a9c81e501c8cc6429bc8758b43fcb2cd50f4cab9a324\",\\n\"sha256:6b20c4e93dbab9786f96268bbe32c208d385f2c4490a278ad3b1e55cc79480e4\",\\n\"sha256:012c308a78ec993a47fdb7c4c6d17b53d8ce2649a463be28ae5c48ab1af2e039\",\\n\"sha256:35a839ac7cc922afd896a0297e692141c77ed6e03eff6a70db13bb23f6cd4f8f\",\\n\"sha256:918caa8070410ccfb2c5b3b4d62ca66742c46bf21fe0bd433738b7796c530e68\",\\n\"sha256:a48b3b3d0c5a693840e7e4abd7971f130b4447573483628bcb996091e1e8e8b8\",\\n\"sha256:ea2d4594dbbef4009441a33dd1dd4c5076d7fe09a171381a6b7583605569dd11\"\\n]\\n},\\nAs previously mentioned, the output shows seven layers because the base image had four\\nlayers, and the Dockerfile added three more.\\nFigure 8.5 maps the Dockerfile instructions to image layers. The bold instructions with\\narrows create layers; the others create metadata. The layer IDs will be different in your\\nenvironment.\\nFigure 8.5\\nNote: Older builders didn’t create a layer forWORKDIR instructions. However,\\nthe instruction modifies filesystem permissions and the current builder\\ncreates a very small layer. This behavior may change in the future.\\nIt’s generally considered a good practice to useDocker Official Imagesand Verified Pub-\\nlisher images as thebase layerfor new images you create. This is because they maintain a',\n",
              " '8: Containerizing an app 106\\nhigh standard and quickly implement fixes for known vulnerabilities.\\nMoving to production with multi-stage builds\\nWhen it comes to container images…big is bad!For example:\\n• Big means slow\\n• Big means more potential vulnerabilities\\n• Big means a larger attack surface\\nFor these reasons, your container images should only contain the stuffneeded to run\\nyour applications in production.\\nThis is wheremulti-stage buildscome into play.\\nAt a high level, multi-stage builds use a single Dockerfile with multipleFROM instructions\\n— eachFROM instruction represents a newbuild stage. This allows you to have astage\\nwhere you do the heavy lifting of building the app inside a large image with compilers\\nand other build tools, but then you have another stage where you copy the compiled app\\ninto a slim image for production. The builder can even run different stages in parallel\\nfor faster builds.\\nFigure 8.6 shows a high-level workflow. Stage 1 builds an image with all the required\\nbuild and compilation tools. Stage 2 copies the app code into the image and builds it.\\nStage 3 creates a small production-ready image containing only the compiled app and\\nanything needed to run it.\\nFigure 8.6\\nLet’s look at an example!',\n",
              " '8: Containerizing an app 107\\nWe’ll work with the code in themulti-stage folder of the book’s GitHub repo. It’s a\\nsimple Go app with a client and server borrowed from theDocker samples buildmerepo\\non GitHub. Don’t worry if you’re not a Go programmer; you don’t need to be. You only\\nneed to know that it compiles theclient and server apps into executable files thatdo not\\nneed the Go language or any other tools or runtimes to execute.\\nHere’s the Dockerfile:\\nFROM golang:1.22.1-alpine AS base <<---- Stage 0\\nWORKDIR /src\\nCOPY go.mod go.sum .\\nRUN go mod download\\nCOPY . .\\nFROM base AS build-client <<---- Stage 1\\nRUN go build -o /bin/client ./cmd/client\\nFROM base AS build-server <<---- Stage 2\\nRUN go build -o /bin/server ./cmd/server\\nFROM scratch AS prod <<---- Stage 3\\nCOPY --from=build-client /bin/client /bin/\\nCOPY --from=build-server /bin/server /bin/\\nENTRYPOINT [ \"/bin/server\" ]\\nThe first thing to note is that there are fourFROM instructions. Each of these is a distinct\\nbuild stage, and Docker numbers them starting from 0. However, we’ve given each stage a\\nfriendly name:\\n• Stage 0 is calledbase and builds an image with compilation tools, etc\\n• Stage 1 is calledbuild-client and compiles the client executable\\n• Stage 2 is calledbuild-server and compiles the server executable\\n• Stage 3 is calledprod and copies the client and server executables into a slim image\\nEach stage outputs an intermediate image that later stages can use. However, Docker\\ndeletes them when the final stage completes.\\nThe goal of thebase stage is to create a reusable build image with all the tools stages\\n1 and 2 need to build the client and server applications. The image created by this\\nstage is only used to compile the executables and not for production. It pulls the\\ngolang:1.22.1-alpine image, which is over 300MB when uncompressed. It sets the\\nworking directory to/src and copies in thego.mod and go.sum files from your working\\ndirectory. These files list the application dependencies and hashes. After that, it uses the\\nRUN instruction to install the dependencies and then theCOPY instruction to copy the\\napplication source code into the image. All of this creates a large image with three layers',\n",
              " '8: Containerizing an app 108\\ncontaining a lot of build stuff but not much app stuff. When this build stage completes, it\\noutputs a large image that later stages can use.\\nThe build-client stage doesn’t pull a new image. Instead, it uses theFROM base AS\\nbuild-client instruction to use the intermediate image created by thebase stage. It\\nthen issues aRUN instruction to compile the client app into a binary executable. The goal\\nof this stage is to create an image with the compiledclient binary that can be referenced\\nby later stages.\\nThe build-server stage does the same for theserver component and outputs a similar\\nimage for use by later stages.\\nThe prod stage pulls the minimalscratch image. It then runs twoCOPY --from instruc-\\ntions to copy the compiledclient appfrom thebuild-client stage and the compiled\\nserver appfrom thebuild-server stage. It then tells Docker to run the server app when\\nit’s started as a container. This stage outputs the final production image containing just\\nthe client and server binaries inside a tiny scratch image and the metadata telling Docker\\nhow to start the app.\\nThe builder will run thebase stage first, then run thebuild-client and build-server\\nstages in parallel, and finally run theprod stage.\\nIt will always attempt to run stages in parallel, but it can only do this when no depen-\\ndency exists. For example, thebuild-client and build-server stages depend on the\\nbase stage and cannot run until that stage completes (both start withFROM base...).\\nHowever, thebuild-client and build-server can run in parallel because they don’t\\ndepend on each other. To work out if build stages can run in parallel, start reading\\nthe Dockerfile from the top and check if theFROM instructions reference otherFROM\\ninstructions immediately before or after — if they do, they can’t run in parallel.\\nLet’s see it in action.\\nChange into themulti-stage directory and verify the Dockerfile and associated app\\nfiles exist.\\n$ ls -l\\ntotal 28\\n-rw-rw-r-- 1 ubuntu ubuntu 368 Mar 25 10:09 Dockerfile\\n-rw-rw-r-- 1 ubuntu ubuntu 433 Mar 25 10:09 Dockerfile-final\\n-rw-rw-r-- 1 ubuntu ubuntu 305 Mar 25 10:09 README.md\\ndrwxrwxr-x 4 ubuntu ubuntu 4096 Mar 25 10:09 cmd\\n-rw-rw-r-- 1 ubuntu ubuntu 1013 Mar 25 10:09 go.mod\\n-rw-rw-r-- 1 ubuntu ubuntu 5631 Mar 25 10:09 go.sum\\nBuild the image and watch thebuild-client and build-server stages execute in\\nparallel. This can significantly improve the performance of large builds.',\n",
              " '8: Containerizing an app 109\\n$ docker build -t multi:full .\\n[+] Building 30.1s (15/15) FINISHED\\n=> [internal] load build definition from Dockerfile 0.0s\\n=> => transferring dockerfile: 409B 0.0s\\n<Snip>\\n=> [build-client 1/1] RUN go build -o /bin/client ./cmd/client 4.9s <<---- parallel\\n=> [build-server 1/1] RUN go build -o /bin/server ./cmd/server 4.8s <<---- parallel\\n<Snip>\\nRun adocker images to see the new image.\\n$ docker images\\nREPO TAG IMAGE ID CREATED SIZE\\nmulti full a7a01440f2b5 5 seconds ago 25.2MB\\nThe final production image is only 25MB, much smaller than the 300MB+ base image\\npulled by thebase stage to build and compile the app. This is because the finalprod\\nstage extracted the compiled client and server binaries and placed them in a tiny new\\nscratch image.\\nRun adocker history to see the final production image. It only has two layers — one\\ncreated by copying in the client binary and the other by copying in the server binary.\\nNone of the previous build stages are included in the final production image.\\n$ docker history multi:full\\nIMAGE CREATED CREATED BY SIZE\\na7a01440f2b5 4 minutes ago ENTRYPOINT [\"/bin/server\"] 0B\\n<missing> 4 minutes ago COPY /bin/server /bin/ # buildkit 8.2MB\\n<missing> 4 minutes ago COPY /bin/client /bin/ # buildkit 8.028MB\\nMulti-stage builds and build targets\\nYou can also build multiple images from a single Dockerfile.\\nThe previous example compiled client and server apps and copied both into the same\\nimage. However, Docker makes it easy to create a separate image for each by splitting\\nthe finalprod stage into two stages as follows:',\n",
              " '8: Containerizing an app 110\\nFROM golang:1.20-alpine AS base\\nWORKDIR /src\\nCOPY go.mod go.sum .\\nRUN go mod download\\nCOPY . .\\nFROM base AS build-client\\nRUN go build -o /bin/client ./cmd/client\\nFROM base AS build-server\\nRUN go build -o /bin/server ./cmd/server\\nFROM scratch AS prod-client <<---- New stage\\nCOPY --from=build-client /bin/client /bin/\\nENTRYPOINT [ \"/bin/client\" ]\\nFROM scratch AS prod-server <<---- New stage\\nCOPY --from=build-server /bin/server /bin/\\nENTRYPOINT [ \"/bin/server\" ]\\nI’ve pre-created the file and called itDockerfile-final in themulti-stage folder, but\\nyou can see the only change is splitting the finalprod stage into two stages — one for\\nthe client build and the other for the server build. With a Dockerfile like this, you tell a\\ndocker build command which of the two final stages to target for the build.\\nLet’s do it.\\nRun the following two commands to create two different images from the same\\nDockerfile-final file. Both commands use the-f flag to tell Docker to use the\\nDockerfile-final file. They also use the--target flag to tell the builder which stage\\nto build from.\\n$ docker build -t multi:client --target prod-client -f Dockerfile-final .\\n<Snip>\\n$ docker build -t multi:server --target prod-server -f Dockerfile-final .\\n<Snip>\\nCheck the builds and image sizes.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nmulti full a7a01440f2b5 4 hours ago 25.2MB\\nmulti server a75778df1b9c 4 seconds ago 11.7MB\\nmulti client 02b621e9415f 37 seconds ago 11.9MB\\nYou now have three images, and theclient and server images are each about half the\\nsize of thefull image. This makes sense because thefull image contains the client and\\nserver binaries, whereas the others only include one.',\n",
              " '8: Containerizing an app 111\\nBuildx, BuildKit, drivers, and Build Cloud\\nThis section takes a quick look at the major components that power builds.\\nBehind the scenes, Docker’s build system has a client and server:\\n• Client: Buildx\\n• Server: BuildKit\\nBuildx is Docker’s latest and greatest build client. It’s implemented as a CLI plugin and\\nsupports all the latest features of BuildKit, such as multi-stage builds, multi-architecture\\nimages, advanced caching, and more. It’s been the default build client since Docker v23.0\\nand Docker Desktop v4.19.\\nYou can configure Buildx to talk to multiple BuildKit instances, and we call each\\ninstance of BuildKit abuilder. Builders can be on your local machine, in your cloud or\\ndatacenter, or Docker’sBuild Cloud.\\nIf you point buildx at a local builder, image builds will be done on your local machine.\\nIf you point it at a remote builder, such as Docker Build Cloud, builds will be done on\\nremote infrastructure.\\nFigure 8.7 shows a Docker environment configured to talk to a local and a remote\\nbuilder.\\nFigure 8.7 - Docker build architecture\\nIn the diagram, the local builder uses thedocker-container driver to create a BuildKit\\ninstance inside a dedicated container. All builds using this driver will run in the\\ndedicated container. The other option uses the cloud driver to send builds to Docker’s',\n",
              " '8: Containerizing an app 112\\nBuild Cloud service. Build Cloud offers fast builds and a shared cache but requires a\\npaid subscription.\\nWhen you run adocker build command, buildx interprets the command and sends\\nthe build request to the selectedbuilder. This includes the Dockerfile, command line\\narguments, caching options, export options, and the build context (app and dependency\\nlist). Thebuilder performs the build and exports the image, while the buildx client\\nreports on progress.\\nRun the following command to see the builders you have configured on your system.\\nI’ve trimmed the output in the book, but you can see a local and a remote builder.\\n$ docker buildx ls\\nNAME/NODE DRIVER/ENDPOINT PLATFORMS\\nbuilder * docker-container\\nbuilder0 desktop-linux linux/arm64, linux/amd64, linux/amd64/v2,\\nlinux/riscv64, linux/ppc64le, linux/s390x,\\nlinux/386, linux/mips64le, linux/mips64,\\nlinux/arm/v7, linux/arm/v6\\ncloud-nigelpoulton-ddd cloud\\nlinux-arm64 cloud://nigel...arm64 linux/arm64*\\nlinux-amd64 cloud://nigel...amd64 linux/amd64*, linux/amd64/v2,\\nlinux/amd64/v3,linux/amd64/v4\\n<Snip>\\nNotice how the first builder supports more platforms than the cloud builder. This is\\nbecause thedocker-container driver utilizes QEMU to emulate target hardware. It\\nusually works but can be slow.\\nThe second builder is Docker’s Build Cloud, which only supports AMD and ARM builds.\\nBuilds running in Build Cloud run on native hardware and offer a shared cache so that\\nteammates can share a common cache for even faster builds. Complex builds can be\\nmuch quicker when executed on native hardware such as Build Cloud.\\nRun adocker buildx inspect command against one of your builders.\\n$ docker buildx inspect cloud-nigelpoulton-ddd\\nName: cloud-nigelpoulton-ddd\\nDriver: cloud\\nNodes:\\nName: linux-arm64\\nEndpoint: cloud://nigelpoulton/ddd_linux-arm64\\nStatus: running\\nBuildkit: v0.12.5\\nPlatforms: linux/arm64*\\nLabels:\\norg.mobyproject.buildkit.worker.executor: oci',\n",
              " '8: Containerizing an app 113\\norg.mobyproject.buildkit.worker.hostname: 2d0e9f41699c\\norg.mobyproject.buildkit.worker.network: host\\norg.mobyproject.buildkit.worker.oci.process-mode: sandbox\\norg.mobyproject.buildkit.worker.selinux.enabled: false\\norg.mobyproject.buildkit.worker.snapshotter: overlayfs\\nGC Policy rule#0:\\nAll: true\\nKeep Bytes: 21.42GiB\\n<Snip>\\nLet’s see how to perform multi-architecture builds.\\nMulti-architecture builds\\nYou can use thedocker build command to build images for multiple architectures,\\nincluding ones different from your local machine. For example:\\n• Docker on an AMD machine can build ARM images\\n• Docker on an ARM machine can build AMD images\\nYou also have the option to perform build builds locally or in the cloud. Both work with\\nthe standarddocker build command and only require minimal backend configuration.\\nRun the following command to list your current builders. Remember, abuilder is an\\ninstance of BuildKit that will perform builds.\\n$ docker buildx ls\\nNAME/NODE DRIVER/ENDPOINT PLATFORMS\\nbuilder * docker-container\\nbuilder0 desktop-linux linux/arm64, linux/amd64, linux/amd64/v2,\\nlinux/riscv64, linux/ppc64le, linux/s390x,\\nlinux/386, linux/mips64le, linux/mips64,\\nlinux/arm/v7, linux/arm/v6\\ncloud-nigelpoulton-ddd cloud\\nlinux-arm64 cloud://nigel...arm64 linux/arm64*\\nlinux-amd64 cloud://nigel...amd64 linux/amd64*, linux/amd64/v2, linux/amd64/v3,\\nlinux/amd64/v4\\n<Snip>\\nThe book’s output shows two builders; the one with the asterisk (*) is the default builder.\\nIn this example, the default builder is calledbuilder and uses thedocker-container\\ndriver to perform builds inside a local build container. Unless you specify a different\\nbuilder, all builds will run inside this build container. It supports multiple architectures,\\nincluding AMD, ARM, RISC-V, s390x, and more.\\nIf you don’t already have one, create a new builder calledcontainer that uses thedocker-\\ncontainer driver with the following command.',\n",
              " '8: Containerizing an app 114\\n$ docker buildx create --driver=docker-container --name=container\\nbuilder\\nRun anotherdocker buildx ls to show the new builder. Don’t worry if it shows as\\npresent but inactive.\\nMake it the default builder.\\n$ docker buildx use container\\nChange into theweb-app directory and run the following command to build the app\\ninto AMD and ARM images and export them directly to Docker Hub.\\nBe sure to substitute your Docker ID as the command pushes directly to Docker Hub\\nand will fail if you try to push it to my repositories. If you don’t have a Docker Hub\\naccount or don’t want to push the images, you can replace the--push with --load.\\n$ docker buildx build --builder=container \\\\\\n--platform=linux/amd64,linux/arm64 \\\\\\n-t nigelpoulton/ddd-book:ch8.1 --push .\\n[+] Building 79.3s (26/26) FINISHED\\n<Snip>\\n=> [linux/arm64 2/5] RUN apk add --update nodejs npm curl 19.0s\\n=> [linux/amd64 2/5] RUN apk add --update nodejs npm curl 17.4s\\n=> [linux/amd64 3/5] COPY . /src 0.0s\\n=> [linux/amd64 4/5] WORKDIR /src 0.0s\\n=> [linux/amd64 5/5] RUN npm install 7.3s\\n=> [linux/arm64 3/5] COPY . /src 0.0s\\n=> [linux/arm64 4/5] WORKDIR /src 0.0s\\n=> [linux/arm64 5/5] RUN npm install 5.6s\\n=> exporting to image\\n<Snip>\\n=> => pushing layers 31.5s\\n=> => pushing manifest for docker.io/nigelpoulton/ddd-book:web0.2@sha256:8fc61... 3.6s\\n=> [auth] nigelpoulton/ddd-book:pull,push token for registry-1.docker.io 0.0s\\nI’ve snipped the output, but you can still see two important things:\\n• Each Dockerfile instruction was executed twice — once for AMD and once for\\nARM\\n• The last few lines show the image layers being pushed directly to Docker Hub\\nNow that you’ve performed a build, the builder will show asactive and list the architec-\\ntures it supports.',\n",
              " '8: Containerizing an app 115\\nFigure 8.8 shows how the images for both architectures appear on Docker Hub under\\nthe same repository and tag.\\nFigure 8.8 - Multi-platform image\\nYou can also perform the builds using Docker Build Cloud. This is a cloud-based service\\nthat offers fast builds and lets you share your build cache with teammates. It requires a\\npaid subscription.\\nIf you have a Docker subscription that grants you access to Build Cloud, you can go to\\nbuild.docker.com and configure your first cloud builder. You can also create cloud\\nbuilders from the CLI as follows. If you’re following along, you’ll need to give yours a\\ndifferent name.\\n$ docker buildx create --driver cloud nigelpoulton/ddd\\ncloud-nigelpoulton-ddd\\nOnce you have a cloud builder, you can either make it your default builder with a\\ndocker buildx use <builder> command, or you can specify it when performing\\nindividual builds.\\nThe following command uses the--builder flag to use thecloud-nigelpoulton-ddd\\ncloud builder to build the same images as in the previous steps. Remember to use your\\nown cloud builder if you’re following along.\\n$ docker buildx build \\\\\\n--builder=cloud-nigelpoulton-ddd \\\\\\n--platform=linux/amd64,linux/arm64 \\\\\\n-t nigelpoulton/ddd-book:ch8.1 --push .\\n=> [internal] connected to docker build cloud service 0.0s\\n<Snip>\\nAt the time of writing, Build Cloud supports various AMD and ARM architectures,\\nwhereas thedocker-container driver supports more but is slower and less reliable.',\n",
              " '8: Containerizing an app 116\\nA few good practices\\nLet’s finish the chapter with a few best practices. This isn’t a full list, and the advice\\napplies to local and cloud builds.\\nLeverage the build cache\\nBuildKit uses a cache to speed up builds. The best way to see the impact is to build a\\nnew image on a clean Docker host and then repeat the same build immediately after.\\nThe first build will pull images and take time to build layers. The second build will\\ninstantly complete because the layers and other artifacts from the first build are cached\\nand leveraged by later builds.\\nIf you use a local builder, the cache is only available to other builds on the same system.\\nHowever, your entire team can share the cache on Docker Build Cloud.\\nFor each build, the builder iterates through the Dockerfile one line at a time, starting\\nfrom the top. For each line, it checks if it already has the layer in its cache. If it does,\\na cache hitoccurs, and it uses the cached layer. If it doesn’t, acache missoccurs, and it\\nbuilds a new layer from the instruction. Cache hits are one of the best ways to make\\nbuilds faster.\\nLet’s take a closer look.\\nAssume the following Dockerfile:\\nFROM alpine\\nRUN apk add --update nodejs npm\\nCOPY . /src\\nWORKDIR /src\\nRUN npm install\\nEXPOSE 8080\\nENTRYPOINT [\"node\", \"./app.js\"]\\nThe first instruction tells Docker to use thealpine:latest image as itsbase image. If\\nyou already have a copy of this image, the builder moves on to the next instruction. If\\nyou don’t have a copy, it pulls it from Docker Hub.\\nThe next instruction (RUN apk...) runs a command to update package lists and install\\nnodejs and npm. Before executing the instruction, Docker checks the build cache for a\\nlayer built from the same base image using the same instruction. In this case, it’s looking\\nfor a layer built by executing theRUN apk add --update nodejs npm instruction\\ndirectly on top of thealpine:latest image.\\nIf it finds a matching layer, it links to that layer and continues the build with the cache\\nintact. If it doesnot find a matching layer, it invalidates the cache and builds the layer.',\n",
              " '8: Containerizing an app 117\\nInvalidating the cache means the builder has to execute all remaining Dockerfile\\ninstructions in full and cannot use the cache.\\nLet’s assume Docker had a cached layer for theRUN instruction and that the layer’s ID is\\nAAA.\\nThe next instruction runs aCOPY . /src command to copy the app code into the image.\\nThe previous instruction scored a cache hit, meaning Docker can check if it has a cached\\nlayer built by running aCOPY . /src against theAAA layer. If it has a cached layer for\\nthis, it links to the layer and proceeds to the next instruction. If it doesn’t have a cached\\nlayer, it builds it and invalidates the cache for the rest of the build.\\nThis process continues for the rest of the Dockerfile.\\nIt’s important to understand a few things.\\nAny time an instruction results in a cache miss, the cache is invalidated and no longer\\nchecked for the rest of the build. This means you should write your Dockerfiles so that\\ninstructions most likely to invalidate the cache go near the end of the Dockerfile. This\\nallows builds to benefit from the cache for as long as possible.\\nYou can force a build to ignore the cache by runningdocker build with the--no-cache\\noption.\\nIt’s also important to understand thatCOPY and ADD instructions include logic to ensure\\nthe content you’re copying into the image hasn’t changed since the last build. For exam-\\nple, you might have a cached layer that Docker built by running aCOPY . /src against\\nthe AAA image. However, if thefiles that theCOPY . /src instruction copies into the\\nlayer have changed since the cached layer was built, you cannot use the cached layer as\\nyou’d get old versions of the files. To protect against this, Docker performs checksums\\nagainst each file it copies. If the checksums don’t match, the cache is invalidated, and\\nDocker builds a new layer.\\nOnly install essential packages\\nWe often joke that weinstall the entire internetwhen we build apps. As a quick example,\\nthe simple Node.js app used earlier in the chapter depends on two packages:\\n• Express\\n• Pug\\nHowever, these packages depend on other packages, which in turn depend on others.\\nAt the time of writing, building this simple application with two dependencies actually\\ndownloads 97 packages!\\nFortunately, some package managers provide a way for you to only download and install\\nessential packages instead ofthe entire internet. One example is theapt package manager',\n",
              " '8: Containerizing an app 118\\nthat lets you specify theno-install-recommends flag so that it only installs packages\\nin thedepends field and not everyrecommended and suggested package. Each package\\nmanager does this differently, but it’s worth investigating as it can massively impact the\\nsize of your images.\\nClean up\\nIf you’ve followed along, you’ll have one running container and several images in your\\nlocal image repository. You should delete the running container and, optionally, the\\nlocal images.\\nRun the following command to delete the container.\\n$ docker rm c1 -f\\nOptionally delete the local images with the following command. Be sure to use the\\nnames of the images in your environment.\\n$ docker rmi \\\\\\nmulti:full multi:client multi:server ddd-book:ch8.node nigelpoulton/ddd-book:ch8.node\\nContainerizing an app – The commands\\n• docker build containerizes applications. It reads a Dockerfile and follows the\\ninstructions to create an OCI image. The-t flag tags the image, and the-f flag\\nlets you specify the name and location of the Dockerfile. Thebuild contextis where\\nyour application files exist and can be a directory on your local Docker host or a\\nremote Git repo.\\n• The DockerfileFROM instruction specifies the base image. It’s usually the first\\ninstruction in a Dockerfile, and it’s considered a good practice to build from\\nDocker Official Imagesor images fromVerified Publishers. FROM is also used to\\nidentify new build stages in multi-stage builds.\\n• The DockerfileRUN instruction lets you run commands during a build. It’s com-\\nmonly used to update packages and install dependencies. EveryRUN instruction\\ncreates a new image layer.\\n• The DockerfileCOPY instruction adds files to images, and you’ll regularly use it to\\ncopy your application code into a new image. EveryCOPY instruction creates an\\nimage layer.\\n• The DockerfileEXPOSE instruction documents an application’s network port.',\n",
              " '8: Containerizing an app 119\\n• The DockerfileENTRYPOINT and CMD instructions tell Docker how to run the app\\nwhen starting a new container.\\n• Some other Dockerfile instructions includeLABEL, ENV, ONBUILD, HEALTHCHECK and\\nmore.\\nChapter summary\\nThis chapter taught you how to containerize an application. This is the process of\\nbuilding an app into a container image and running it as a container.\\nYou pulled some application source code from GitHub and used thedocker init\\ncommand to auto-generate a Dockerfile with instructions telling Docker how to build\\nthe app into a container image. You then useddocker build to create the image,docker\\npush to push it to Docker Hub, anddocker run to run it as a container.\\nAlong the way, you learned that Dockerfile instructions that add content to an image\\ncreate new layers, whereas instructions that don’t add content only add metadata.\\nAfter that, you learned how multi-stage builds allow you to create small and efficient\\nproduction images without the bloat carried over from compiling the app, etc.\\nAfter that, you learned that buildx is the default build client that integrates with the\\nlatest features of the BuildKit build engine. You learned how to create local and remote\\nbuilders (BuildKit instances) and how to use them to perform multi-architecture builds.\\nYou also learned the importance of the build cache for speeding up builds and how to\\noptimize Dockerfiles to leverage the build cache.',\n",
              " '9: Multi-container apps with Compose\\nIn this chapter, you’ll deploy and manage a multi-container application using Docker\\nCompose. When we talk about Docker Compose, we usually shorten it toCompose and\\nalways write it with a capital “C”.\\nI’ve organized the chapter as follows:\\n• Docker Compose – The TLDR\\n• Compose background\\n• Installing Compose\\n• The sample app\\n• Compose files\\n• Deploying apps with Compose\\n• Managing apps with Compose\\nDocker Compose – The TLDR\\nWe create modern cloud-native applications by combining lots of small services that\\nwork together to form a useful app. We call themmicroservices applications, and they\\nbring a lot of benefits, such as self-healing, autoscaling, and rolling updates. However,\\nthey can be complex.\\nFor example, you might have a microservices app with the following services:\\n• Web front-end\\n• Ordering\\n• Catalog\\n• Back-end datastore\\n• Logging\\n• Authentication\\n• Authorization',\n",
              " '9: Multi-container apps with Compose 121\\nInstead of hacking together complex scripts and longdocker commands, Compose lets\\nyou describe the application in a simple YAML file called aCompose file. You then use the\\nCompose file with thedocker compose command to deploy and manage the app.\\nYou should keep your Compose files in a version control system such as Git.\\nThat’s the basics. Let’s dig deeper.\\nCompose background\\nWhen Docker was new, a company calledOrchard Labsbuilt a tool calledFig that made\\ndeploying and managing multi-container apps easy. It was a Python tool that ran on\\ntop of Docker and let you define complex multi-container microservices apps in a\\nsimple YAML file. You could even use thefig command-line tool to manage the entire\\napplication lifecycle.\\nBehind the scenes, Fig would read the YAML file and call the appropriate Docker\\ncommands to deploy and manage the app.\\nFig was so good that Docker, Inc. acquired Orchard Labs and rebranded Fig asDocker\\nCompose. They renamed the command-line tool fromfig to docker-compose, and then\\nmore recently, they folded it into thedocker CLI with its owncompose sub-command.\\nYou can now run simpledocker compose commands to easily manage multi-container\\nmicroservices apps.\\nThere is also aCompose Specification15 driving Compose as an open standard for\\ndefining multi-container microservices apps. The specification is community-led and\\nkept separate from the Docker implementation to maintain better governance and\\nclearer demarcation. However,Docker Composeis thereference implementation, and you\\nshould expect Docker to implement the full spec.\\nReading the spec is also a great way to learn the details.\\nInstalling Compose\\nAll modern versions of Docker come with Docker Compose pre-installed, and you no\\nlonger need to install it as a separate application.\\nTest it with the following command. Be sure to use thedocker compose command\\ninstead of the olderdocker-compose.\\n15https://www.compose-spec.io/',\n",
              " \"9: Multi-container apps with Compose 122\\n$ docker compose version\\nDocker Compose version v2.25.0\\nThe sample app\\nWe’ll use the sample app shown in Figure 9.1 with two services, a network, and a\\nvolume.\\nFigure9.1 - Sample app\\nThe web-fe service runs a web server that increments a counter in theredis service\\nevery time it receives a request for the web page. Both services are connected to the\\ncounter-net network and use it to communicate with each other. Theweb-fe service\\nmounts thecounter-vol volume.\\nThis is all defined in thecompose.yaml file in themulti-container folder of the book’s\\nGitHub repo.\\nIf you haven’t already done so, clone the repo so you have a local copy of everything\\nyou’ll need. You’ll needgit installed, and the command will create a new directory\\ncalled ddd-book.\\n$ git clone https://github.com/nigelpoulton/ddd-book.git\\nCloning into 'ddd-book'...\\nremote: Enumerating objects: 67, done.\\nremote: Counting objects: 100% (67/67), done.\\nremote: Compressing objects: 100% (47/47), done.\\nremote: Total 67 (delta 17), reused 63 (delta 16), pack-reused 0\\nReceiving objects: 100% (67/67), 173.61 KiB | 1.83 MiB/s, done.\\nResolving deltas: 100% (17/17), done.\\nChange into theddd-book/multi-container directory and list its contents.\",\n",
              " '9: Multi-container apps with Compose 123\\n$ cd ddd-book/multi-container/\\n$ ls -l\\ntotal 20\\ndrwxrwxr-x 4 ubuntu ubuntu 4096 May 21 15:53 app\\n-rw-rw-r-- 1 ubuntu ubuntu 288 May 21 15:53 Dockerfile\\n-rw-rw-r-- 1 ubuntu ubuntu 18 May 21 15:53 requirements.txt\\n-rw-rw-r-- 1 ubuntu ubuntu 355 May 21 15:53 compose.yaml\\n-rw-rw-r-- 1 ubuntu ubuntu 332 May 21 15:53 README.md\\nThis directory is yourbuild contextand contains all the app code and configuration files\\nneeded to deploy and manage the app.\\n• The app folder contains the application code, views, and templates\\n• The Dockerfile describes how to build the image for theweb-fe service\\n• The requirements.txt file lists the application dependencies\\n• The compose.yaml file is the Compose file that describes how the app works\\nFigure 9.2 is slightly more complex and shows how the files in your build context relate\\nto the app.\\nFigure 9.2 - Detailed view of sample app\\nWhen you deploy the app, you’ll use thedocker compose command to send the\\ncompose.yaml file to Docker. Docker will create thecounter-net network and the\\ncounter-vol volume and use the Dockerfile to build an OCI image for theweb-fe\\nservice. The Dockerfile tells Docker to copy the code from theapp folder into the image.',\n",
              " '9: Multi-container apps with Compose 124\\nDocker then starts theweb-fe, mounts thecounter-vol volume, and connects to the\\ncounter-net network. It also starts a container for theredis service and connects that\\nto the samecounter-net network.\\nDocker also uses the name of the build context directory to name the services, network,\\nand volume so that you can easily identify them.\\nNow that you know what the app looks like, let’s look at the Compose file.\\nCompose files\\nCompose uses YAML files to define microservices applications. We call themCompose\\nfiles, and Compose expects you to name themcompose.yaml or compose.yml. However,\\nyou can use the-f flag with thedocker compose command to specify files with\\narbitrary names.\\nHere is the Compose file we’ll be using. It’s calledcompose.yaml and is in themulti-\\ncontainer folder.\\nservices: <<---- Microservices are defined in the \"services\" block\\nweb-fe: <<---- This block defines the web front-end microservice\\ndeploy:\\nreplicas: 1\\nbuild: .\\ncommand: python app.py\\nports:\\n- target: 8080\\npublished: 5001\\nnetworks:\\n- counter-net\\nvolumes:\\n- type: volume\\nsource: counter-vol\\ntarget: /app\\nredis: <<---- This block defines the Redis back-end microservice\\ndeploy:\\nreplicas: 1\\nimage: \"redis:alpine\"\\nnetworks:\\ncounter-net:\\nnetworks: <<---- Networks are defined in this block\\ncounter-net:\\nvolumes: <<---- Volumes are defined in this block\\ncounter-vol:\\nThe first thing to note is that the file has three top-level keys with a block of code\\nbeneath each:',\n",
              " '9: Multi-container apps with Compose 125\\n• services\\n• networks\\n• volumes\\nMore top-level keys exist, but this app only uses the three in the list.\\nLet’s have a closer look at each.\\nThe top-levelservices key is mandatory and is where you define application microser-\\nvices. This app has two microservices calledweb-fe and redis.\\nLet’s look at both, starting with theweb-fe service.\\nservices:\\nweb-fe: <<---- Service name. Containers will inherit this name\\ndeploy:\\nreplicas: 1 <<---- Deploy a single container for this service\\nbuild: . <<---- Build from the Dockerfile in the same directory\\ncommand: python app.py <<---- Execute this command when starting containers\\nports:\\n- target: 8080 <<---- Map port 8080 in the container...\\npublished: 5001 <<---- ...to port 5001 on the Docker host\\nnetworks:\\n- counter-net <<---- Attach containers to the \"counter-net\" network\\nvolumes:\\n- type: volume\\nsource: counter-vol <<---- Mount the \"counter-vol\" volume...\\ntarget: /app <<---- ...to \"/app\" in the containers for this service\\nLet’s step through it.\\n• web-fe: is the service’s name, and all containers created as part of this service will\\nhave “web-fe” in their names.\\n• deploy.replicas: 1 tells Docker to deploy a single container for this service. You\\ncan specify a different number of replicas to deploy multiple identical containers\\nfor the service. However, this won’t work on Docker Desktop installations as you\\nonly have a single Docker host, and only one container can use port5001 on the\\nDocker Desktop host. You’ll learn how to deploy multiple replicas for this service\\nin theDeploying apps with Docker stackschapter.\\n• build: . tells Docker to build the image for this service’s container from the\\nDockerfile in the current directory.\\n• command: python app.py is the command Docker will execute inside every\\ncontainer it creates for this service. Theapp.py file must exist in the image, and\\nthe image must have Python installed. The Dockerfile takes care of both of these\\nrequirements.',\n",
              " '9: Multi-container apps with Compose 126\\n• ports: is where you map network ports from the service’s containers to the\\nDocker host. This example maps port5001 on the Docker host to port8080 inside\\nthe container.\\n• networks: tells Docker to attach this service’s containers to thecounter-net\\nnetwork. The network should already exist or be defined in thenetworks top-level\\nkey.\\n• volumes: tells Docker to mount thecounter-vol volume to the/app directory\\ninside all of the service’s containers. The volume should already exist or be defined\\nin thevolumes top-level key.\\nIn summary, Compose will instruct Docker to deploy a single container for theweb-fe\\nservice. It will build a new image from the Dockerfile in the same directory and start\\nthe container from that image. When it starts, the container will haveweb-fe in its\\nname and run thepython app.py command. It will attach to thecounter-net network,\\nexpose the web service on the host’s port5001, and mount thecounter-vol volume to\\n/app. If you defined multiple replicas (containers) they would all be identical.\\nNote: You don’t need to specify thecommand: python app.py option in the\\nCompose file as it’s already defined in the Dockerfile. However, I’ve shown\\nit here so you know how it works. You can also use Compose to override\\ninstructions set in Dockerfiles.\\nThe redis service is a lot simpler.\\nservices:\\n..\\nredis: <<---- Service name. Containers will inherit this name\\ndeploy:\\nreplicas: 1 <<---- Deploy a single container for this service\\nimage: \"redis:alpine\" <<---- Pull the \"redis:alpine\" image for this service\\nnetworks:\\ncounter-net: <<---- Attach containers to the \"counter-net\" network\\nLet’s step through this one.\\n• redis: is the service’s name, and all containers created as part of this service will\\ninherit “redis” as part of their names.\\n• deploy.replicas: 1 tells Docker to deploy a single container for this service.\\n• image: redis:alpine tells Docker to pull theredis:alpine image from Docker\\nHub and use it to start the service’s containers.\\n• networks: tells Docker to attach the service’s containers to thecounter-net\\nnetwork.',\n",
              " '9: Multi-container apps with Compose 127\\nConnecting both services to thecounter-net network means they can resolve each\\nother by name and communicate. This is important, as the following extract from the\\napp.py file shows the web app communicating with theredis service by name.\\nimport time\\nimport redis\\nfrom flask import Flask, render_template\\napp = Flask(__name__)\\ncache = redis.Redis(host=\\'redis\\', port=6379) <<---- \"redis\" is the name of the service\\n<Snip>\\nThe network and volumes blocks are extremely simple and define a network called\\ncounter-net and a volume calledcounter-vol.\\nnetworks: <<---- This block defines a new network called \"counter-net\"\\ncounter-net:\\nvolumes: <<---- This block defines a new volume called \"counter-vol\"\\ncounter-vol:\\nNow that we understand how the Compose file works let’s deploy the app.\\nDeploying apps with Compose\\nIn this section, you’ll use the Compose file to deploy the app. You’ll need a local copy\\nof the book’s GitHub repo, and you’ll need to run all commands from themulti-\\ncontainer folder.\\nRun the following command to deploy the app. By default, it deploys the app defined in\\nthe compose.yaml file in the working directory.\\n$ docker compose up &\\n- redis 7 layers [||||||] 0B/0B Pulled 5.2s\\n- b0dd12c8e070: Pull complete\\n<Snip>\\n- 4f4fb700ef54: Pull complete\\n- redis Pulled\\n<Snip>\\n=> [web-fe internal] load build definition from Dockerfile\\n[+] Building 10.3s (9/9) FINISHED\\n<Snip>\\n[+] Running 4/4\\n- Network multi-container_counter-net Created 0.0s\\n- Volume \"multi-container_counter-vol\" Created 0.0s\\n- Container multi-container-redis-1 Created 0.1s\\n- Container multi-container-web-fe-1 Created 0.1s',\n",
              " '9: Multi-container apps with Compose 128\\nIt’ll take a few seconds to build and pull the images and then start the app. Once it’s\\nrunning, you’ll have to hitReturn to reclaim your shell prompt.\\nWe’ll review what happened in a second, but let’s talk about thedocker compose\\ncommand first.\\nRunning adocker compose up command is the most common way to deploy a\\nCompose app. It reads through the Compose file in the local directory and runs the\\nDocker commands required to deploy the app. This includes building and pulling all\\nthe necessary images, creating all required networks and volumes, and starting all\\ncontainers.\\nThe command you executed didn’t specify the name or location of the Compose file, so\\nDocker assumed it was calledcompose.yaml in the local directory. However, you can use\\nthe -f flag to point to a Compose file with a different name in a different directory. For\\nexample, the following command will deploy the application defined in a Compose file\\ncalled sample-app.yml in theapps/ddd-book directory.\\n$ docker compose -f apps/ddd-book/sample-app.yml up &\\nYou’ll normally use the--detach flag to bring the app up in the background, and we’ll\\nsee that later. However, bringing it up in the foreground as we did will print informative\\nmessages to your terminal that we’ll refer to later.\\nNow that you’ve deployed the app, you can run regulardocker commands to see\\nthe images, containers, networks, and volumes that Compose created — remember,\\nCompose is building and working with regular Docker constructs behind the scenes.\\nRun the following command to see the imagecreated for theweb-fe service and the\\nimage pulled for theredis service.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nredis alpine 7635b0bfdd7d 2 months ago 61.4MB\\nmulti-container-web-fe latest f8fc9490e335 24 minutes ago 113MB\\nDocker pulled theredis:alpine image from Docker Hub, but it used the Dockerfile to\\nbuild themulti-container-web-fe:latest image.\\nIf you look at the Dockerfile, you’ll see it pulls thepython:alpine image, copies in the\\napp code, installs requirements, and sets the command to start the app.',\n",
              " '9: Multi-container apps with Compose 129\\nFROM python:alpine <<---- Base image\\nCOPY . /app <<---- Copy app code into image\\nWORKDIR /app <<---- Set working directory\\nRUN pip install -r requirements.txt <<---- Install requirements\\nENTRYPOINT [\"python\", \"app.py\"] <<---- Set the default app\\nNotice how the newly built image’s name is a combination of the project name and the\\nservice name. The project name is the name of the build context directory, which in our\\nexample ismulti-container, and the service name isweb-fe. Compose uses this format\\nto name all resources, and the following table shows the names it will give the resources\\nfor our sample app.\\nResource type Resource Name\\nService web-fe multi-container-web-fe-1\\nService redis multi-container-redis-1\\nNetwork counter-net multi-container_counter-net\\nVolume counter-vol multi-container_counter-vol\\nList running containers to see the containers Compose created for the app.\\n$ docker ps\\nID COMMAND STATUS PORTS NAMES\\n61.. \"python app/app.py\" Up 35 mins 0.0.0.0:5001->8080/tcp.. multi-container-web-fe-1\\n80.. \"docker-entrypoint..\" Up 35 mins 6379/tcp multi-container-redis-1\\nAs you can see, themulti-container-web-fe-1 container is running the Python web\\napp and is mapped to port5001 on all interfaces on the Docker host. We’ll connect to\\nthis later.\\nThe number at the end of the container names allows each service to have multiple\\nreplicas. For example, if theweb-fe service had three replicas they would be called\\nmulti-container-web-fe-1, multi-container-web-fe-2, andmulti-container-web-\\nfe-3.\\nRun the following commands to see thecounter-net network andcounter-vol\\nvolume.',\n",
              " '9: Multi-container apps with Compose 130\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\n46100cae7441 multi-container_counter-net bridge local\\n<Snip>\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal multi-container_counter-vol\\n<Snip>\\nWith the application deployed, you can point a web browser at your Docker host on port\\n5001 to view it. You can connect tolocalhost:5001 if you’re running Docker Desktop.\\nRefreshing the page will cause the counter to increment. This is because the app counts\\npage hits and stores the counter in the Redis service.\\nIf you followed along and brought the application up in the foreground, you’ll see HTTP\\nresponse codes in your terminal for every page refresh. You won’t see this if you used\\nthe --detach flag to bring it up in the background.\\nweb-fe-1 | 192.168.65.1 - [31/Mar/2024 19:15:33] \"GET / HTTP/1.1\" 200 -\\nweb-fe-1 | 192.168.65.1 - [31/Mar/2024 19:15:33] \"GET /static/images/image.png HTTP/1.1\" 304\\nweb-fe-1 | 192.168.65.1 - [31/Mar/2024 19:15:33] \"GET /static/css/main.css HTTP/1.1\" 304\\nCongratulations. You’ve successfully deployed a multi-container application using\\nDocker Compose!\\nManaging apps with Compose\\nIn this section, you’ll see how to stop, restart, delete, and get the status of Compose apps.\\nRun the following command to shut down the app. If you started it in the foreground,\\nyou’ll see messages requesting the app to gracefully quit.',\n",
              " '9: Multi-container apps with Compose 131\\n$ docker compose down\\nredis-1 | 1:signal-handler (1711912860) Received SIGTERM scheduling shutdown...\\nredis-1 | 1:M 31 Mar 2024 19:21:00.406 * User requested shutdown...\\nredis-1 | 1:M 31 Mar 2024 19:21:00.406 * Saving the final RDB snapshot before exiting.\\nredis-1 | 1:M 31 Mar 2024 19:21:00.409 * DB saved on disk\\nredis-1 | 1:M 31 Mar 2024 19:21:00.409 # Redis is now ready to exit, bye bye...\\n<Snip>\\n- Container multi-container-redis-1 Removed 0.1s\\n- Container multi-container-web-fe-1 Removed 0.2s\\n- Container multi-container-redis-1 Removed 0.1s\\n- Network multi-container_counter-net Removed 0.1s\\nThe output shows Docker removing both containers and the network. However, it\\ndoesn’t mention the volume.\\nRun adocker volumes ls command to see if the volume still exists.\\n$ docker volume ls\\n<Snip>\\nlocal multi-container_counter-vol\\nThe volume still exists, including the counter data stored in it. This is because Docker\\nknows that we store important information in volumes and might not want to delete\\nthem with other application resources. With this in mind, Docker decouples the lifecycle\\nof volumes from the rest of the application.\\nDocker also keeps the images it built and pulled when it started the app. This makes\\nfuture deployments faster.\\nFeel free to run adocker images command to verify the images still exist.\\nLet’s explore a few otherdocker compose sub-commands.\\nRun the following command to redeploy the app.\\n$ docker compose up --detach\\n<Snip>\\n[+] Running 2/3\\n- Network multi-container_counter-net Created 0.2s\\n- Container multi-container-redis-1 Started 0.2s\\n- Container multi-container-web-fe-1 Started 0.2s\\nNotice how it started much faster this time. This is because the volume and both images\\nalready exist.\\nCheck the current state of the app with thedocker compose ps command.',\n",
              " '9: Multi-container apps with Compose 132\\n$ docker compose ps\\nNAME COMMAND SERVICE STATUS PORTS\\nmulti-container-redis-1 \"docker-entrypoint..\" redis Up 33 sec 6379/tcp\\nmulti-container-web-fe-1 \"python app/app.py\" web-fe Up 33 sec 0.0.0.0:5001->8080\\nThe output shows both containers, the commands they’re executing, their current state,\\nand the network ports they’re listening on.\\nRun adocker compose top to list the processes inside each container.\\n$ docker compose top\\nmulti-container-redis-1\\nUID PID PPID ... CMD\\nlxd 12023 11980 redis-server *:6379\\nmulti-container-web-fe-1\\nUID PID PPID ... CMD\\nroot 12024 12002 0 python app/app.py python app.py\\nroot 12085 12024 0 /usr/local/bin/python app/app.py python app.py\\nThe PID numbers returned are the PID numbers as seen from the Docker host (not from\\nwithin the containers).\\nYou’re about to stop and restart the app. However, before doing this, connect your\\nbrowser to the app again and refresh the page a few times. Make a note of the counter\\nvalue before continuing.\\nRun the following commands to stop the app and recheck its status.\\n$ docker compose stop\\n[+] Running 2/2\\n- Container multi-container-redis-1 Stopped 0.4s\\n- Container multi-container-web-fe-1 Stopped 0.5\\n$ docker compose ps\\nNAME COMMAND SERVICE STATUS PORTS\\nThe app is down, but Docker hasn’t deleted any of its resources. Verify this by checking\\nif the containers still exist.\\n$ docker compose ps -a\\nNAME IMAGE ... SERVICE STATUS\\nmulti-container-redis-1 redis:alpine ... redis Exited (0) 18 seconds ago\\nmulti-container-web-fe-1 multi-container-web-fe ... web-fe Exited (0) 18 seconds ago\\nBoth containers still exist in theExited state.\\nRestart the app with thedocker compose restart command.',\n",
              " '9: Multi-container apps with Compose 133\\n$ docker compose restart\\n[+] Running 2/2\\n- Container multi-container-redis-1 Started 0.4s\\n- Container multi-container-web-fe-1 Started 0.5s\\nVerify the operation worked.\\n$ docker compose ls\\nNAME STATUS CONFIG FILES\\nmulti-container running(2) /Users/nigelpoulton/temp/ddd-book/multi-container/compose.yaml\\nGo back to your browser and refresh the page again. Notice how the counter picks up\\nfrom where you left it. This is because the Redis container persisted the count value\\nacross the stop and restart operations.\\nCongratulations. You’ve deployed and managed a multi-container microservices app\\nusing Docker Compose.\\nBefore cleaning up and reviewing the commands, it’s important to understand that\\nthis was a simple example and that Docker Compose can deploy and manage far more\\ncomplex applications.\\nClean up\\nRun the following command tostop and deletethe app. The--volumes flag will delete\\nall of the app’s volumes, and the--rmi all will delete all of its images.\\n$ docker-compose down --volumes --rmi all\\n- Container multi-container-web-fe-1 Removed 0.2s\\n- Container multi-container-redis-1 Removed 0.1s\\n- Volume multi-container_counter-vol Removed 0.0s\\n- Image multi-container-web-fe:latest Removed 0.1s\\n- Image redis:alpine Removed 0.1s\\n- Network multi-container_counter-net Removed 0.1s\\nDeploying apps with Compose – The commands\\n• docker compose up is the command to deploy a Compose app. It creates all\\nimages, containers, networks, and volumes the app needs. It expects you to call\\nthe Compose filecompose.yaml, but you can specify a custom filename with the-f\\nflag. You’ll normally start the app in the background with the--detach flag.',\n",
              " '9: Multi-container apps with Compose 134\\n• docker compose stop will stop all containers in a Compose app without deleting\\nthem from the system. You can easily restart them withdocker compose restart,\\nand you shouldn’t lose any data.\\n• docker compose restart will restart a stopped Compose app. If you make\\nchanges to the Compose file while it’s stopped, these changes willnot appear in\\nthe restarted app. You need to redeploy the app to see any changes you made in the\\nCompose file.\\n• docker compose ps lists each container in the Compose app. It shows the current\\nstate, the command each container is running, and network ports.\\n• docker compose down will stop and delete a running Compose app. By default, it\\ndeletes containers and networks but not volumes and images.\\nChapter Summary\\nIn this chapter, you learned how to deploy and manage multi-container applications\\nusing Docker Compose.\\nCompose is now fully integrated into the Docker Engine and has its owndocker\\ncompose sub-command. It lets you define multi-container applications in declarative\\nconfiguration files and deploy them with a single command.\\nCompose files define all the containers, networks, volumes, secrets, and other configu-\\nrations an application needs. You then use thedocker compose command to post the\\nCompose file to Docker, and Docker deploys it.\\nOnce you’ve deployed the app, you can manage its entire lifecycle usingdocker\\ncompose sub-commands.\\nDocker Compose is popular with developers, and the Compose file is an excellent\\nsource of application documentation — it defines all the services that make up the app,\\nthe images they use, the ports they expose, the networks and volumes they use, and\\nmuch more. As such, it can help bridge the gap between development and operations\\nteams. You should also treat Compose files as code and store them in version control\\nsystems.',\n",
              " '10: Docker Swarm\\nNow that you know how to install Docker, pull images, and work with containers, the\\nnext logical step is to do it all at scale. That’s where Docker Swarm comes into play.\\nI’ve split the chapter into the following main parts:\\n• Docker Swarm – The TLDR\\n• Swarm primer\\n• Build a secure Swarm cluster\\n• Deploy and manage an app on swarm\\n• Service logs\\n• The commands\\nDocker Swarm – The TLDR\\nDocker Swarm is two things:\\n1. An enterprise-grade cluster of Docker nodes\\n2. An orchestrator of microservices apps\\nOn theclustering front, Swarm groups one or more Docker nodes into a cluster. Out\\nof the box, you get an encrypted distributed cluster store, encrypted networks, mutual\\nTLS, secure cluster join tokens, and a PKI that makes managing and rotating certificates\\na breeze. You can even add and remove nodes non-disruptively. We call these clusters\\n“swarms”.\\nOn theorchestration front, Swarm makes deploying and managing complex microser-\\nvices apps easy. You define applications declaratively in Compose files and use simple\\nDocker commands to deploy them to theswarm. You can even perform rolling updates,\\nrollbacks, and scaling operations.\\nDocker Swarm is similar to Kubernetes — both are clusters that run and manage con-\\ntainerized applications. Kubernetes is more popular and has a more active community\\nand ecosystem. However, Swarm is easier to use and is a popular choice for small-to-\\nmedium businesses and smaller application deployments. Learning Swarm also prepares\\nyou to learn and work with Kubernetes. In fact, if you plan on learning Kubernetes, you\\nshould check out my books,Quick Start Kubernetesand The Kubernetes Book.',\n",
              " '10: Docker Swarm 136\\nSwarm primer\\nOn the clustering front, aswarm is one or more Docker nodes that can be physical\\nservers, VMs, cloud instances, Raspberry Pi’s, and more. The only requirement is that\\nthey all run Docker and can communicate over reliable networks.\\nTerminology: When referring to theDocker Swarmtechnology, we’ll write\\nSwarm with an uppercase “S”. When referring to a swarm as a cluster of\\nnodes, we’ll use a lowercase “s”.\\nEvery node in a swarm is either amanager or aworker:\\n• Managers run thecontrol planeservices that maintain the state of the cluster and\\nschedule user applications to workers\\n• Workers run user applications\\nSwarm runs user applications on managersand workers in a default installation.\\nHowever, you can force user applications to run on worker nodes on important clusters,\\nallowing your managers to focus on cluster management operations.\\nThe swarm stores its state and configuration in an in-memory distributed database that\\nreplicates across all manager nodes. Fortunately, it’s automatically configured when you\\ncreate the swarm and takes care of itself.\\nSwarm uses TLS to encrypt communications, authenticate nodes, and authorize roles\\n(managers and workers). It also configures and performs automatic key rotation. And\\nas with the cluster store, it’s automatically configured when you create the swarm and\\ntakes care of itself.\\nFigure 10.1 shows a high-level view of a swarm with three managers, three workers, and\\na distributed cluster store replicated across all three managers.',\n",
              " '10: Docker Swarm 137\\nFigure 10.1 High-level swarm\\nOn the orchestration front, Swarm makes it easy to deploy and manage containerized\\napplications. It balances workloads across the cluster and adds cloud-native capabilities\\nsuch as self-healing, scaling, rollouts, and rollbacks.\\nThat’s enough of a primer. Let’s build a swarm.\\nBuild a secure swarm cluster\\nIn this section, you’ll build a secure swarm cluster. I’ll build the cluster shown in Figure\\n10.2 with threemanagers and twoworkers. You can build a different cluster, but the\\nfollowing rules usually apply:\\n• Three or five managers are recommended for high-availability\\n• Enough workers to handle your application requirements',\n",
              " '10: Docker Swarm 138\\nFigure 10.2 - Sample lab environment\\nPre-reqs\\nIf you’re following along, I recommend using Multipass to create five Docker VMs on\\nyour laptop or local machine. Multipass is free and easy to use, and I’ll use it for the\\nexamples.\\nHowever, running five Multipass VMs requires a lot of system resources as each VM\\nneeds 2 CPUs, 40GB of disk space, and 4GB of memory. If your system can’t handle this,\\nyou can create fewer managers and workers or build your lab inPlay with Docker16.\\nPlay with Docker (PWD) is a free cloud-based service that lets you create up to five\\nnodes in a 4-hour Docker playground. Other Docker environments will also work. All\\nyou need is one or more Docker nodes that can communicate over a reliable network.\\nYou’ll also find it easier if you configure name resolution so you can reference nodes by\\nname rather than IP address.\\nI don’t recommend following along on Docker Desktop as it only supports a single node,\\nwhich isn’t enough for some of the examples later in the book.\\nBuild your Docker nodes\\nThis section uses Multipass to build the swarm in Figure 10.2.\\nSearch the web forinstall multipassand follow the instructions to install it on your\\nsystem.\\n16https://labs.play-with-docker.com/',\n",
              " '10: Docker Swarm 139\\nRun the following fivemultipass launch commands to create five VMs running\\nDocker. We’ll refer to each VM as anode, and each will require 2 CPUs, 40GB of disk\\nspace, and 4GB of RAM. If your machine doesn’t have those resources, go to Play with\\nDocker and build your lab there.\\nName three of the nodes asmgr1, mgr2, andmgr3, and name the other twowrk1 and\\nwrk2. Yours will get different IPs, but that doesn’t matter. It can take a minute or two\\nto create each VM.\\n$ multipass launch docker --name mgr1\\nLaunched: mgr1\\n$ multipass launch docker --name mgr2\\nLaunched: mgr2\\n$ multipass launch docker --name mgr3\\nLaunched: mgr3\\n$ multipass launch docker --name wrk1\\nLaunched: wrk1\\n$ multipass launch docker --name wrk2\\nLaunched: wrk2\\nRun the following command and note the node IP addresses. Yours will have different\\naddresses, and you’re only interested in the192.168.x.x address for each.\\n$ multipass ls\\nName State IPv4 Image\\nmgr1 Running 192.168.64.61 Ubuntu 22.04 LTS\\n172.17.0.1\\nmgr2 Running 192.168.64.62 Ubuntu 22.04 LTS\\n172.17.0.1\\nmgr3 Running 192.168.64.63 Ubuntu 22.04 LTS\\n172.17.0.1\\nwrk1 Running 192.168.64.64 Ubuntu 22.04 LTS\\n172.17.0.1\\nwrk2 Running 192.168.64.65 Ubuntu 22.04 LTS\\n172.17.0.1\\nOnce you’ve built the nodes, you can log on to them with themultipass shell\\ncommand, and you can log out by typingexit.\\nLog on to one of the nodes and check that Docker is installed and running.',\n",
              " '10: Docker Swarm 140\\n$ multipass shell mgr1\\n$ docker --version\\nDocker version 26.1.1, build 2ae903e\\nType exit to log out of the node and return to your local shell.\\nOnce you’ve confirmed your nodes are working, you can move to the next section and\\nbuild a swarm.\\nInitializing a new swarm\\nThe process of building a swarm is calledinitializing a swarm, and this is the high-level\\nprocess:\\n1. Initialize the first manager\\n2. Join additional managers\\n3. Join workers.\\nBefore a Docker node joins a swarm, it runs insingle-engine modeand can only run\\nregular containers. After joining a swarm, it switches intoswarm modeand can run\\nadvanced containers calledswarm services. More onservices later.\\nWhen you run adocker swarm init command on a Docker node that’s currently in\\nsingle-engine mode, Docker switches it intoswarm mode, creates a newswarm, and makes\\nthe node the firstmanager of the swarm. You can then add more nodes as managers and\\nworkers, and these nodes also get switched into swarm mode.\\nYou’re about to complete all of the following:\\n1. Initialize a new swarm frommgr1\\n2. Join wrk1 and wrk2 as worker nodes\\n3. Join mgr2 and mgr3 as additional managers\\nAfter completing the procedure, all five nodes will be in swarm mode and part of the\\nsame swarm.\\nThe examples will use the names and IPs from Figure 10.2. Yours will be different, and\\nyou should use your own.\\n1. Log on tomgr1 and initialize a new swarm. Be sure to use the private IP address of\\nyour mgr1 node.',\n",
              " '10: Docker Swarm 141\\n$ docker swarm init \\\\\\n--advertise-addr 192.168.64.61:2377 \\\\\\n--listen-addr 192.168.64.61:2377\\nSwarm initialized: current node (d21lyz...c79qzkx) is now a manager.\\n<Snip>\\nLet’s step through the different parts of the command:\\n• docker swarm init tells Docker to initialize a new swarm with this node as\\nthe first manager.\\n• The --advertise-addr flag is optional and tells Docker which of the node’s\\nIP addresses to advertise as the swarm API endpoint. It’s usually one of the\\nnode’s IP addresses but can also be an external load balancer.\\n• The --listen-addr flag tells Docker which of the node’s interfaces to accept\\nswarm traffic on. It defaults to the same value as--advertise-addr if you\\ndon’t specify it. However, if--advertise-addr is a load balancer, you must\\nuse --listen-addr to specify a local IP.\\nI recommend specifying both flags in important production environments.\\nHowever, for most lab environments, you can just run adocker swarm init\\nwithout any flags.\\nYou can also specify a different port for the advertise and listen addresses, but\\nusing the default2377 is common.\\n2. List the nodes in the swarm.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\nd21...qzkx * mgr1 Ready Active Leader\\nmgr1 is the only node in the swarm and is listed as theLeader. We’ll talk about\\nleaders later.\\n3. Run the following command frommgr1 to see the commands and tokens needed\\nto add new workers and managers.\\n$ docker swarm join-token worker\\nTo add a manager to this swarm, run the following command:\\ndocker swarm join --token SWMTKN-1-0uahebax...c87tu8dx2c 192.168.64.61:2377\\n$ docker swarm join-token manager\\nTo add a manager to this swarm, run the following command:\\ndocker swarm join --token SWMTKN-1-0uahebax...ue4hv6ps3p 192.168.64.61:2377\\nNotice how the commands are identical except for the join tokens (SWMTKN...).\\nYou should keep your join tokens in a safe place, as they’re all that’s required to\\njoin other nodes to your swarm!',\n",
              " '10: Docker Swarm 142\\n4. Log on towrk1 and use the correct command and token to join it as a worker\\nnode. I’ve added the optionaladvertise-addr and listen-addr flags in the\\nexample so you know how to use them. You can leave them off, but if you do use\\nthem, be sure to use the correct IPs for your environment.\\n$ docker swarm join \\\\\\n--token SWMTKN-1-0uahebax...c87tu8dx2c \\\\\\n10.0.0.1:2377 \\\\\\n--advertise-addr 192.168.64.64:2377 \\\\\\n--listen-addr 192.168.64.64:2377\\nThis node joined a swarm as a worker.\\nIf you experience joining problems, make sure the following network ports are\\nopen between all nodes:\\n• 2377/tcp: for secure client-to-swarm communication\\n• 7946/tcp and udp: for control plane gossip\\n• 4789/udp: for VXLAN-based overlay networks\\n5. Repeat the previous step onwrk2 so that you have two workers. If you specify\\nthe --advertise-addr and --listen-addr flags, make sure you usewrk2’s IP\\naddresses.\\n6. Log on tomgr2 and magr3 and join them as managers using themanager token\\nfrom step 3. Again, you don’t have to use the--advertise-addr and --listen-\\naddr flags, but if you do, be sure to use the correct IPs for your environment.\\n$ docker swarm join \\\\\\n--token SWMTKN-1-0uahebax...ue4hv6ps3p \\\\\\n10.0.0.1:2377 \\\\\\n--advertise-addr 192.168.64.62:2377 \\\\\\n--listen-addr 192.168.64.62:2377\\nThis node joined a swarm as a manager.\\n7. List the nodes in your swarm by runningdocker node ls from any of the\\nmanager nodes.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION\\n0g4rl...babl8 * mgr2 Ready Active Reachable 26.1.1\\n2xlti...l0nyp mgr3 Ready Active Reachable 26.1.1\\nd21ly...9qzkx mgr1 Ready Active Leader 26.1.1\\n8yv0b...wmr67 wrk1 Ready Active 26.1.1\\ne62gf...l5wt6 wrk2 Ready Active 26.1.1',\n",
              " '10: Docker Swarm 143\\nIf you look closely at theMANAGER STATUS column, you’ll see all three managers are\\nshowing as eitherReachable or Leader. The nodes with nothing in this column are\\nworker nodes. The manager with the asterisk after its name is the one you’re executing\\ncommands from.\\nCongratulations. You’ve created a five-node swarm with three managers and two\\nworkers. The Docker Engine on each node is now operating in swarm mode, and the\\nswarm is secured with TLS.\\nSwarm manager high availability\\nSwarm clusters are highly available (HA), meaning one or more managers can fail, and\\nthe swarm will keep running.\\nTechnically speaking, Swarm implementsactive/passive multi-manager HA. This means\\na swarm with three managers will have one active manager, and the other two will be\\npassive. In a swarm, we call the active manager theleader and the passive managers\\nfollowers, and the leader is the only manager that can update the swarm configuration.\\nIf the leader fails, one of the followers will be elected as the new leader and the swarm\\nwill keep running without any service interruption. If you send commands to a follower,\\nit proxies them to the leader.\\nFigure 10.3 shows you issuing a command to a follower manager requesting an update\\nto the swarm. Step 1 shows you issuing the command. Step 2 shows the follower\\nmanager receiving and proxying the command to the leader. Step 3 shows the leader\\nexecuting it.\\nFigure 10.3\\nLeader and follower is Raft terminology, and we use it because Swarm implements the\\nRaft consensus algorithm17 to maintain a consistent cluster state across multiple highly-\\navailable managers.\\n17https://raft.github.io/',\n",
              " '10: Docker Swarm 144\\nThe following good practices apply when it comes to manager HA:\\n1. Always deploy an odd number of managers\\n2. Don’t deploy too many managers (3 or 5 is usually enough)\\n3. Spread managers across availability zones\\nConsider the two examples shown in Figure 10.4. The swarm on the left has an even\\nnumber of managers, and a network incident has created a network partition with two\\nmanagers on either side. We call this asplit brainbecause neither side can be sure it has\\na majority, and the cluster goes into read-only mode. When this happens, your apps\\ncontinue working but you can’t make changes to them or to the cluster. However, the\\nswarm on the right has anodd number of managersand remains fully operational in\\nread-write mode because the two managers on the right side of the network partition\\nknow they have a majority (quorum). So, even though the swarm on the right has fewer\\nmanagers than the one on the left, it has better availability.\\nFigure 10.4 - Swarm high availability\\nAs with all consensus algorithms, more participants means longer times to achieve\\nconsensus. It’s like choosing where to eat — it’s always quicker and easier for three\\npeople to decide than it is for 33! With this in mind, it’s usually a best practice to have\\nthree or five managers for HA. Seven might work, but three or five usually right for\\nmost swarms.\\nA final word of caution regarding manager HA. While you should definitely spread\\nyour managers across availability zones, they need to be connected by fast and reliable\\nnetworks. This means you should think carefully and do lots of testing before imple-\\nmenting multi-cloud clusters implementing multi-cloud HA.',\n",
              " '10: Docker Swarm 145\\nBuilt-in Swarm security\\nSwarm ships with a lot of security features, such as a built-in certificate authority (CA),\\nmutual TLS, an encrypted cluster store, encrypted networks, cryptographic node IDs\\nand join tokens, and more. Fortunately, Swarm automatically configures them with\\nsensible defaults.\\nLocking a Swarm\\nDespite all of Swarm’s security features, restarting an old manager or restoring an old\\nbackup can potentially compromise your cluster. For example, a malicious actor with\\naccess to an old manager node may be able to re-join it to the swarm and gain unwanted\\naccess.\\nFortunately, you can use Swarm’sautolock feature to force restarted managers to present\\na key before being admitted back into the swarm.\\nYou can autolock new swarms at build time by passing the--autolock flag to the\\ndocker swarm init command. For existing swarms, you can autolock them with the\\ndocker swarm update command.\\nRun the following command from one of your swarm managers to lock your existing\\nswarm cluster.\\n$ docker swarm update --autolock=true\\nSwarm updated.\\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock` command and\\nprovide the following key:\\nSWMKEY-1-XDeU3XC75Ku7rvGXixJ0V7evhDJGvIAvq0D8VuEAEaw\\nPlease remember to store this key in a password manager...\\nWhile it’s possible to run adocker swarm unlock-key command on any manager to\\nview your unlock key, you should also keep a secure copy outside of your cluster in case\\nyou’re ever locked out of your swarm.\\nRestart Docker on one of your managers to see if it automatically re-joins the cluster.\\n$ sudo systemctl restart docker\\nTry to list the nodes in the swarm.',\n",
              " '10: Docker Swarm 146\\n$ docker node ls\\nError response from daemon: Swarm is encrypted and needs to be unlocked before it can be used.\\nDocker has restarted, but the manager hasn’t been allowed to re-join the swarm. You\\ncan prove this by running adocker node ls from one of your other managers and the\\nrestarted manager will be listed asdown and unreachable.\\nRun thedocker swarm unlock command to unlock the swarm for the restarted\\nmanager and provide the unlock key.\\n$ docker swarm unlock\\nPlease enter unlock key: <enter your key>\\nDocker will re-admit the node to the swarm, and it will show asready and reachable in\\nfuture commands.\\nYou should lock your production swarms and protect the unlock keys.\\nDedicated manager nodes\\nBy default, Swarm runs user applications on workersand managers. However, you may\\nwant to configure your production swarm clusters to only run user applications on\\nworkers. This allows managers to focus exclusively on control-plane duties.\\nRun the following command on any manager to prevent it from running user applica-\\ntions. You’ll need to use the names of your managers, and you’ll need to run it on all\\nmanagers if you want to force user applications to run only on workers.\\n$ docker node update --availability drain mgr1\\nYou’ll see the impact of this in later steps when you deploy services with multiple\\nreplicas.\\nNow that you’ve built a swarm and understand the concepts ofleaders and manager HA,\\nlet’s move on to the application side of things.\\nDeploy and manage an app on swarm\\nSwarm nodes can run regular containers, but they can also run enhanced containers\\ncalled services. Eachservice takes a single container definition and augments it with\\ncloud-native features such as self-healing, scaling, and automated rollouts and rollbacks.\\nIn this section, you’ll deploy a simple web server as aswarm serviceand see how to scale\\nit, perform rollouts, and delete it.\\nSwarm lets you create and manage services in two ways:',\n",
              " '10: Docker Swarm 147\\n1. Imperatively on the command line\\n2. Declaratively with a Compose file\\nWe’ll look at Compose files in the next chapter. For now, we’ll focus on the imperative\\nmethod.\\nRun the following command to deploy a new service calledweb-fe with five identical\\nreplicas.\\n$ docker service create --name web-fe \\\\\\n-p 8080:8080 \\\\\\n--replicas 5 \\\\\\nnigelpoulton/ddd-book:web0.1\\nz7ovearqmruwk0u2vc5o7ql0p\\noverall progress: 5 out of 5 tasks\\n1/5: running [==================================================>]\\n2/5: running [==================================================>]\\n3/5: running [==================================================>]\\n4/5: running [==================================================>]\\n5/5: running [==================================================>]\\nverify: Service converged\\nThe docker service create command tells Docker to deploy a new service. The--\\nname flag calls this oneweb-fe, the-p flag maps port8080 on every swarm node to8080\\ninside of each service replica (container), and the--replicas flag tells Swarm to deploy\\nfive replicas of the service. The last line tells Swarm which image to build the replicas\\nfrom.\\nTerminology: We use the termsreplicas and containers interchangeably when\\nreferring to services. For example, a service defining fivereplicas will deploy\\nfive identicalcontainers.\\nYour Docker client sent the command to a swarm manager, and theleader manager\\ninitiated the work to deploy the five replicas across theswarm. You configured this\\nswarm so that user applications won’t run on managers, meaning all five replicas will\\nbe running on your worker nodes. Each worker node that received a work task pulled\\nthe image, started the required containers, and created the port mappings.\\nHowever, it doesn’t end there. The command you issued requested a new service with\\nfive replicas, and Swarm stored this in the cluster store as yourdesired state. In the\\nbackground, Swarm runs areconciliation loopthat constantly compares theobserved state\\nof the cluster with thedesired state. When the two states match, the world is a happy\\nplace, and no further action is needed. When they don’t match, Swarm takes action to\\nbring theobserved stateinto line withdesired state.',\n",
              " '10: Docker Swarm 148\\nFor example, if aworker hosting one of the five replicas fails, theobserved stateof the\\ncluster will drop from 5 replicas to 4 and no longer match thedesired stateof 5. As soon\\nas the swarm observes the difference, it will start a new replica to bring theobserved state\\nback in line withdesired state. We call thisreconciliation or self-healing, and it’s a key tenet\\nof cloud-native applications.\\nYou’ll soon learn that desired state and reconciliation are also at the heart of scaling,\\nrollouts, and rollbacks.\\nViewing and inspecting services\\nRun adocker service ls command to see a list of all the services on your swarm.\\n$ docker service ls\\nID NAME MODE REPLICAS IMAGE PORTS\\nz7o...uw web-fe replicated 5/5 nigelpoulton/ddd... *:8080->8080/tcp\\nThe output shows a single service along with some basic config and state info. Among\\nother things, you can see the name of the service and that all five replicas are running.\\nSome of the replicas might not be in the running state if you run the command too soon\\nafter deploying the service. This is usually because the workers are still pulling the image\\nand starting the replicas.\\nYou can run adocker service ps command to see a list of service replicas and the state\\nof each.\\n$ docker service ps web-fe\\nID NAME IMAGE NODE DESIRED CURRENT\\n817...f6z web-fe.1 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 2 mins\\na1d...mzn web-fe.2 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 2 mins\\ncc0...ar0 web-fe.3 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 2 mins\\n6f0...azu web-fe.4 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 2 mins\\ndyl...p3e web-fe.5 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 2 mins\\nThe output displays each replica on its own line, including the node it’s running on\\nand each replica’s desired state and current observed state. As expected, you have five\\nreplicas balanced across all of your workers.\\nIf you need detailed information about a service, you can run thedocker service\\ninspect command.',\n",
              " '10: Docker Swarm 149\\n$ docker service inspect --pretty web-fe\\nID: z7ovearqmruwk0u2vc5o7ql0p\\nName: web-fe\\nService Mode: Replicated\\nReplicas: 5\\nPlacement:\\nUpdateConfig:\\nParallelism: 1\\nOn failure: pause\\nMonitoring Period: 5s\\nMax failure ratio: 0\\nUpdate order: stop-first\\nRollbackConfig:\\nParallelism: 1\\nOn failure: pause\\nMonitoring Period: 5s\\nMax failure ratio: 0\\nRollback order: stop-first\\nContainerSpec:\\nImage: nigelpoulton/ddd-book:web0.1@sha256:8d6280c0042...1b9e4336730e5\\nInit: false\\nResources:\\nEndpoint Mode: vip\\nPorts:\\nPublishedPort = 8080\\nProtocol = tcp\\nTargetPort = 8080\\nPublishMode = ingress\\nThe example uses the--pretty flag to limit the output to the most interesting items\\nprinted in an easy-to-read format. You’ll get more info if you leave off the--pretty\\nflag. I highly recommend reading through the output ofdocker inspect commands,\\nas they’re a great way to learn what’s going on under the hood.\\nWe’ll come back to some of these outputs later.\\nReplicated vs global services\\nSwarm has two modes for deploying replicas to nodes:\\n• Replicated (default)\\n• Global\\nThe defaultreplicated mode allows you to deploy as many replicas as you need and\\nattempts to distribute them evenly across available nodes.\\nThe global mode deploys a single replica on every available node in the swarm.',\n",
              " '10: Docker Swarm 150\\nBoth modes respect node availability and will only deploy replicas to eligible nodes. For\\nexample, if you don’t allow your swarm to deploy applications to managers, both modes\\nwill only deploy to workers.\\nIf you want to deploy aglobal service, you need to pass the--mode global flag to the\\ndocker service create command. Obviously, it doesn’t support the--replicas flag,\\nas it always deploys a single replica on each available node.\\nScaling services\\nAnother powerful feature ofservices is the ability to scale them up and down by adding\\nor removing replicas.\\nAssume business is booming and you’re experiencing double the number of requests to\\nyour web front-end. Fortunately, you can easily scale the number of replicas with the\\ndocker service scale command.\\nRun the following command to scale theweb-fe service from 5 to 10 replicas.\\n$ docker service scale web-fe=10\\nweb-fe scaled to 10\\noverall progress: 10 out of 10 tasks\\n1/10: running [==================================================>]\\n2/10: running [==================================================>]\\n3/10: running [==================================================>]\\n4/10: running [==================================================>]\\n5/10: running [==================================================>]\\n6/10: running [==================================================>]\\n7/10: running [==================================================>]\\n8/10: running [==================================================>]\\n9/10: running [==================================================>]\\n10/10: running [==================================================>]\\nverify: Service web-fe converged\\nBehind the scenes, the command updated the desired state of theweb-fe service in the\\ncluster store from 5 replicas to 10. The reconciliation loop read the new desired state of\\n10 replicas, compared it with the current observed state of 5 replicas, and added 5 more\\nto bring the observed state into sync with the new desired state.\\nRun anotherdocker service ls to verify the operation completed successfully.\\n$ docker service ls\\nID NAME MODE REPLICAS IMAGE PORTS\\nz7o...uw web-fe replicated 10/10 nigelpoulton/ddd... *:8080->8080/tcp\\nIf you run adocker service ps, you’ll see that the replicas are balanced evenly across\\nall available nodes. In this example, they’re balanced across both available workers. If\\nyour swarm had ten worker nodes, you would probably have one replica running on\\neach worker.',\n",
              " '10: Docker Swarm 151\\n$ docker service ps web-fe\\nID NAME IMAGE NODE DESIRED CURRENT\\nnwf...tpn web-fe.1 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 7 mins\\nyb0...e3e web-fe.2 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 7 mins\\nmos...gf6 web-fe.3 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 7 mins\\nutn...6ak web-fe.4 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 7 mins\\n2ge...fyy web-fe.5 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 7 mins\\n64y...m49 web-fe.6 nigelpoulton/ddd-book:web0.1 wrk2 Running Running about a min\\nild...51s web-fe.7 nigelpoulton/ddd-book:web0.1 wrk1 Running Running about a min\\nvah...rjf web-fe.8 nigelpoulton/ddd-book:web0.1 wrk1 Running Running about a min\\nxe7...fvu web-fe.9 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 45 secs ago\\nl7k...jkv web-fe.10 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 46 secs ago\\nBehind the scenes, Swarm runs a scheduling algorithm calledspread that attempts to\\nbalance replicas as evenly as possible across available nodes.\\nRun anotherdocker service scale command to bring the number of replicas back\\ndown to 5.\\n$ docker service scale web-fe=5\\nweb-fe scaled to 5\\noverall progress: 5 out of 5 tasks\\n1/5: running [==================================================>]\\n2/5: running [==================================================>]\\n3/5: running [==================================================>]\\n4/5: running [==================================================>]\\n5/5: running [==================================================>]\\nverify: Service web-fe converged\\nNow that you know how to scale services, let’s see how to delete them.\\nDeleting services\\nDeleting services is simple — may be too simple.\\nRun the following command to delete the web-fe service. Be careful using this com-\\nmand, as it doesn’t ask for confirmation.\\n$ docker service rm web-fe\\nweb-fe\\nConfirm it’s gone.',\n",
              " '10: Docker Swarm 152\\n$ docker service ls\\nID NAME MODE REPLICAS IMAGE PORTS\\nLet’s look at how to perform a rollout.\\nRollouts\\nApplication updates are a fact of life, and for the longest time they were painful. I’ve\\npersonally lost more than enough weekends to major application updates and I’ve no\\nintention of doing it again.\\nFortunately, thanks to Dockerservices, updating well-designed microservices apps is\\neasy.\\nTerminology: We use terms likerollouts, updates,and rolling updatesto mean\\nthe same thing — updating a live application.\\nYou’re about to deploy a new service to help demonstrate a rollout. However, before\\ndoing that, you’ll create a new overlay network for the service. This isn’t necessary, but I\\nwant you to see how to attach services to networks.\\nRun the following two commands to create a new overlay network calleduber-net and\\nthen check if it exists.\\n$ docker network create -d overlay uber-net\\n43wfp6pzea470et4d57udn9ws\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\n43wfp6pzea47 uber-net overlay swarm\\n<Snip>\\nThe new network exists and was successfully created as anoverlay network scoped to\\nthe entireswarm. You’ll learn about overlay networks in a later chapter, but for now,\\nit’s enough to know that they span all swarm nodes and that all containers on the same\\noverlay network can communicate, even if they’re deployed on different nodes.\\nFigure 10.5 shows four swarm nodes connected to two different underlay networks\\nconnected by a layer 3 router. The overlay network spans all four nodes and creates a\\nsingle flat layer 2 network abstracting all the underlying networks. All container replicas\\nare connected to the overlay and can communicate with each other.',\n",
              " '10: Docker Swarm 153\\nFigure 10.5\\nLet’s create a new service and attach it to theuber-net network.\\n$ docker service create --name uber-svc \\\\\\n--network uber-net \\\\\\n-p 8080:8080 --replicas 12 \\\\\\nnigelpoulton/ddd-book:web0.1\\ndhbtgvqrg2q4sg07ttfuhg8nz\\noverall progress: 12 out of 12 tasks\\n1/12: running [==================================================>]\\n2/12: running [==================================================>]\\n3/12: running [==================================================>]\\n<Snip>\\n12/12: running [==================================================>]\\nverify: Service converged\\nLet’s step through what just happened.\\nOn the first line, you named the serviceuber-svc. You then used the--network flag\\nto tell Docker to attach all service replicas to the newuber-net network. Then, you\\nmapped 8080 on every swarm node to port8080 in all 12 replicas. Finally, you told\\nDocker which image to base the 12 replicas on.\\nThis mode of publishing a service on every swarm node, including nodes not even\\nrunning replicas, is calledingress modeand is the default. The alternative ishost mode,\\nwhich only publishes a service on nodes running replicas.\\nRun adocker service ls and adocker service ps to verify the state of the service\\nand each replica.',\n",
              " '10: Docker Swarm 154\\n$ docker service ls\\nID NAME REPLICAS IMAGE\\ndhbtgvqrg2q4 uber-svc 12/12 nigelpoulton/ddd-book:web0.1\\n$ docker service ps uber-svc\\nID NAME IMAGE NODE DESIRED CURRENT STATE\\n0v...7e5 uber-svc.1 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nbh...wa0 uber-svc.2 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n23...u97 uber-svc.3 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n82...5y1 uber-svc.4 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nc3...gny uber-svc.5 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne6...3u0 uber-svc.6 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n78...r7z uber-svc.7 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\n2m...kdz uber-svc.8 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nb9...k7w uber-svc.9 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nag...v16 uber-svc.10 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\ne6...dfk uber-svc.11 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne2...k1j uber-svc.12 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nConnect to the service by opening a web browser and pointing it to the IP address of\\nany swarm node on port8080. If you’re following along on Multipass, you’ll need to\\nconnect to the192.168 address of one of your nodes.\\nFigure 10.6\\nYou should also point your browser to the IP of one of your managers to prove you can\\naccess the service from nodes that are not running replicas. That’s the beauty ofingress\\nmode — you can access a service from every node in the cluster, even nodes not running\\nreplicas.',\n",
              " '10: Docker Swarm 155\\nRight now, the web page has links to two books. Let’s assume you need to add a third\\nbook. In the real world, you’d update the app code and create a new image containing\\nthe updates. I’ve already done this and uploaded the image to the same Docker Hub repo\\nwith theweb0.2 tag. All that’s left for you to do is perform the rollout.\\nLet’s assume you need the rollout to proceed in a staged manner by updating two\\nreplicas at a time with a 20-second delay between each. You can use the following\\ncommand to accomplish this.\\n$ docker service update \\\\\\n--image nigelpoulton/ddd-book:web0.2 \\\\\\n--update-parallelism 2 \\\\\\n--update-delay 20s \\\\\\nuber-svc\\nuber-svc\\noverall progress: 2 out of 12 tasks\\n1/12: running [==================================================>]\\n2/12: running [==================================================>]\\n3/12: ready [======================================> ]\\n4/12: ready [======================================> ]\\n5/12:\\n6/12:\\n<Snip>\\n11/12:\\n12/12:\\nLet’s review the command.\\ndocker service update lets you update existing services. This example tells Docker\\nto update all 12 replicas to the newweb0.2 version of the image. The--update-\\nparallelism flag tells Docker to update two replicas at a time, and the--update-delay\\nflag tells Docker to insert a 20-second cool-off period after each batch of updates. The\\nlast line tells it which service to update.\\nIf you run adocker service ps uber-svc while the update is in progress, some\\nreplicas will show the new version and some will show the old version. If you give the\\noperation enough time to complete, all replicas will eventually reach the new desired\\nstate and show theweb0.2 image.',\n",
              " '10: Docker Swarm 156\\n$ docker service ps uber-svc\\nID NAME IMAGE NODE DESIRED CURRENT STATE\\n7z...nys uber-svc.1 nigelpoulton/ddd-book:web0.2 wrk1 Running Running 13 secs\\n0v...7e5 \\\\_uber-svc.1 nigelpoulton/ddd-book:web0.1 wrk1 Shutdown Shutdown 13 secs\\nbh...wa0 uber-svc.2 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\ne3...gr2 uber-svc.3 nigelpoulton/ddd-book:web0.2 wrk2 Running Running 13 secs\\n23...u97 \\\\_uber-svc.3 nigelpoulton/ddd-book:web0.1 wrk2 Shutdown Shutdown 13 secs\\n82...5y1 uber-svc.4 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nc3...gny uber-svc.5 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne6...3u0 uber-svc.6 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\n78...r7z uber-svc.7 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\n2m...kdz uber-svc.8 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\nb9...k7w uber-svc.9 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nag...v16 uber-svc.10 nigelpoulton/ddd-book:web0.1 wrk2 Running Running 1 min\\ne6...dfk uber-svc.11 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\ne2...k1j uber-svc.12 nigelpoulton/ddd-book:web0.1 wrk1 Running Running 1 min\\nYou can observe the update in real-time by opening a web browser to any swarm node\\non port8080 and hitting refresh a few times. Some of the requests will respond with\\nthe old version and some will respond with the new. After enough time, all requests will\\nretrieve the new version.\\nCongratulations. You’ve just completed a zero-downtime rolling update of a live\\ncontainerized application.\\nIf you run adocker service inspect --pretty command against the service, you’ll\\nsee theupdate parallelismand update delaysettings have been merged into the service’s\\ndefinition. This means future updates will automatically use these settings unless you\\noverride them as part of the update command.\\n$ docker service inspect --pretty uber-svc\\nID: mub0dgtc8szm80ez5bs8wlt19\\nName: uber-svc\\nService Mode: Replicated\\nReplicas: 12\\n<Snip>\\nUpdateConfig:\\nParallelism: 2 <<--------\\nDelay: 20s <<--------\\n<Snip>\\nService logs\\nYou can inspect a service’s logs with thedocker service logs command. It gathers the\\nlogs from every replica and displays them in a single output.',\n",
              " '10: Docker Swarm 157\\nIf you’ve been following along, you’ll have a service calleduber-svc with 12 replicas\\nmapped to port8080 on every cluster node. Open a browser tab to the IP address of any\\ncluster node on port8080 and refresh the page a few times.\\nNow run adocker service logs command to see any log messages generated by the\\nservice.\\n$ docker service logs uber-svc\\nuber-svc.8.r33cra80eeut@wrk1 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.2.vid8et52oj48@wrk1 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.7.jv67x3zfkyar@wrk2 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.1.zlmvvo5k1i56@wrk2 | {\"level\":\"info\",\"message\":\"/\"}\\nuber-svc.3.kleeiy0vuue5@wrk2 | {\"level\":\"info\",\"message\":\"/\"}\\nThe output shows five lines relating to five requests serviced by five different replicas.\\nThe long string on the left of each line is the name of the replica which Docker creates\\nby concatenating the service name, replica number, replica ID, and the name of the node\\nit’s scheduled on. The following table maps the names of the five replicas that serviced\\nrequests.\\nService name Replica number Replica ID node\\nuber-svc 8 r33cra80eeut @wrk1\\nuber-svc 2 vid8et52oj48 @wrk1\\nuber-svc 7 jv67x3zfkyar @wrk2\\nuber-svc 1 zlmvvo5k1i56 @wrk2\\nuber-svc 3 kleeiy0vuue5 @wrk2\\nNotice how Swarm balanced the requests across five different replicas. In the real world,\\nyour requests probably won’t be as evenly balanced as this due to the way browsers and\\ndifferent tools create and handle requests. However, if you refresh enough times, you’ll\\neventually see the requests being serviced by different replicas.\\nBy default, Docker configures services to use thejson-file log driver, but other drivers\\nexist, including:\\n• awslogs\\n• gelf\\n• gcplogs\\n• journald (only works on Linux hosts runningsystemd)\\n• splunk\\n• syslog\\njson-file and journald are the easiest to configure, and they both work with thedocker\\nservice logs command. However, other drivers don’t always work with the native\\nDocker commands and you may have to use the logging platform’s native tools.',\n",
              " '10: Docker Swarm 158\\nThe following snippet is from adaemon.json configuration file on a Docker host\\nconfigured to use thesyslog driver. Thedaemon.json file is usually located in/etc/-\\ndocker/daemon.json, but sometimes it doesn’t exist unless you create it manually for\\ncustom settings.\\n{\\n\"log-driver\": \"syslog\"\\n}\\nYou can force new services to use specific log drivers by starting them with the--log-\\ndriver and --log-opts flags. Doing this will override anything set in thedaemon.json\\nfile.\\nService logs expect applications to run as PID 1 and send logs toSTDOUT and errors to\\nSTDERR. The logging driver then forwards the logs to the locations configured via the\\nlogging driver.\\nYou can run adocker service logs command with the--follow flag to follow logs,\\nthe --tail flag to tail them, and the--details flag to get extra information.\\nDocker Swarm – The Commands\\n• docker swarm init creates a new swarm. The node you run the command on\\nbecomes the first manager and the Docker Engine on that node switches into\\nswarm mode.\\n• docker swarm join-token reveals the commands and tokens you need to join\\nworkers and managers to the swarm. You addmanager to the command to get the\\nmanager join token andworker to get the worker token. Be sure to keep your join\\ntokens secure!\\n• docker node ls lists managers and workers, including which manager is the\\nleader.\\n• docker service create creates a new service.\\n• docker service ls lists running services and gives basic information on their\\nstate and the number of replicas they’re running.\\n• docker service ps <service> gives more detailed information about individual\\nservice replicas.\\n• docker service inspect gives very detailed information on a service. You can\\nadd the--pretty flag to get a nicely formatted summary view.\\n• docker service scale lets you scale the number of service replicas.\\n• docker service update lets you update the properties of a running service.',\n",
              " '10: Docker Swarm 159\\n• docker service logs shows service logs.\\n• docker service rm deletes a service without asking for confirmation.\\nChapter summary\\nSwarm is Docker’s native technology for managing clusters of Docker nodes and\\norchestrating microservices applications. It is similar to Kubernetes but less advanced\\nand easier to use.\\nThe enterprise-grade clustering component offers a wealth of security and HA features\\nthat are automatically configured and extremely simple to modify.\\nThe orchestration component deploys and manages cloud-native microservices\\napplications.',\n",
              " '11: Deploying apps with Docker Stacks\\nDeploying and managing cloud-native microservices applications at scale is hard.\\nFortunately, Docker Stacks are here to help.\\nI’ve split this chapter as follows:\\n• Deploying apps with Docker Stacks – The TLDR\\n• Build a Swarm lab\\n• The sample app\\n• Deploy the app\\n• Manage the app\\nDeploying apps with Docker Stacks – The TLDR\\nDocker Stacks combine Compose and Swarm to create a platform for easy deployment\\nand management of complex multi-container apps on secure, highly available infrastruc-\\nture.\\nYou build a swarm, define your apps in Compose files, and deploy and manage them\\nwith thedocker stack command.\\nFrom an architecture perspective, stacks are at the top of the Docker application\\nhierarchy — they build on top ofservices, which in turn build on top of containers, and\\nthey only run on swarms.',\n",
              " '11: Deploying apps with Docker Stacks 161\\nFigure 11.1\\nIf you’ve been following along, you’ll already know Compose and Swarm and find\\nDocker Stacks easy.\\nBuild a Swarm lab\\nThis section shows you how to build a three-node swarm lab with a single manager and\\ntwo workers. You can skip to the next section if you already have a swarm.\\nYou can build the lab in Multipass, Play with Docker, or just about any Docker envi-\\nronment. However, I don’t recommend using Docker Desktop, as you only get a single\\nswarm node.\\nRun the following command from the node that will be your swarm manager.\\n$ docker swarm init\\nSwarm initialized: current node (lhma...w4nn) is now a manager.\\n<Snip>\\nCopy thedocker swarm join command shown in the output and paste it into the two\\nnodes you want to join as workers.\\nWorker 1\\n$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377\\nThis node joined a swarm as a worker.\\nWorker 2',\n",
              " '11: Deploying apps with Docker Stacks 162\\n$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377\\nThis node joined a swarm as a worker.\\nRun the following command from your manager node to confirm the swarm is initial-\\nized with one manager and two workers.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\nlhm...4nn * mgr1 Ready Active Leader\\nb74...gz3 wrk1 Ready Active\\no9x...um8 wrk2 Ready Active\\nNow that you have a swarm lab, let’s look at the sample app.\\nThe sample app\\nFigure 11.2 shows the application we’ll use for the rest of the chapter. It’s a multi-\\ncontainer microservices application with:\\n• Two services (web-fe and redis)\\n• An encrypted overlay network (counter-net)\\n• A volume (counter-vol)\\n• A published port (5001)\\nFigure 11.2',\n",
              " \"11: Deploying apps with Docker Stacks 163\\nTerminology: Throughout this chapter, we’ll use the termservice to refer to\\nthe Docker service object that manages one or more identical containers on\\na swarm. We’ll also refer to the Compose file as thestack file, and sometimes\\nwe’ll refer to the application as thestack.\\nIf you haven’t already done so, log on to a swarm manager and clone the book’s GitHub\\nrepo.\\n$ git clone https://github.com/nigelpoulton/ddd-book.git\\nCloning into 'ddd-book'...\\nremote: Enumerating objects: 8904, done.\\nremote: Counting objects: 100% (74/74), done.\\nremote: Compressing objects: 100% (52/52), done.\\nremote: Total 8904 (delta 21), reused 70 (delta 18), pack-reused 8830\\nReceiving objects: 100% (8904/8904), 74.00 MiB | 4.18 MiB/s, done.\\nResolving deltas: 100% (1378/1378), done.\\nChange directory into theddd-book/swarm-app folder.\\n$ cd ddd-book/swarm-app\\nFeel free to look at the application in theapp subfolder, but we’ll focus on thecom-\\npose.yaml file.\\nIf you look at the Compose file, you’ll see it has three top-level keys.\\n• networks\\n• volumes\\n• services\\nNetworks is where you define the networks the app needs,volumes defines the\\nvolumes it needs, andservices defines the microservices that make up the app. The file\\nis a simple example ofinfrastructure as code— the application and its infrastructure are\\ndefined in a configuration file that you deploy and manage them from.\\nIf you expand each top-level key, you’ll see how things map to Figure 11.2 with a\\nnetwork calledcounter-net, a volume calledcounter-vol, and two services calledweb-\\nfe and redis.\",\n",
              " \"11: Deploying apps with Docker Stacks 164\\nnetworks:\\ncounter-net:\\nvolumes:\\ncounter-vol:\\nservices:\\nweb-fe:\\nredis:\\nLet’s look at each section of thestack file.\\nLooking closer at the stack file\\nStack filesare identical to Compose files with a few differences at run-time. For example,\\nDocker Compose lets you build images at run-time, but Docker Stacks don’t.\\nLet’s look at the networking elements defined in our stack file.\\nNetworks and networking\\nOne of the first things Docker does when you deploy an app from a stack file is create\\nthe networks listed under thenetworks key. If they already exist, Docker doesn’t need to\\ndo anything. But if they don’t exist, it creates them.\\nThis app defines a single encrypted overlay network calledcounter-net.\\nnetworks:\\ncounter-net: <<---- Network name\\ndriver: overlay <<---- Driver/type of network\\ndriver_opts:\\nencrypted: 'yes' <<---- Encrypt the data plane\\nIt needs to be an overlay network so it can span all nodes in the swarm.\\nEncrypting the data plane will keep the application traffic private, but it incurs a\\nperformance penalty that varies based on factors such as traffic type and traffic flow. It’s\\nnot uncommon for the performance penalty to be around 10%, but you should perform\\nyour own testing against your own applications.\\nThe stack also publishes theweb-fe service on port5001 on the swarm-wideingress\\nnetwork and maps it back to port8080 on all service replicas. This allows external clients\\nto reach the service by hitting any swarm node on port5001.\",\n",
              " '11: Deploying apps with Docker Stacks 165\\nservices:\\nweb-fe:\\n<Snip>\\nports:\\n- target: 8080 <<---- Service replicas listen on this port\\npublished: 5001 <<---- and are published externally on this port\\nLet’s look at the volumes and mounts.\\nVolumes and mounts\\nThe app defines a single volume calledcounter-vol and mounts it into the/app\\ndirectory on allweb-fe replicas. This means all reads and writes to the/app folder will\\nbe read and written to the volume.\\nvolumes:\\ncounter-vol: <<---- Volume name\\nservices:\\nweb-fe:\\n<Snip>\\nvolumes:\\n- type: volume\\nsource: counter-vol <<---- Mount the \"counter-vol\" volume to\\ntarget: /app <<---- \"/app\" in all service replicas\\nLet’s look at the services.\\nServices\\nServices are where most of the action happens. Our application defines two, and we’ll\\nlook at each in turn.\\nThe web-fe service\\nAs you can see, theweb-fe service defines an image, an app, the desired number of\\nreplicas, rules for updating the app, a restart policy, which network to connect to, which\\nports to publish, and how to mount a volume. That’s a lot to take in, so I’ve annotated it\\nin the book. Take a minute to read through the file and annotations.',\n",
              " '11: Deploying apps with Docker Stacks 166\\nweb-fe:\\nimage: nigelpoulton/ddd-book:swarm-app <<---- Create all replicas with this image\\ncommand: python app.py <<---- Run this command when each replica starts\\ndeploy:\\nreplicas: 4 <<---- Deploy 4 replicas\\nupdate_config: ------┐ When you perform an update,\\nparallelism: 2 | update 2 replicas at a time,\\ndelay: 10s | wait 10 seconds in between each set of two replicas\\nfailure_action: rollback ------┘ and perform a rollback if you encounter issues\\nplacement: ------┐\\nconstraints: | Only run replicas on worker nodes\\n- \\'node.role == worker\\' ------┘\\nrestart_policy: ------┐ Only restart replicas if\\ncondition: on-failure | they\\'ve failed (non-zero return code),\\ndelay: 5s | wait five seconds between each restart attempt,\\nmax_attempts: 3 | only try three restarts,\\nwindow: 120s ------┘ and give up trying after two minutes\\nnetworks:\\n- counter-net <<---- Connect replicas to the \"counter-net\" network\\nports:\\n- published: 5001 ------┐ Publish the service externally on port 5001 and\\ntarget: 8080 ------┘ map traffic to each replica on port 8080\\nvolumes:\\n- type: volume\\nsource: counter-vol ------┐ Mount the \"counter-vol\" volume to\\ntarget: /app ------┘ \"/app\" in each service replica\\nThe image key is the only mandatory key and tells Docker which image to use when\\ncreating service replicas. Swarm stacks don’t support building images at deploy time, so\\nthe image must exist before you deploy the app. Docker is alsoopinionated and assumes\\nyou want to pull images from Docker Hub. However, you can add the registry’s DNS\\nname before the image name if you want to use a third-party registry. For example,\\nadding ghcr.io before an image name will pull it from GitHub Container Registry.\\nThe command key tells Docker how to start the app in each replica. Our example tells it to\\nexecute python app.py in replicas.\\nThe deploy.replicas key tells Swarm to deploy and manage four service replicas\\n(identical containers).\\nweb-fe:\\ndeploy:\\nreplicas: 4\\nIf you need to change the number of replicas after you’ve deployed the service, you\\nshould update the value ofdeploy.replicas in the stack file and then redeploy the\\nstack. We’ll see this later.\\nThe deploy.update_config block tells Docker how to perform rollouts.',\n",
              " \"11: Deploying apps with Docker Stacks 167\\nweb-fe:\\ndeploy:\\nupdate_config:\\nparallelism: 2\\ndelay: 10s\\nfailure_action: rollback\\nThis application tells Docker to update two replicas at a time, wait 10 seconds between\\neach set of two, and perform a rollback if it encounters any problems.\\nRolling back replaces the new replicas with fresh copies of the old version. The default\\nvalue forfailure_action is pause, which will halt the update where it is and may result\\nin some replicas running the old version and some running the new. The other option is\\ncontinue.\\nThe deploy.placement block forces all replicas onto worker nodes. You’ll need to delete\\nthis section if you’re running a single-node cluster with one manager and zero workers.\\nweb-fe:\\ndeploy:\\nplacement:\\nconstraints:\\n- 'node.role == worker'\\nThe deploy.restart_policy block tells Docker what to do when it encounters a failure\\nduring a rollout.\\nweb-fe:\\ndeploy:\\nrestart_policy:\\ncondition: on-failure\\nmax_attempts: 3\\ndelay: 5s\\nwindow: 120s\\nThis app tells Docker to restart replicas if they fail, to attempt a maximum of 3 restarts,\\nto wait 5 seconds between each restart attempt, and to wait up to 120 seconds to decide\\nif the restart worked.\\nThe networks key tells Docker to attach all replicas to thecounter-net network.\\nweb-fe:\\nnetworks:\\n- counter-net\\nThe ports block publishes the app on port5001 on the ingress network and port8080\\non the counter-net network. This means external traffic can hit any swarm node on\\n5001 and get redirected to the service replicas on8080.\",\n",
              " '11: Deploying apps with Docker Stacks 168\\nweb-fe:\\nports:\\n- published: 5001\\ntarget: 8080\\nFinally, thevolumes block mounts thecounter-vol volume into each service replica’s\\n/app directory.\\nweb-fe:\\nvolumes:\\n- type: volume\\nsource: counter-vol\\ntarget: /app\\nThe redis service\\nThe redis service is much simpler. It pulls theredis:alpine image, starts a single\\nreplica, and attaches it to thecounter-net network. This is the same network as the\\nweb-fe service, meaning the two services will be able to communicate with each other\\nby name (“redis” and “web-fe”).\\nredis:\\nimage: \"redis:alpine\"\\nnetworks:\\ncounter-net:\\nAs previously mentioned, Compose files are a great source of application documenta-\\ntion. We can read this file and know the application has two services, two networks, and\\none volume. We know how the services communicate, how they’re published outside of\\nthe swarm, and a bit about how they’ll be deployed, updated, and restarted from failures.\\nLet’s deploy and manage the app.\\nDeploying the app\\nIf you plan on following this section, you’ll need a copy of the book’s GitHub repo and\\na swarm. The examples use a three-node swarm with a single manager calledmgr1 and\\ntwo workers calledwrk1 and wrk2.\\nDeploy the app\\nYou deploy Stacks with thedocker stack deploy command, and in its basic form it\\naccepts two arguments:',\n",
              " '11: Deploying apps with Docker Stacks 169\\n• The name of the stack file (Compose file)\\n• The name of the stack (app)\\nWe’ll use thecompose.yaml file in theddd-book/swarm-app folder, and we’ll call the app\\nddd.\\nRun all of the following commands from theswarm-app directory on a swarm manager.\\nDeploy the stack.\\n$ docker stack deploy -c compose.yaml ddd\\nCreating network ddd_counter-net\\nCreating service ddd_web-fe\\nCreating service ddd_redis\\nA few things to note from the output.\\nDocker creates the networks and volumes before services. This is because services\\nconsume networks and volumes, and will fail to start if they don’t exist.\\nDocker also prefixes the name of the stack to every resource. We called our stackddd,\\nmeaning Docker will prefix every resource name withddd:\\n• ddd_counter-net\\n• ddd_counter-vol\\n• ddd_web-fe\\n• ddd_redis\\nYou can check the status of services, volumes, and networks with their usual commands,\\nbut thedocker stack command also has a couple of options for checking stack status:\\n• docker stack ls prints a list of running stacks and how many services they have\\n• docker stack ps <stack-name> gives more detailed information about a specific\\nstack and its replicas\\nLet’s look at both.',\n",
              " '11: Deploying apps with Docker Stacks 170\\n$ docker stack ls\\nNAME SERVICES\\nddd 2\\n$ docker stack ps ddd\\nNAME IMAGE NODE DESIRED STATE CURRENT STATE\\nddd_redis.1 redis:alpine mgr1 Running Running 4 mins\\nddd_web-fe.1 nigelpoulton/ddd-book:swarm-app wrk1 Running Running 4 mins\\nddd_web-fe.2 nigelpoulton/ddd-book:swarm-app wrk2 Running Running 4 mins\\nddd_web-fe.3 nigelpoulton/ddd-book:swarm-app wrk2 Running Running 4 mins\\n<Snip>\\nddd_web-fe.10 nigelpoulton/ddd-book:swarm-app wrk1 Running Running 4 mins\\nThe docker stack ps command is great for troubleshooting services. You can see the\\nimage each replica is based on, which nodes replicas are running on, anddesired stateand\\ncurrent state.\\nThe following output shows two failed attempts to startweb-fe replicas on thewrk2\\nnode.\\n$ docker stack ps ddd\\nNAME NODE DESIRED CURRENT ERROR\\nweb-fe.1 wrk2 Shutdown Failed \"task: non-zero exit (1)\"\\n\\\\_web-fe.1 wrk2 Shutdown Failed \"task: non-zero exit (1)\"\\nYou can also use thedocker service logs command to inspect service logs. If you\\npass it the service name or ID, you’ll get the logs for all service replicas. If you pass it a\\nspecific replica name or ID, you’ll only get the logs for that replica.\\nThe following example shows the logs for all replicas in theddd_web-fe service.\\n$ docker service logs ddd_web-fe\\nddd_web-fe.9.i23puo71kq12@node2 | * Serving Flask app \\'app\\'\\nddd_web-fe.5.z4otpnjrvc58@node2 | * Debug mode: on\\n<Snip>\\nddd_web-fe.6.novrixi5iuxy@node2 | * Debug mode: on\\nddd_web-fe.6.novrixi5iuxy@node2 | * Debugger is active!\\nddd_web-fe.6.novrixi5iuxy@node2 | * Debugger PIN: 127-233-151\\nYou can follow logs (--follow), tail them (--tail), and you can sometimes get extra\\ndetails (--details).\\nSo far, you’ve used Docker commands to prove the stack is up and running. However,\\nyou can also test it with your browser.\\nRun adocker stack services command to discover the port theweb-fe service is\\npublished on.',\n",
              " '11: Deploying apps with Docker Stacks 171\\n$ docker stack services ddd\\nNAME MODE REPLICAS IMAGE PORTS\\nddd_redis replicated 1/1 redis:alpine\\nddd_web-fe replicated 4/4 nigelpoulton/ddd-book:swarm-app *:5001->8080/tcp\\nIt’s published on port5001, meaning you can point your browser to the name or IP\\nof any swarm node on5001. If you’re using Multipass, the node IPs usually start with\\n192.168, and you can find them by running amultipass list command from your\\nhost’s terminal. Alternatively, you can run anip a command inside a VM and use the\\nIP address of theenp0s1 interface.\\nFigure 11.3 shows a browser connecting to the app via a swarm node with the192.168.64.41\\nIP address.\\nFigure 11.3\\nLet’s switch tack and see how to manage swarm stacks.\\nManaging the app\\nThere are two ways to manage Docker stacks:\\n• Imperatively\\n• Declaratively\\nThe imperative methodis where you run Docker commands to make changes to the stack.\\nFor example, using thedocker service scale command to increase and decrease the\\nnumber of service replicas.',\n",
              " '11: Deploying apps with Docker Stacks 172\\nThe declarative methodis where you make all changes via the stack file. For example,\\nif you want to change the number of service replicas, you edit the stack file with the\\ndesired replica count and run anotherdocker stack deploy command.\\nThe declarative method is the preferred method.\\nConsider the following example that demonstrates why you should manage stacks\\ndeclaratively.\\nImagine you’ve deployed an app from a stack file that includes areporting service and acatalog\\nservice. The stack file includes other services that are part of the app, but we’re only interested\\nin these two. It’s currently running five replicas of the reporting service, but year-end reporting\\nhas started, and it’s experiencing slow performance due to increased demand. You decide to run\\nan imperativedocker service scale command to increase the number of reporting replicas\\nto 10. This fixes the performance issues, but the current state of the app is out of sync with the\\nstack file — the stack file defines five replicas but the cluster is running 10. Later in the day, a\\ncolleague is tasked with rolling out a new version of the catalog service — the catalog service is\\npart of the same app and, therefore, is defined in the same stack file as the reporting service. Your\\ncolleague makes the changes declaratively by editing the stack file with the new version of the\\ncatalog service and running adocker stack deploy command to push the updates. When\\nDocker receives the updated version of the stack file, it rolls out the new version of the catalog\\nservice and changes the number of reporting replicas back to five. This will cause the reporting\\nservice to start running slowly again.\\nThis is why you should make all changes declaratively via your stack files and manage\\nthem in a version control system.\\nGetting back to the app you deployed, let’s do the following:\\n• Increase the number ofweb-fe replicas from 4 to 10\\n• Update theweb-fe service to the newerswarm-appv2 image\\nEdit thecompose.yaml, increase theweb-fe replica count to 10, and change the image to\\nswarm-appv2.\\n<Snip>\\nservices:\\nweb-fe:\\nimage: nigelpoulton/ddd-book:swarm-appv2 <<---- changed to swarm-appv2\\ncommand: python app.py\\ndeploy:\\nreplicas: 10 <<---- Changed from 4 to 10\\n<Snip>\\nSave your changes and redeploy the app. This will cause Docker torollout a new version\\nof theweb-fe service with all 10 replicas running the new image.',\n",
              " '11: Deploying apps with Docker Stacks 173\\n$ docker stack deploy -c compose.yaml ddd\\nUpdating service ddd_redis (id: ozljsazuv7mmh14ep70pv43cf)\\nUpdating service ddd_web-fe (id: zbbplw0hul2gbr593mvwslz5i)\\nEven though it looks like Docker has redeployed both services, it hasn’t. It’s clever\\nenough only to redeploy the bits you’ve changed.\\nRun adocker stack ps to see the rollout’s progress.\\n$ docker stack ps ddd\\nNAME IMAGE NODE DESIRED CURRENT STATE\\nddd_web-fe.1 nigelpoulton/ddd-book:swarm-app node2 Running Running 8 mins ago\\nddd_web-fe.2 nigelpoulton/ddd-book:swarm-appv2 node2 Running Running 13 secs ago\\n\\\\_ddd_web-fe.2 nigelpoulton/ddd-book:swarm-app node2 Shutdown Shutdown 26 secs ago\\nddd_web-fe.3 nigelpoulton/ddd-book:swarm-app node1 Running Running 8 mins ago\\n<Snip>\\nI’ve trimmed the output to fit the book, and I’ve only listed some of the replicas.\\nHowever, you can see a few things.\\nThe top line shows theddd_web-fe.1 replica running the old image for the last 8\\nminutes.\\nThe next two lines show theddd_web-fe.2 replica. You can see the old copy was\\nrunning the old image, was shut down 26 seconds ago, and was replaced with a new\\nversion running the new image. The new replica has been running for 13 seconds.\\nThe last line shows theddd_web-fe.3 replica is still running the old version.\\nIt’s important to emphasize two things.\\nFirst, Docker followed the rules in thedeploy.update_config section of the Compose\\nfile. If you look at that section of the file again, you’ll see that Docker can update two\\nreplicas, wait for 10 seconds, update the next two, wait 10 seconds, and then attempt a\\nrollback to the previous version if it encounters any issues.\\nweb-fe:\\ndeploy:\\nupdate_config:\\nparallelism: 2\\ndelay: 10s\\nfailure_action: rollback\\nSecond, containers are immutable and Docker didn’tupdate the existing containers to\\nrun the new image. It deleted and replaced them with new containers running the new\\nimage.\\nAfter a short period, all 10 replicas will be running updated image.',\n",
              " '11: Deploying apps with Docker Stacks 174\\nRefresh your browser to make sure you see the updated version of the app with a purple\\nWebAssembly is the futurebanner.\\nSomething needs to be fixed because it still displays the old web page.\\nThe issue is with the volume. Let’s walk through what happened.\\nThe new image has the updated app version with the purpleWebAssembly is the future\\nbanner. The rollout deleted the old replicas and replaced them with new ones running\\nthe new version of the app. So far, so good. However, when Docker started each new\\nreplica, it mounted the old volume with the old app version. This had the effect of\\noverwriting the new version of the app with the old.\\nYou should be aware of this when working with volumes. However, it will usually be\\nfine because you typically only use volumes for data stores, not for hosting application\\nbinaries.\\nLet’s assume you realize theweb-fe service is stateless and doesn’t require a volume.\\nThe declarative way to remove the volume is to edit the Compose file again, remove the\\nvolume and volume mount, and redeploy the app. Let’s do it.\\nEdit thecompose.yaml file and make the following changes.\\nvolumes: <<---- Delete this line\\ncounter-vol: <<---- Delete this line\\n<Snip>\\nservices:\\nweb-fe:\\nimage: nigelpoulton/ddd-book:swarm-appv2\\n<Snip>\\nvolumes: <<---- Delete this line\\n- type: volume <<---- Delete this line\\nsource: counter-vol <<---- Delete this line\\ntarget: /app <<---- Delete this line\\nSave your changes and redeploy.\\n$ docker stack deploy -c compose.yaml ddd\\nUpdating service ddd_redis (id: ozljsazuv7mmh14ep70pv43cf)\\nUpdating service ddd_web-fe (id: zbbplw0hul2gbr593mvwslz5i)\\nThe stack will update two replicas at a time and wait 10 seconds between each. Once the\\nstack has converged and all replicas are updated, you should see the new version of the\\napp as shown in Figure 11.4. Refresh your browser a few times to make sure it works.\\nDon’t worry if some requests get the old version while the rollout is in progress.',\n",
              " '11: Deploying apps with Docker Stacks 175\\nFigure 11.4 - The updated app\\nThe volume will still exist, and you’ll need to delete it manually.\\nCongratulations. You’ve successfully deployed and managed a multi-container app using\\nDocker Stacks. You also learned to deploy and manage apps declaratively by making all\\nchanges via the stack file (Compose file).\\nClean up\\nIf you’ve been following along, you’ve deployed an app with two services, a network,\\nand a volume.\\nYou can delete the stack with thedocker stack rm command. However, be warned that\\nit deletes the stack without asking for confirmation.\\n$ docker stack rm ddd\\nRemoving service ddd_redis\\nRemoving service ddd_web-fe\\nRemoving network ddd_counter-net\\nThe command deleted the network and services but not the volume. This is because\\nvolume lifecycles are decoupled from containers and services, and you need to delete\\nthem manually.\\nRun the following command on every swarm node that hosted a replica.\\n$ docker volume rm ddd_counter-vol\\nddd_counter-vol',\n",
              " '11: Deploying apps with Docker Stacks 176\\nDeploying apps with Docker Stacks – The Commands\\n• docker stack deploy is the command you’ll run to deployand update stacks. You\\nneed to specify the name of the stack and the stack file. Docker expects the stack\\nfile to be calledcompose.yaml by default.docker stack ls lists all stacks on a\\nswarm and shows the number of services each one has.\\n• docker stack ps gives you detailed information about a stack. It tells you which\\nnodes replicas are running on, which images they’re based on, and shows the\\ndesired stateand current stateof each service replica.\\n• docker stack rm deletes a stack and doesn’t ask for confirmation.\\nChapter Summary\\nStacks are Docker’s native solution for running cloud-native microservices applications\\non Swarm clusters. They offer a simple declarative interface for managing the entire\\nlifecycle of applications and infrastructure.\\nYou start with application code and infrastructure requirements — things like networks,\\nports, volumes, and secrets. You containerize the application and combine all the\\nservices and infrastructure definitions into a single declarative stack file. You set the\\nnumber of replicas, as well as rollout and restart policies. Then, you use thedocker\\nstack deploy command to deploy the application from the stack file.\\nYou should also perform all updates declaratively by updating the stack file and rede-\\nploying the app.',\n",
              " '12: Docker and WebAssembly\\nWebAssembly is driving the third of cloud computing, and Docker is evolving to take\\nadvantage.\\nWe built the first wave on virtual machines (VMs), the second on containers, and we’re\\nbuilding the third on WebAssembly. Each wave is driving smaller, faster, and more\\nsecure workloads, and all three will work together in the future of cloud computing.\\nIn this chapter, you’ll write a simple WebAssembly application and use Docker to\\ncontainerize and run it in a container. The goal is to introduce you to WebAssembly and\\nshow you how easy it is to work with Docker and WebAssembly together.\\nThe termsWebAssembly and Wasm mean the same thing, and we’ll use the term Wasm.\\nI’ve divided the chapter as follows:\\n• Pre-reqs\\n• Intro to Wasm\\n• Write a Wasm app\\n• Containerize a Wasm app\\n• Deploy a Wasm app',\n",
              " '12: Docker and WebAssembly 178\\nPre-reqs\\nYou’ll need all of the following if you plan on following along:\\n• Docker Desktop4.30+ with Wasm enabled\\n• Rust 1.72+ with the Wasm target installed\\n• Spin 2.5+\\nAt the time of writing, support for Wasm is a beta feature in Docker Desktop and doesn’t\\nwork with Multipass Docker VMs. This may change in the future. It also means there’s a\\nhigher risk of bugs. I’ve tested the examples in this chapter on Docker Desktop 4.30.0.\\nConfigure Docker Desktop for Wasm\\nOpen the Docker Desktop UI, click the Settings icon at the top right, and make sure\\nUse containerd for pulling and storing imagesis selected. Next, click theFeatures in\\ndevelopment tab, select theEnable Wasmoption and click the blueApply & restart\\nbutton.\\nSome of the settings are shown in Figure 12.2.',\n",
              " \"12: Docker and WebAssembly 179\\nFigure 12.2. Docker Desktop Wasm settings\\nInstall Rust and configure for Wasm\\nSearch the web forhow to install Rustand follow the instructions for your platform.\\nOnce you’ve installed Rust, run the following command to install thewasm32-wasi\\ntarget so that Rust can compile to Wasm.\\n$ rustup target add wasm32-wasi\\ninfo: downloading component 'rust-std' for 'wasm32-wasi'\\ninfo: installing component 'rust-std' for 'wasm32-wasi'\\nInstall Spin\\nSpin is a Wasm framework and runtime that makes building and running Wasm apps\\neasy.\\nSearch the web forhow to install Fermyon spinand follow the instructions for your\\nsystem.\",\n",
              " '12: Docker and WebAssembly 180\\nRun the following command to ensure it’s installed correctly.\\n$ spin --version\\nspin 2.5.0 (83eb68d 2024-05-08)\\nYou’re now ready to build and run Wasm apps on your local machine.\\nIntro to Wasm and Wasm containers\\nWasm is a new type of application that is smaller, faster, and more portable than tradi-\\ntional Linux containers. However, traditional Linux containers can do a lot more than\\nWasm apps. For example, Wasm apps are currently great for AI workloads, serverless\\nfunctions, plugins, and edge devices but not so good for complex networking or heavy\\nI/O.\\nHowever, Wasm is evolving fast and may become better at other workloads in the\\nfuture.\\nDigging a little deeper…\\nAs we’re about to see, Wasm is a new virtual machine architecture that programming\\nlanguages compile to. So, instead of compiling apps toLinux on ARMor Linux on AMD,\\nyou compile them toWasm. You can then run these Wasm apps on any system with a\\nWasm runtime. Fortunately, Docker Desktop already ships with several Wasm runtimes.\\nRun the following command to see the list of Wasm runtimes installed as part of your\\nDocker Desktop environment. The first time you run the command, it will download\\nthe image.\\n$ docker run --rm -i --privileged --pid=host jorgeprendes420/docker-desktop-shim-manager:latest\\nio.containerd.wasmtime.v1\\nio.containerd.spin.v2\\nio.containerd.wws.v1\\nio.containerd.lunatic.v1\\nio.containerd.slight.v1\\nio.containerd.wasmedge.v1\\nio.containerd.wasmer.v1\\nMy installation has seven Wasm runtimes, including theio.containerd.spin.v2\\nruntime we’ll use in the examples.\\nThese Wasm runtimes allowcontainerd to deploy and manageWasm containers. AWasm\\ncontainer is a Wasm binary running inside a minimalscratch containerso that you can\\nbuild, ship, and run them with familiar Docker tools such as thedocker run command\\nand Docker Hub.\\nTalk is cheap though, let’s see it in action.',\n",
              " '12: Docker and WebAssembly 181\\nWrite a Wasm app\\nWe’ll usespin to create a simple web server that we’ll compile as a Wasm app. In future\\nsteps, you’ll build, ship, and run the app as a Wasm container.\\nChange into a new directory and then run the following command to create a new\\nWasm app calledhello-world. Respond to the prompts as shown in the example.\\n$ spin new hello-world -t http-rust\\nDescription: Wasm app\\nHTTP path: /hello\\nThe command creates a newhello-world directory and scaffolds a simple Rust-based\\nweb app. Change into this directory and inspect the app files. If you don’t have thetree\\ncommand, you can run anls -l for similar results.\\n$ cd hello-world\\n$ tree\\n.\\n├── Cargo.toml\\n├── spin.toml\\n└── src\\n└── lib.rs\\nWe’re only interested in thespin.toml and src/lib.rs files.\\nEdit thesrc/lib.rs file and change the text inside the double quotes as shown in the\\nfollowing snippet. This configures the app to displayDocker loves Wasm.\\nuse spin_sdk::http::{IntoResponse, Request};\\n<Snip>\\nOk(http::Response::builder()\\n.status(200)\\n.header(\"content-type\", \"text/plain\")\\n.body(\"Docker loves Wasm\")?) <<---- Change text inside quotes\\n}\\nOnce you’ve saved your changes, run aspin build command to compile the app as a\\nWasm binary.',\n",
              " '12: Docker and WebAssembly 182\\n$ spin build\\nBuilding component with `cargo build --target wasm32-wasi --release`\\n<Snip>\\nFinished building all Spin components\\nIf you look at the first line of the output, you’ll see it’s running a more complexcargo\\nbuild command that compiles the app as a Wasm binary.\\nRun anothertree command to see the Wasm binary.\\n$ tree\\n<Snip>\\n└── target\\n└── wasm32-wasi\\n└── release\\n└── hello_world.wasm\\nThe output is much longer this time, and I’ve trimmed the example in the book so you\\nonly see thehello_world.wasm binary. This is the Wasm app and will run on any system\\nwith thespin Wasm runtime.\\nYou’ll containerize the app in the next section, but you should test it works before\\nproceeding.\\nRun aspin up command to start a local instance of the app. This will start the app using\\nthe localspin runtime you installed earlier.\\n$ spin up\\nLogging component stdio to \".spin/logs/\"\\nServing http://127.0.0.1:3000\\nAvailable Routes:\\nhello-world: http://127.0.0.1:3000/hello\\nPoint your browser tohttp://127.0.0.1:3000/hello and make sure the app works.',\n",
              " '12: Docker and WebAssembly 183\\nFigure 12.3. Wasm app running locally\\nCongratulations. You just built a simple web server, compiled it to Wasm, and executed\\nit locally usingspin! In the next section, you’ll containerize it.\\nPress Ctrl-C to kill the app.\\nContainerize a Wasm app\\nDocker Desktop lets you containerize Wasm apps so you can use familiar Docker tools\\nto push and pull them to OCI registries and run them inside containers.\\nAs always, you need a Dockerfile that tells Docker how to package the app as an image.\\nCreate a new file calledDockerfile in your current directory and populate it with the\\nfollowing three lines.\\nFROM scratch\\nCOPY /target/wasm32-wasi/release/hello_world.wasm .\\nCOPY spin.toml .\\nThe file references a special empty base image calledscratch because Wasm containers\\ndon’t need a Linux OS.\\nThe twoCOPY instructions copy thehello_world.wasm Wasm app and thespin.toml\\nfile into the image.\\nIf you look closely, you’ll see that thespin.toml file expects the Wasm app to be in the\\ntarget/wasm32-wasi/release/ directory. However, the secondCOPY instruction places',\n",
              " '12: Docker and WebAssembly 184\\nit in the root folder. This means we’ll need to update thespin.toml file so it knows\\nwhere to find the app after it’s copied into the root of the image.\\nEdit thespin.toml file and remove the leading path for thesource line as shown.\\n<Snip>\\n[component.hello-world]\\nsource = \"hello_world.wasm\" <<---- Remove any leading directories so it looks like this\\n<Snip>\\nSave your changes.\\nRun the following command to containerize the Wasm app. Be sure to tag the image\\nwith your own Docker Hub username instead of mine.\\n$ docker build \\\\\\n--platform wasi/wasm \\\\\\n--provenance=false \\\\\\n-t nigelpoulton/ddd-book:wasm .\\nThe --platform wasi/wasm flag sets the image as a Wasm image.\\nSome older versions of Docker have an older builder and will fail. If this happens, try\\nrunning the same command, but change the first line todocker buildx build \\\\.\\nList the images on your system.\\n$ docker images\\nREPOSITORY TAG IMAGE ID CREATED SIZE\\nnigelpoulton/ddd-book wasm 7b55889f1006 2 minutes ago 620kB\\nSee how the Wasm image looks like a regular image, just smaller.\\nYou can push and pull the image to Docker Hub and other OCI registries as normal.\\nThe following command pushes the image to one of my repos in Docker Hub. Be sure to\\ntag the image with your own Docker username.\\n$ docker push nigelpoulton/ddd-book:wasm\\nThe push refers to repository [docker.io/nigelpoulton/ddd-book]\\n301823195c36: Pushed\\n8966226af76a: Pushed\\nwasm: digest: sha256:7b55889f1006285ed6c394dcc7a56aca8955c107587b2216340e592299b8ae4c size: 695\\nIf you look at Docker Hub, you can see Docker Hub has recognized it as awasi/wasm\\nimage. You’ll also see there’s no vulnerability analysis data. This is because image\\nscanning tools can’t analyze Wasm images yet.',\n",
              " '12: Docker and WebAssembly 185\\nFigure 12.4. Wasm image on Docker Hub\\nRun a Wasm container\\nNow that you’ve packaged the Wasm app as an OCI image and pushed it to a registry,\\nyou can run it as a container.\\nThe following command runs it in a new container calledwasm-ctr and maps it to\\nport 5556 on your Docker host. The--runtime flag makes sure Docker executes the\\ncontainer with thespin Wasm runtime. Versions of Docker Desktop older than 4.24\\nmay not have thespin runtime and will fail.\\n$ docker run -d --name wasm-ctr \\\\\\n--runtime=io.containerd.spin.v2 \\\\\\n--platform=wasi/wasm \\\\\\n-p 5556:80 \\\\\\nnigelpoulton/ddd-book:wasm /\\nYou can check it’s running with a regulardocker ps command.\\nConnect your browser tohttp://localhost:5556/hello to see the app.',\n",
              " '12: Docker and WebAssembly 186\\nFigure 12.5. Wasm app running in container\\nCongratulations, your Wasm app is running inside a Wasm container.\\nClean up\\nRun the following commands to delete the container and the local image.\\n$ docker rm wasm-ctr -f\\nwasm-ctr\\n$ docker rmi nigelpoulton/ddd-book:wasm\\nUntagged: nigelpoulton/ddd-book:wasm\\nDeleted: sha256:7b55889f1006285ed6c394dcc7a56aca8955c107587b2216340e592299b8ae4c\\nYou’ll still have a copy of the image on Docker Hub and the spin app in your local\\nfilesystem. Feel free to delete these as well.\\nChapter summary\\nIn this chapter, you containerized a Wasm app and ran it in a Wasm container.',\n",
              " '12: Docker and WebAssembly 187\\nWasm is a new technology driving the next wave of cloud computing. Wasm apps\\nare smaller, faster, more secure, and more portable than traditional Linux containers.\\nHowever, they’re not as flexible. For example, at the time of writing, Wasm apps aren’t\\ngreat for apps with heavy I/O requirements or complex networking. This will change\\nquickly as the Wasm ecosystem is evolving fast.\\nFortunately, Docker already works with Wasm, and Docker Desktop ships with lots\\nof popular Wasm runtimes. This means you can use industry-standard tools such as\\ndocker build and docker run to containerize and run Wasm apps. You can even push\\nthem to OCI registries such as Docker Hub.',\n",
              " '13: Docker Networking\\nIt’s always the network!\\nAny time we experience infrastructure issues, we always blame the network. One of the\\nreasons we do this is that networks are at the center of everything. With this in mind, it’s\\nimportant you have a strong understanding of Docker networking.\\nIn the early days of Docker, networking was hard. Fortunately, these days it’s almost a\\npleasure ;-)\\nThis chapter will get you up to speed with the fundamentals of Docker networking.\\nYou’ll learn all the theory behind theContainer Network Model (CNM)and libnetwork, and\\nyou’ll get your hands dirty with lots of examples. You’ll learn about overlay networks in\\nthe next chapter.\\nI’ve divided the chapter into the following sections:\\n• Docker networking – the TLDR\\n• Docker networking theory\\n• Single-host bridge networks\\n• External access via port mappings\\n• Connecting to existing networks and VLANs\\n• Service Discovery\\n• Ingress load balancing\\nA few quick things before we start.\\nEverything we’ll cover relates to Linux containers, and I recommend you follow along\\nusing something like Multipass or Play with Docker, as they give you easy access to\\nsome of the Linux commands we’ll use. I don’t recommend following along on Docker\\nDesktop as it runs everything inside a Linux VM and you won’t have access to the Linux\\ncommands.\\nSome of the examples explain how networking works on a swarm. You’ll only be able to\\nfollow these if you’re following along with a Swarm cluster.',\n",
              " '13: Docker Networking 189\\nDocker Networking – The TLDR\\nDocker runs microservices applications comprised of many containers that work\\ntogether to form the overall app. These containers need to be able to communicate,\\nand some will have to connect with external services, such as physical servers, virtual\\nmachines, or something else.\\nFortunately, Docker has solutions for both of these requirements.\\nDocker networking is based onlibnetwork, which is the reference implementation of an\\nopen-source architecture called theContainer Network Model (CNM).\\nFor a smooth out-of-the-box experience, Docker ships with everything you need\\nfor the most common networking requirements, including multi-host container-to-\\ncontainer networks and options for plugging into existing VLANs. However, the model\\nis pluggable, and the ecosystem can extend Docker’s networking capabilities via drivers\\nthat plug into libnetwork.\\nLast but not least, libnetwork also provides native service discovery and basic load\\nbalancing.\\nThat’s the big picture. Let’s get into the detail.\\nDocker networking theory\\nAt the highest level, Docker networking is based on the following three components:\\n• The Container Network Model (CNM)\\n• Libnetwork\\n• Drivers\\nThe CNMis the design specification and outlines the fundamental building blocks of a\\nDocker network.\\nLibnetwork is a real-world implementation of the CNM. It’s open-sourced as part of\\nthe Moby project18 and used by Docker and other platforms.\\nDrivers extend the model by implementing specific network topologies such as VXLAN\\noverlay networks.\\nFigure 13.1 shows all three.\\n18https://mobyproject.org/',\n",
              " '13: Docker Networking 190\\nFigure 13.1\\nLet’s take a closer look at each.\\nThe Container Network Model (CNM)\\nEverything starts with a design.\\nThe design guide for Docker networking is the CNM that outlines the fundamental\\nbuilding blocks of a Docker network.\\nI recommend you read thespecification document19, but at a high level, it defines three\\nbuilding blocks:\\n• Sandboxes\\n• Endpoints\\n• Networks\\nA sandbox is an isolated network stack inside a container. It includes Ethernet interfaces,\\nports, routing tables, DNS configuration, and everything else you’d expect from a\\nnetwork stack.\\nEndpoints are virtual network interfaces that look, smell, and feel like regular network\\ninterfaces. They connect sandboxes to networks.\\nNetworks are virtual switches (usually software implementations of an 802.1d bridge). As\\nsuch, they group together and isolate one or more endpoints that need to communicate.\\nFigure 13.2 shows how all three connect and relate to familiar infrastructure compo-\\nnents. Using CNM terminology,endpoints connect sandboxes to networks. Every container\\nyou create will have a sandbox with at least one endpoint connecting it to a network.\\n19https://github.com/moby/moby/blob/master/libnetwork/docs/design.md',\n",
              " '13: Docker Networking 191\\nFigure 13.2 The Container Network Model (CNM)\\nAs the name suggests, theContainer Network Model is all about providing networking\\nfor containers. Figure 13.3 shows how CNM components relate to containers —\\neach container gets its ownsandbox which hosts the container’s entire network stack,\\nincluding one or more endpoints that act as Ethernet interfaces and can be connected to\\nnetworks.\\nFigure 13.3\\nContainer Ahas a single interface (endpoint) and is only connected toNetwork A.\\nHowever, Container Bhas two interfaces connected toNetwork Aand Network B.\\nThe containers can communicate with each other because they are both connected to\\nNetwork A. However, the two endpoints inside ofContainer Bcannot communicate\\nwith each other as they’re on different networks.\\nIt’s also important to understand that endpoints behave exactly like regular network\\nadapters, meaning you can only connect them to a single network. This is whyCon-\\ntainer Bneeds two endpoints if it wants to connect to both networks.\\nFigure 13.4 extends the diagram further by adding the Docker host. Even though both\\ncontainers are running on the same host this time, their network stacks are completely\\nisolated and can only communicate via a network.',\n",
              " '13: Docker Networking 192\\nFigure 13.4\\nLibnetwork\\nLibnetwork is the reference implementation of the CNM. It’s open-source, cross-\\nplatform (Linux and Windows), maintained by the Moby project, and used by Docker.\\nBefore Docker created libnetwork, it implemented all of its networking code inside\\nthe daemon. However, over time, the daemon became bloated and difficult for other\\nprojects to use. As a result, Docker removed the networking code from the daemon and\\nrefactored it as an external library calledlibnetwork based on the CNM design. Today,\\nDocker implements all of its core networking in libnetwork.\\nAs well as implementing the core components of the CNM, libnetwork also implements\\nthe network control plane, including management APIs, service discovery, and ingress-\\nbased container load balancing.\\nDrivers\\nLibnetwork implements the control plane, but it relies on drivers to implement the data\\nplane. For example, drivers are responsible for creating networks and ensuring isolation\\nand connectivity.\\nDocker ships with several built-in drivers that we sometimes callnative driversor local\\ndrivers. These includebridge, overlay, andmacvlan, and they build the most common\\nnetwork topologies. Third parties can also write network drivers to implement other\\nnetwork topologies and more advanced configurations.',\n",
              " '13: Docker Networking 193\\nFigure 13.5 shows the roles of libnetwork and drivers and how they relate to control\\nplane and data place responsibilities.\\nFigure 13.5\\nEvery network you create is owned by a driver, and the driver creates and manages\\neverything about the network. For example, if you create an overlay network called\\nprod-fe-cuda, Docker will invoke the overlay driver to create the network and its\\nresources.\\nTo meet the demands of complex, highly fluid environments, a single Docker host or\\nSwarm cluster can have multiple heterogeneous networks managed by different drivers.\\nLet’s look at single-host bridge networks and connecting to existing networks. You’ll\\nlearn about overlay networks in the next chapter.\\nSingle-host bridge networks\\nThe simplest type of Docker network is thesingle-host bridge network.\\nThe name tells us two things:\\n• Single-host tells us the network only spans a single Docker host\\n• Bridge tells us that it’s an implementation of an 802.1d bridge (layer 2 switch)\\nDocker creates single-host bridge networks with the built-inbridge driver. If you run\\nWindows containersyou’ll need to use thenat driver, but for all intents and purposes they\\nwork the same.\\nFigure 13.6 shows two Docker hosts with identical local bridge networks, both called\\nmynet. Even though the networks are identical, they are independent and isolated,',\n",
              " '13: Docker Networking 194\\nmeaning the containers in the picture cannot communicate, even if the nodes are part\\nof the same swarm.\\nFigure 13.6\\nEvery new Docker host gets a default single-host bridge network calledbridge that\\nDocker connects new containers to unless you override it with the--network flag.\\nThe following commands show the output of adocker network ls command on\\nDocker installation.\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\nc7464dce29ce bridge bridge local <<---- Default on all Docker hosts\\nc65ab18d0580 host host local\\n42a783df0fbe none null local\\nAs always, you can rundocker inspect commands to get more information. I highly\\nrecommend running the command on your own system and studying the output.\\n$ docker network inspect bridge\\n[\\n{\\n\"Name\": \"bridge\",\\n\"Id\": \"c7464dce2...ba2e3b8\",\\n\"Scope\": \"local\",\\n\"Driver\": \"bridge\",\\n\"EnableIPv6\": false,\\n\"IPAM\": {\\n\"Driver\": \"default\",\\n\"Options\": null,\\n\"Config\": [\\n{\\n\"Subnet\": \"172.17.0.0/16\",\\n\"Gateway\": \"172.17.0.1\"\\n}',\n",
              " '13: Docker Networking 195\\n]\\n},\\n\"Internal\": false,\\n\"Attachable\": false,\\n\"Ingress\": false,\\n\"ConfigFrom\": {\\n\"Network\": \"\"\\n},\\n<Snip>\\n}\\n]\\nAll bridge networks are based on the battle-hardenedLinux bridgetechnology that has\\nexisted in the Linux kernel for over 20 years. This means they’re high-performance and\\nhighly stable. It also means you can inspect them using standard Linux utilities.\\nThe defaultbridge network on all Linux-based Docker hosts is calledbridge and maps to\\nan underlyingLinux bridgein the host’s kernel calleddocker0. This is shown in Figure\\n13.7.\\nFigure 13.7 - Mapping the default Docker “bridge” network to the “docker0” bridge in the host’s kernel\\nYou can run adocker network inspect command to confirm that thebridge network\\nis based on thedocker0 bridge in the host’s kernel. If you’re on Windows using Power-\\nShell, you’ll need to replace\\x18grep** with **SelectString‘.\\n$ docker network inspect bridge | grep bridge.name\\n\"com.docker.network.bridge.name\": \"docker0\",\\nNow run these Linux commands to inspect thedocker 0bridge from the Linux host.\\nYou might need to manually install thebrctl utility.',\n",
              " '13: Docker Networking 196\\n$ brctl show\\nbridge name bridge id STP enabled interfaces\\ndocker0 8000.0242aff9eb4f no\\ndocker_gwbridge 8000.02427abba76b no\\n$ ip link show docker0\\n3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc...\\nlink/ether 02:42:af:f9:eb:4f brd ff:ff:ff:ff:ff:ff\\nThe first command lists all thebridges on your Docker host and shows if they have any\\ndevices connected to them. The example in the book shows thedocker0 bridge with no\\ndevices connected in theinterfaces column. You’ll only see thedocker_gwbridge if\\nyour host is a member of a swarm cluster.\\nThe second command shows the configuration and state of thedocker0 bridge.\\nFigure 13.8 shows the complete stack with containers connecting to thebridge\\nnetwork, which, in turn, maps to thedocker0 Linux bridge in the host’s kernel. It also\\nshows how you can use port mappings to publish connected devices on the Docker\\nhost’s interface. More on port mappings later.\\nFigure 13.8\\nIn the next few steps, you’ll complete all of the following:\\n1. Create a new Docker bridge network\\n2. Connect a container to the new network',\n",
              " '13: Docker Networking 197\\n3. Inspect the new network\\n4. Test name-based discovery\\nRun the following command to create a new single-host bridge network calledlocal-\\nnet.\\n$ docker network create -d bridge localnet\\nf918f1bb0602373bf949615d99cb2bbbef14ede935fbb2ff8e83c74f10e4b986\\nThe long number returned by the command is the network’s ID and you’ll need it in the\\nnext step.\\nAs expected, the command creates a new Docker bridge network calledlocalnet that\\nyou can list and inspect with the usualdocker commands. However, behind the scenes,\\nit also creates a new Linux bridge in the host’s kernel.\\nRun anotherbrctl show command to see it.\\n$ brctl show\\nbridge name bridge id STP enabled interfaces\\nbr-f918f1bb0602 8000.0242372a886b no\\ndocker0 8000.024258ee84bc no\\ndocker_gwbridge 8000.02427abba76b no\\nThe example in the book shows a new bridge calledbr-f918f1bb0602 with no devices\\nconnected. If you look closely at the name, you’ll recognizef918f1bb0602 as the first 12\\ncharacters from the ID of the newlocalnet network you just created.\\nAt this point, the bridge configuration on the host looks like Figure 13.9, with three\\nDocker networks and three associated bridges in the host’s kernel.\\nFigure 13.9\\nLet’s create a new container calledc1 and attach it to the newlocalnet bridge network.',\n",
              " '13: Docker Networking 198\\n$ docker run -d --name c1 \\\\\\n--network localnet \\\\\\nalpine sleep 1d\\nOnce you’ve created the container, inspect thelocalnet network and verify the\\ncontainer is connected to it. You’ll need thejq utility installed for the command to work.\\nLeave off the\"| jq\" if it doesn’t work.\\n$ docker network inspect localnet --format \\'{{json .Containers}}\\' | jq\\n{\\n\"09c5f4926c87da12039b3b510a5950b3fe9db80e13431dc17d870450a45fd84a\": {\\n\"Name\": \"c1\",\\n\"EndpointID\": \"27770ac305773b352d716690fb9f8e05c1b71e10dc66f67b88e93cb923ab9749\",\\n\"MacAddress\": \"02:42:ac:15:00:02\",\\n\"IPv4Address\": \"172.21.0.2/16\",\\n\"IPv6Address\": \"\"\\n}\\n}\\nThe output shows thec1 container and its IP address. This proves Docker connected it\\nto the network.\\nIf you run anotherbrctl show command, you’ll see thec1 container’s interface\\nconnected to thebr-1597657726bc bridge.\\n$ brctl show\\nbridge name bridge id STP enabled interfaces\\nbr-f918f1bb0602 8000.0242372a886b no veth833aaf9\\ndocker0 8000.024258ee84bc no\\ndocker_gwbridge 8000.02427abba76b no\\nFigure 13.10 shows the updated configuration. Your veth IDs will be different, but the\\nimportant thing to understand is that every veth is like a cable with an interface on\\neither end. One end is connected to the Docker network, and the other end is connected\\nto the associated bridge in the kernel.',\n",
              " '13: Docker Networking 199\\nFigure 13.10\\nIf you add more containers to thelocalnet network, they’ll all be able to communicate\\nusing names. This is because Docker automatically registers container names with an\\ninternal DNS service and allows containers on the same network to find each other by\\nname. The exception to this rule is the built-inbridge network that does not support\\nDNS resolution.\\nLet’s test name resolution by creating a new container calledc2 on the samelocalnet\\nnetwork and seeing if it can ping thec1 container.\\nRun the following command to create thec2 container on thelocalnet network. You’ll\\nneed to typeexit if you’re still logged in to thec1 container.\\n$ docker run -it --name c2 \\\\\\n--network localnet \\\\\\nalpine sh\\nYour terminal will switch into thec2 container.\\nTry to ping thec1 container by name.\\n# ping c1\\nPING c1 (172.21.0.2): 56 data bytes\\n64 bytes from 172.21.0.2: seq=0 ttl=64 time=1.564 ms\\n64 bytes from 172.21.0.2: seq=1 ttl=64 time=0.338 ms\\n64 bytes from 172.21.0.2: seq=2 ttl=64 time=0.248 ms\\n<Control-c>\\nIt works! This is because all containers run a DNS resolver that forwards name lookups\\nto Docker’s internal DNS server that holds name-to-IP mappings for all containers\\nstarted with the--name or --net-alias flag.\\nType exit to log out of the container and return to your local shell.',\n",
              " '13: Docker Networking 200\\nExternal access via port mappings\\nSo far, we’ve said that containers on bridge networks can only communicate with\\nother containers on the same network. However, you can get around this by mapping\\ncontainers to ports on the Docker host. It’s a bit clunky and has a lot of limitations, but it\\nmight be useful for occasional testing and development work.\\nFigure 13.11 shows a single Docker host running two containers. Theweb container on\\nthe right is running a web server on port80 that is mapped to port5005 on the Docker\\nhost. Theclient container on the left is sending requests to the Docker host on port\\n5005 and the external client at the bottom is doing the same. Both requests will hit the\\nhost’s IP on port5005 and be redirected to the web server running in theweb container.\\nFigure 13.11\\nLet’s test the setup to see if it works.\\nCreate a new container calledweb running NGINX on port80 and map it to port5005\\non the Docker host. If you’re still logged on to the container from the previous example,\\nyou’ll need to typeexit first.',\n",
              " '13: Docker Networking 201\\n$ docker run -d --name web \\\\\\n--network localnet \\\\\\n--publish 5005:80 \\\\\\nnginx\\nVerify the port mapping.\\n$ docker port web\\n80/tcp -> 0.0.0.0:5005\\n80/tcp -> [::]:5005\\nThe output shows the port mapping exists on all interfaces on the Docker host.\\nYou can test external access by pointing a web browser to the Docker host on port5005.\\nYou’ll need to know the IP or DNS name of your Docker host (if you’re following along\\non Multipass it will probably be your Multipass VM’s192.168.x.x address). You’ll see\\nthe Welcome to nginx!page.\\nLet’s create another container and see if it can reach the web container via the port\\nmapping.\\nRun the following command to create a new container calledclient on thebridge\\nnetwork.\\n$ docker run -it --name client --network bridge alpine sh\\n#\\nThe command will log you into the container and your prompt will change.\\nInstall thecurl utility.\\n# apk add curl\\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.19/main/aarch64/APKINDEX.tar.gz\\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.19/community/aarch64/APKINDEX.tar.gz\\n(1/8) Installing ca-certificates (20240226-r0)\\n<Snip>\\nNow connect to the IP of your Docker host on port5005 to see if you can reach the\\ncontainer.',\n",
              " '13: Docker Networking 202\\n# curl 192.168.64.69:5005\\n<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Welcome to nginx!</title>\\n...\\n</html>\\nYou’ve reached the NGINX web server running on thec1 container via a port mapping\\nto the Docker host’s IP.\\nEven though this works, it’s clunky and doesn’t scale. For example, no other containers\\nor host processes will be able to use port5005 on the host. This is one of the reasons\\nthat single-host bridge networks are only useful for local development or very small\\napplications.\\nConnecting to existing networks and VLANs\\nThe ability to connect containerized apps to external systems and physical networks is\\nimportant. A common example is partially containerized apps where the parts running\\nin containers need to be able to communicate with the parts not running in containers.\\nThe built-inMACVLAN driver (transparent if you’re using Windows containers) was\\ncreated with this in mind. It gives every container its own IP and MAC address on the\\nexternal physical network, making each one look, smell, and feel like a physical server or\\nVM. This is shown in Figure 13.12.\\nFigure 13.12 - MACVLAN driver making containers visible on external networks\\nOn the positive side, MACVLAN performance is good as it doesn’t require port\\nmappings or additional bridges. However, you need to run your host NICs inpromis-\\ncuous mode, which isn’t allowed on many corporate networks and public clouds. So,\\nMACVLAN will work on your data center networksif your network team allows\\npromiscuous mode, but it probably won’t work on your public cloud.',\n",
              " '13: Docker Networking 203\\nLet’s dig a bit deeper with the help of some pictures and a hypothetical example. This\\nexample will only work if your host NIC is in promiscuous mode on a network that\\nallows it. It also requires an existing VLAN 100. You can adapt it if the VLAN config on\\nyour physical network is different. You can follow along without the VLANs, but you\\nwon’t get the full experience.\\nAssume you have the network shown in Figure 13.13 with two VLANs:\\nFigure 13.13\\nNext, you add a Docker host and connect it to the network.\\nFigure 13.14\\nNow comes the requirement to attach a container to VLAN 100. To do this, you create a\\nnew Docker network with themacvlan driver and configure it with all of the following:\\n• Subnet info\\n• Gateway\\n• Range of IPs it can assign to containers\\n• Which of the host’s interfaces or sub-interfaces to use\\nRun the following command to create a new MACVLAN network calledmacvlan100\\nthat will connect containers to VLAN 100. You may need to change the name of the',\n",
              " '13: Docker Networking 204\\nparent interface to match the parent interface name on your system. For example,\\nchanging -o parent=eth0.100 to -o parent=enp0s1.100. The parent interface must\\nbe connected to the VLAN, and you’ll need to typeexit if you’re still logged on to the\\ncontainer from the previous example.\\n$ docker network create -d macvlan \\\\\\n--subnet=10.0.0.0/24 \\\\\\n--ip-range=10.0.0.0/25 \\\\\\n--gateway=10.0.0.1 \\\\\\n-o parent=eth0.100 \\\\ <<---- Make sure this matches your system\\nmacvlan100\\nDocker will create themacvlan100 network and a new sub-interface on the host called\\neth0.100@eth0. The config now looks like this.\\nFigure 13.15\\nThe MACVLAN driver creates standard Linuxsub-interfaces and tags them with the ID\\nof the VLAN they will connect to. In this example, we’re connecting to VLAN 100, so we\\ntag the sub-interface with.100 (-o parent=eth0.100).\\nWe also used the--ip-range flag to tell the new network which sub-set of IP addresses\\nit can assign to containers. It’s vital that you reserve this range of addresses for Docker,',\n",
              " '13: Docker Networking 205\\nas the MACVLAN driver has no management plane feature to check if IPs are already in\\nuse.\\nIf you inspect the network, you’ll be able to see the important configuration information.\\nI’ve snipped the output to show the most relevant parts.\\n$ docker network inspect macvlan100\\n[\\n{\\n\"Name\": \"macvlan100\",\\n\"Driver\": \"macvlan\",\\n\"IPAM\": {\\n\"Config\": [\\n{\\n\"Subnet\": \"10.0.0.0/24\",\\n\"IPRange\": \"10.0.0.0/25\",\\n\"Gateway\": \"10.0.0.1\"\\n}\\n]\\n},\\n\"Options\": {\\n\"parent\": \"enp0s1.100\"\\n},\\n}\\n]\\nOnce you’ve created themacvlan100 network, you can connect containers to it and\\nDocker will assign the IP and MAC addresses on the underlying VLAN so they’ll be\\nvisible to other systems.\\nThe following command creates a new container calledmactainer1 and connects it to\\nthe macvlan100 network.\\n$ docker run -d --name mactainer1 \\\\\\n--network macvlan100 \\\\\\nalpine sleep 1d\\nThe config now looks like Figure 13.16.',\n",
              " '13: Docker Networking 206\\nFigure 13.16\\nHowever, remember that the underlying network (VLAN 100) does not see any of\\nthe MACVLAN magic, it only sees the container with its MAC and IP addresses,\\nmeaning themactainer1 container will be able to communicate with every other system\\nconnected to VLAN 100!\\nNote: If you can’t get this to work, it might be because your host NIC isn’t\\nin promiscuous mode. Also, remember that public cloud platforms normally\\nblock promiscuous mode.\\nAt this point, you’ve got a MACVLAN network and used it to connect a new container\\nto an existing VLAN. If you have the complete setup, with the existing VLAN, you can\\ntest that the container is reachable form other system on the VLAN.\\nHowever, it doesn’t stop there. The Docker MACVLAN driver supports VLAN trunking.\\nThis means you can create multiple MACVLAN networks that connect to different\\nVLANs. Figure 13.17 shows a single Docker host running two MACVLAN networks\\nconnecting containers to two different VLANs.',\n",
              " '13: Docker Networking 207\\nFigure 13.17\\nTroubleshooting connectivity problems\\nA quick note on troubleshooting connectivity issues before moving on to service\\ndiscovery.\\nDaemon logs and container logs can be useful when troubleshooting connectivity issues.\\nIf you’re running Windows containers, you can view them in the Windows Event\\nViewer or directly in\\x18\\\\AppData\\\\Local\\\\Docker. For Linux containers, it depends on\\nwhich init system you’re using. If you’re running a systemd, Docker will post logs to\\njournald and you can view them with thejournalctl -u docker.service command.\\nIf you’re using a different init system, you might want to check the following locations:\\n• Ubuntu systems runningupstart: /var/log/upstart/docker.log\\n• RHEL-based systems:/var/log/messages\\n• Debian: /var/log/daemon.log\\nYou can also tell Docker how verbose you want daemon logging to be. To do this, edit\\nthe daemon config file at/etc/docker/daemon.json and set\"debug\" to \"true\" and\\n\"log-level\" to one of the following:',\n",
              " '13: Docker Networking 208\\n• debug – the most verbose option\\n• info – the default value and second-most verbose option\\n• warn – third most verbose option\\n• error – fourth most verbose option\\n• fatal – least verbose option\\nThe following snippet from adaemon.json enables debugging and sets the level to\\ndebug. It will work on all Docker platforms.\\n{\\n<Snip>\\n\"debug\":true,\\n\"log-level\":\"debug\",\\n<Snip>\\n}\\nIf yourdaemon.json file doesn’t exist, create it. Also, be sure to restart Docker after\\nmaking any changes to the file.\\nThat was the daemon logs. What about container logs?\\nYou can normally view container logs with thedocker logs command. If you’re\\nrunning Swarm, you should use thedocker service logs command. However,\\nDocker supports a few different log drivers, and they don’t all work with native Docker\\ncommands. For some of them, you might have to view logs using the platform’s native\\ntools.\\njson-file and journald are probably the easiest to configure and they both work with\\nthe docker logs and docker service logs commands.\\nThe following snippet from adaemon.json shows a Docker host configured to use\\njournald.\\n{\\n\"log-driver\": \"journald\"\\n}\\nYou can also start a container or a service with the--log-driver and --log-opts flags\\nto override the settings indaemon.json.\\nContainer logs work on the premise that your application runs as PID 1 and sends logs\\nto STDOUT and errors toSTDERR. The logging driver then forwards everything to the\\nlocations configured via the logging driver.\\nThe following is an example of running thedocker logs command against a container\\ncalled vantage-db that is configured with thejson-file logging driver.',\n",
              " '13: Docker Networking 209\\n$ docker logs vantage-db\\n1:C 2 Feb 09:53:22.903 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\\n1:C 2 Feb 09:53:22.904 # Redis version=4.0.6, bits=64, commit=00000000, modified=0, pid=1\\n1:C 2 Feb 09:53:22.904 # Warning: no config file specified, using the default config.\\n1:M 2 Feb 09:53:22.906 * Running mode=standalone, port=6379.\\n1:M 2 Feb 09:53:22.906 # WARNING: The TCP backlog setting of 511 cannot be enforced...\\n1:M 2 Feb 09:53:22.906 # Server initialized\\n1:M 2 Feb 09:53:22.906 # WARNING overcommit_memory is set to 0!\\nThere’s a good chance you’ll find network connectivity errors in the daemon logs or\\ncontainer logs.\\nService discovery\\nAs well as core networking,libnetwork also providesservice discoverythat allows all\\ncontainers and Swarm services to locate each other by name. The only requirement is\\nthat the containers be on the same network.\\nUnder the hood, Docker implements a native DNS server and configures every con-\\ntainer to use it for name resolution.\\nFigure 13.18 shows a container calledc1 pinging another container calledc2 by name.\\nThe same principle applies to Swarm service replicas.\\nFigure 13.18\\nLet’s step through the process.\\n• Step 1:The c1 container issues aping c2 command. The container’s local DNS\\nresolver checks its cache to see if it has an IP address forc2. All Docker containers\\nhave a local DNS resolver.\\n• Step 2:The local resolver doesn’t have an IP address forc2, so it initiates a\\nrecursive query to the embedded Docker DNS server. All Docker containers are\\npre-configured to know how to send queries to the embedded DNS server.',\n",
              " '13: Docker Networking 210\\n• Step 3:The Docker DNS server maintains name-to-IP mappings for every\\ncontainer you create with the--name or --net-alias flags. This means it knows\\nthe IP address of thec2 container.\\n• Step 4:The DNS server returns the IP address of thec2 container to the local\\nresolver in thec1 container. Ifc1 and c2 are on different Docker networks it won’t\\nreturn the IP address — name resolution only works for containers on the same\\nnetwork.\\n• Step 5:The c1 container sends the ping request (ICMP echo request) to the IP\\naddress ofc2.\\nJust to confirm a few points.\\nDocker will automatically register the name and IP of every container you create\\nwith the--name or net-alias flag with the embedded Docker DNS service. It also\\nautomatically configures every container to use the embedded DNS service to convert\\nnames to IPs. And name resolution (service discovery) isnetwork scoped, meaning it only\\nworks for containers and services on the same network.\\nOne last point on service discovery and name resolution…\\nYou can use the--dns flag to start containers and services with a customized list of\\nDNS servers, and you can use the--dns-search flag to add custom search domains\\nfor queries against unqualified names (i.e., when the application doesn’t specify fully\\nqualified DNS names for services they consume). You’ll find both of these useful if your\\napplications query names outside of your Docker environment such as internet services.\\nBoth of these options work by adding entries to the container’s/etc/resolv.conf file.\\nRun the following command to start a new container with the infamous8.8.8.8\\nGoogle DNS server andnigelpoulton.com as a search domain for unqualified queries.\\n$ docker run -it --name custom-dns \\\\\\n--dns=8.8.8.8 \\\\\\n--dns-search=nigelpoulton.com \\\\\\nalpine sh\\nYour shell prompt will change to indicate you’re connected to the container.\\nInspect its/etc/resolv.conf file.',\n",
              " '13: Docker Networking 211\\n# cat /etc/resolv.conf\\nGenerated by Docker Engine.\\nThis file can be edited; Docker Engine will not make further changes once it\\nhas been modified.\\nnameserver 8.8.8.8\\nsearch nigelpoulton.com\\nThe file’s contents might be slightly different if you connect the container to a custom\\nnetwork, but the options work the same.\\nType exit to return to your local terminal.\\nIngress load balancing\\nThis section only applies to Docker Swarm.\\nSwarm supports two ways of publishing services to external clients:\\n• Ingress mode (default)\\n• Host mode\\nExternal clients can accessingress modeservices via any swarm node — even nodes not\\nhosting a service replica. However, they can only accesshost modeservices via nodes\\nrunning replicas. Figure 13.19 shows both modes.',\n",
              " '13: Docker Networking 212\\nFigure 13.19\\nIngress mode is the default, meaning any time you create a service with-p or --publish,\\nDocker will publish it iningress mode. If you want to publish a service inhost mode, you’ll\\nneed to use the--publish flag with themode=host option. The following example\\npublishes a service in host mode and will only work on a swarm.\\n$ docker service create -d --name svc1 \\\\\\n--publish published=5005,target=80,mode=host \\\\\\nnginx\\nA few notes about the command.docker service create lets you publish services\\nusing eitherlong form syntaxor short form syntax.\\nThe short form looks like-p 5005:80 and you’ve seen it a few times already. However,\\nyou cannot publish a service inhost modeusing the short form.\\nLong form looks like this:--publish published=5005,target=80,mode=host. It’s a\\ncomma-separated list with no whitespace after the commands, and the options work as\\nfollows:\\n• published=5005 makes the service available to external clients via port5005\\n• target=80 makes sure requests hitting thepublished port get mapped back to port\\n80 on service replicas',\n",
              " '13: Docker Networking 213\\n• mode=host makes sure requests will only reach the service if they arrive on nodes\\nrunning a service replica\\nYou’ll almost always use ingress mode.\\nBehind the scenes, ingress mode uses a layer 4 routing mesh that Docker calls the\\nservice meshor theswarm-mode service mesh. Figure 13.20 shows the basic traffic\\nflow when an external request hits the cluster for a service exposed in ingress mode.\\nFigure 13.20\\nLet’s quickly walk through the diagram.\\nThe command at the top deploys a new Swarm service calledsvc1 with one replica,\\nattaches it to theovernet network and publishes it on port5005 on theingress network.\\nDocker automatically creates the ingress network when you create the swarm, and\\nit attaches every node to it. The act of publishing the service on port5005 makes it\\naccessible via port5005 on every swarm node because every node is connected to the\\ningress network. Docker also creates a swarm-wide rule to route all traffic hitting nodes\\non port5005 to port80 in the svc1 replicas via the ingress network.\\nNow let’s track that external request.\\n1. The external client sends a request toNode 1on port5005\\n2. Node 1 receives the request and knows to forward traffic arriving on port5005 to\\nthe ingress network\\n3. The ingress network forwards the request toNode 2which is running a replica\\n4. Node 2receives the request and passes it to the replica on port80',\n",
              " '13: Docker Networking 214\\nIf the service has multiple replicas, swarm is clever enough to balance requests across\\nthem all.\\nClean-up\\nIf you’ve been following along, you’ll have a lot of containers, networks, and services\\nthat you probably want to clean up.\\nRun the following command to delete the services you created.\\n$ docker service rm svc1\\nNow, delete the standalone containers you created.\\n$ docker rm c1 c2 client web mactainer1 -f\\nFinally, delete the networks you created.\\n$ docker network rm localnet macvlan100\\nDocker Networking – The Commands\\nDocker networking has its owndocker network sub-command, and the main com-\\nmands include:\\n• docker network ls lists all the Docker networks available to the host.\\n• docker network create is how you create a new Docker network. You have\\nto give the network a name and you can use the-d flag to specify which driver\\ncreates it.\\n• docker network inspect provides detailed configuration information about\\nDocker networks.\\n• docker network prune deletes all unused networks on a Docker host.\\n• docker network rm Deletes specific networks on a Docker host or swarm.\\nYou also ran some native Linux commands.\\n• brctl show prints a list of all kernel bridges on the Docker host and shows if any\\ncontainers are connected.\\n• ip link show prints bridge configuration data. You ran anip link show\\ndocker0 to see the configuration of thedocker0 bridge on your Docker host.',\n",
              " '13: Docker Networking 215\\nChapter Summary\\nThe Container Network Model (CNM) is the design document for Docker networks\\nand defines the three major constructs —sandboxes, endpoints, andnetworks.\\nLibnetwork is the reference implementation of the CMN and is an open-source project\\nmaintained by the Moby project. Docker uses it to implement its core networking,\\nincluding control plane services such as service discovery.\\nDrivers extend the capabilities of libnetwork by implementing specific network topolo-\\ngies, such as bridge and overlay networks. Docker ships with built-in drivers, but you\\ncan also use third-party drivers.\\nSingle-host bridge networksare the most basic type of Docker network but are only\\nsuitable for local development and very small applications. They do not scale, and you\\nneed to map containers to host ports if you want to publish services outside of the\\nnetwork.\\nOverlay networksare all the rage and are excellent container-only multi-host networks.\\nWe’ll talk about them in-depth in the next chapter.\\nThe macvlan driver lets you create Docker networks that connect containers to existing\\nphysical networks and VLANs. They make containers first-class citizens on external\\nnetworks by giving them their own MAC and IP addresses. Unfortunately, you have to\\nrun your host NICs in promiscuous mode, meaning they won’t work in public clouds.',\n",
              " '14: Docker overlay networking\\nOverlay networks are at the center of most cloud-native microservices apps, and this\\nchapter will get you up to speed on how they work in Docker.\\nI’ve divided the chapter into the following sections:\\n• Docker overlay networking – The TLDR\\n• Docker overlay networking history\\n• Building and testing overlay networks\\n• Overlay networks explained\\nLet’s do some networking magic!\\nDocker overlay networking – The TLDR\\nReal-world containers need a reliable and secure way to communicate without caring\\nwhich host they’re running on or which networks those hosts are connected to. This\\nis where overlay networks come into play — they create flat, secure, layer 2 networks\\nthat span multiple hosts. Containers on different hosts can connect to the same overlay\\nnetwork and communicate directly.\\nDocker offers native overlay networking that is simple to configure and secure by\\ndefault.\\nBehind the scenes, Docker builds overlay networking on top oflibnetwork and the native\\noverlay driver. Libnetwork is the canonical implementation of the Container Network\\nModel (CNM), and theoverlay driver implements all of the machinery to build overlay\\nnetworks.\\nDocker overlay networking history\\nIn March 2015, Docker, Inc. acquired a container networking startup calledSocket Plane\\nwith two goals in mind:\\n1. Bring overlay networking to Docker',\n",
              " '14: Docker overlay networking 217\\n2. Make container networking simple for developers\\nThey accomplished both goals, and overlay networking continues to be at the heart of\\ncontainer networking in 2024 and the foreseeable future.\\nHowever, there’s a lot of complexity hiding behind the simple Docker commands.\\nKnowing the commands is probably enough if you’re a casual Docker user. However,\\nif you plan to use Docker in production, especially if you plan to use Swarm and Docker\\nnetworking, then the things we’ll cover will be vital.\\nBuilding and testing Docker overlay networks\\nYou’ll need at least two Docker nodes configured in a swarm to follow along. The\\nexamples in the book show the two nodes on different networks connected by a router,\\nbut yours can be on the same network. You can follow along with two Multipass VMs\\non the same laptop or computer, but any Docker configuration will work as long as the\\nnodes can communicate. I don’t recommend using Docker Desktop as you only get a\\nsingle node and won’t get the full experience.\\nFigure 14.1 shows the initial lab configuration. Remember, your nodes can be on\\nthe same network, this will just mean yourunderlay networkis simpler. We’ll explain\\nunderlay networks later.\\nFigure 14.1',\n",
              " '14: Docker overlay networking 218\\nBuild a Swarm\\nIf you’re following along, you’ll need a swarm because overlay networks leverage the\\nswarm’s key-value store and other security features.\\nThis section builds a two-node swarm with two Docker nodes callednode1 and node2.\\nIf you already have a swarm, you can skip this section.\\nYou’ll need to substitute the IP addresses and names with the values from your environ-\\nment. You’ll also need to ensure the following network ports are open between the two\\nnodes:\\n• 2377/tcp for management plane comms\\n• 7946/tcp and 7946/udp for control plane comms (SWIM-based gossip)\\n• 4789/udp for the VXLAN data plane\\nRun the following command onnode1.\\n$ docker swarm init\\nSwarm initialized: current node (1ex3...o3px) is now a manager.\\nThe command output includes adocker swarm join command. Copy this command\\nand run itnode2.\\n$ docker swarm join \\\\\\n--token SWMTKN-1-0hz2ec...2vye \\\\\\n172.31.1.5:2377\\nThis node joined a swarm as a worker.\\nYou now have a two-node Swarm withnode1 as a manager andnode2 as a worker.\\nCreate a new overlay network\\nLet’s create a new encrypted overlay network calleduber-net.\\nRun the following command from your manager node (node1).\\n$ docker network create -d overlay -o encrypted uber-net\\nvdu1yly429jvt04hgdm0mjqc6',\n",
              " '14: Docker overlay networking 219\\nThat’s it. You’ve created a brand-new encrypted overlay network. The network spans\\nboth nodes in the swarm and Docker uses TLS to encrypt it (AES in GCM mode). It also\\nrotates the encryption keys every 12 hours.\\nIf you don’t specify the-o encrypted flag, Docker will still encrypt the control plane\\n(management traffic) but won’t encrypt the data plane (application traffic). This can\\nbe important, as encrypting the data plane can decrease network performance by\\napproximately 10%.\\nList the networks onnode1.\\n$ docker network ls\\nNETWORK ID NAME DRIVER SCOPE\\n65585dda7500 bridge bridge local\\n7e368a1105c7 docker_gwbridge bridge local\\na38083cdab1c host host local\\n4dsqo7jc36ip ingress overlay swarm\\nd97e92a23945 none null local\\nvdu1yly429jv uber-net overlay swarm <<---- New overlay network\\nThe new network is at the bottom of the list calleduber-net and is scoped to the entire\\nswarm (SCOPE = swarm). This means it spans every node in the swarm. However, if you\\nlist networks onnode2 you won’t see theuber-net network. This is because Docker\\nonly extends overlay networks to worker nodes when they need them. In our example,\\nDocker will extend theuber-net network tonode2 when it runs a container that needs\\nit. This lazy approach to network deployment improves scalability by reducing the\\namount of network gossip on the swarm.\\nAttach a container to the overlay network\\nNow that you have an overlay network let’s connect a container to it.\\nBy default, you can only attach containers that are part ofswarm servicesto overlay\\nnetworks. If you want to addstandalone containers, you need to create the overlay with\\nthe --attachable flag.\\nThe example will create a swarm service calledtest with two replicas on theuber-net\\nnetwork. One replica will be deployed tonode1 and the other tonode2, causing Docker\\nto extend the overlay network tonode2.\\nRun the following commands fromnode1.',\n",
              " '14: Docker overlay networking 220\\n$ docker service create --name test \\\\\\n--network uber-net \\\\\\n--replicas 2 \\\\\\nubuntu sleep infinity\\nCheck the status of the service.\\n$ docker service ps test\\nID NAME IMAGE NODE DESIRED STATE CURRENT STATE\\nsm1...1nw test.1 ubuntu:latest node1 Running Running\\ntro...kgk test.2 ubuntu:latest node2 Running Running\\nThe NODE column shows one replica running on each node.\\nSwitch over tonode2 and run adocker network ls to verify it can now see theuber-\\nnet network.\\nCongratulations. You’ve created a new overlay network spanning two nodes on separate\\nunderlay networks and attached two containers to it. You’ll appreciate the simplicity\\nof what you’ve done when we reach the theory section and learn about the outrageous\\ncomplexity going on behind the scenes!\\nTest the overlay network\\nFigure 14.2 shows the current setup with two containers running on different Docker\\nhosts but connected to the same overlay.\\nFigure 14.2',\n",
              " '14: Docker overlay networking 221\\nThe following steps will walk you through obtaining the container names and IP\\naddresses and then seeing if they can ping each other.\\nSwitch back tonode1 and run adocker network inspect to see the overlay network’s\\nsubnet information and any IP addresses it’s assigned to service replicas.\\n$ docker network inspect uber-net\\n[\\n{\\n\"Name\": \"uber-net\",\\n\"Id\": \"vdu1yly429jvt04hgdm0mjqc6\",\\n\"Scope\": \"swarm\",\\n\"Driver\": \"overlay\",\\n\"EnableIPv6\": false,\\n\"IPAM\": {\\n\"Driver\": \"default\",\\n\"Options\": null,\\n\"Config\": [\\n{\\n\"Subnet\": \"10.0.0.0/24\", <<---- Subnet info\\n\"Gateway\": \"10.0.0.1\" <<---- Subnet info\\n}\\n\"Containers\": {\\n\"Name\": \"test.1.tro80xqwm7k1bsyn3mt1fjkgk\", <<---- Replica ID\\n\"IPv4Address\": \"10.0.0.3/24\", <<---- Container IP\\n<Snip>\\n},\\n<Snip>\\nI’ve snipped the output and highlighted the subnet info and the IPs of connected con-\\ntainers. One thing to note is that Docker only shows you the IP addresses of containers\\nrunning on the local node. For example, the output in the book only shows the IP of the\\nfirst replica calledtest.1.tro...kgk. If you run the same command onnode2, you’ll\\nsee the name and IP of the other replica.\\nRun the following commands on both nodes to get the local container names, IDs, and\\nIP addresses of both replicas and make a note of them.\\nThe ID at the end of the second command (d7766923a5a7) is the container ID as\\nreturned by thedocker ps command. You’ll need to substitute the value from your\\nenvironment.',\n",
              " '14: Docker overlay networking 222\\n$ docker ps\\nCONTAINER ID IMAGE COMMAND CREATED STATUS NAME\\nd7766923a5a7 ubuntu:latest \"sleep infinity\" 2 hrs ago Up 2 hrs test.1.tro...kgk\\n$ docker inspect \\\\\\n--format=\\'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\\' d7766923a5a7\\n10.0.0.3\\nI have the following in my environment :\\n• replica 1:ID=d7766923a5a7, Name=test.1.tr0...kgk, IP=10.0.0.3\\n• replica 2:ID=b6c897d1186d, Name=test.2.sm1...1nw, IP=10.0.0.4\\nFigure 14.3 shows the configuration so far. Subnet and IP addresses may be different in\\nyour lab.\\nFigure 14.3\\nAs you can see, a layer 2 overlay network spans both nodes, and each container is\\nconnected to it with its own IP. This means the container onnode1 can ping the\\ncontainer onnode2 even though both nodes are on different underlay networks.\\nLet’s test it. You’ll need the names and IPs of your containers.\\nLog on to either of the containers and install theping utility.',\n",
              " '14: Docker overlay networking 223\\n$ docker exec -it d7766923a5a7 bash\\n# apt update && apt-get install iputils-ping -y\\n<Snip>\\nReading package lists... Done\\nBuilding dependency tree\\nReading state information... Done\\n<Snip>\\nSetting up iputils-ping (3:20190709-3) ...\\nProcessing triggers for libc-bin (2.31-0ubuntu9) ...\\nNow ping the remote container by IP and then by replica ID.\\n# ping 10.0.0.4\\nPING 10.0.0.4 (10.0.0.4) 56(84) bytes of data.\\n64 bytes from 10.0.0.4: icmp_seq=1 ttl=64 time=1.06 ms\\n64 bytes from 10.0.0.4: icmp_seq=2 ttl=64 time=1.07 ms\\n64 bytes from 10.0.0.4: icmp_seq=3 ttl=64 time=1.03 ms\\n64 bytes from 10.0.0.4: icmp_seq=4 ttl=64 time=1.26 ms\\n^C\\n# ping test.2.sm180xqwm7k1bsyn3mt1fj1nw\\nPING test.2.sm180xqwm7k1bsyn3mt1fj1nw (10.0.0.4) 56(84) bytes of data.\\n64 bytes from test.2.sm1...1nw.uber-net (10.0.0.4): icmp_seq=1 ttl=64 time=2.83 ms\\n64 bytes from test.2.sm1...1nw.uber-net (10.0.0.4): icmp_seq=2 ttl=64 time=8.39 ms\\n64 bytes from test.2.sm1...1nw.uber-net (10.0.0.4): icmp_seq=3 ttl=64 time=5.88 ms\\n^C\\nCongratulations. The containers can ping each other via the overlay network, and all the\\ntraffic is encrypted.\\nYou can also trace the route of the ping command. This will report a single hop, proving\\nthat the containers are communicating directly via the overlay network — blissfully\\nunaware of any underlay networks being traversed.\\nYou’ll need to installtraceroute in the container for this to work.\\n# apt install traceroute\\n<Snip>\\n# traceroute 10.0.0.4\\ntraceroute to 10.0.0.4 (10.0.0.4), 30 hops max, 60 byte packets\\n1 test-svc.2.sm180xqwm7k1bsyn3mt1fj1nw.uber-net (10.0.0.4) 1.110ms 1.034ms 1.073ms\\nSo far, you’ve created an overlay network and a swarm service that connected two\\ncontainers to it. Swarm scheduled the containers to two different nodes and you proved\\nthey could ping each other via the overlay network.\\nNow that you’ve seen how easy it is to build and use secure overlay networks, let’s find\\nout how Docker builds them behind the scenes.',\n",
              " '14: Docker overlay networking 224\\nOverlay networks explained\\nFirst and foremost, Docker usesVXLAN tunnelsto create virtual layer 2 overlay net-\\nworks. So, let’s do a quick VXLAN primer.\\nVXLAN primer\\nAt the highest level, Docker uses VXLANs to create layer 2 networks on top of existing\\nlayer 3 infrastructure. That’s a lot of jargon that means you can create simple networks\\non top of complex networks. The hands-on example in the previous sections created\\na new 10.0.0.0/24 layer 2 network that abstracted a more complex network topology\\nbelow. See Figure 14.4 and remember that your underlay network configuration was\\nprobably different.\\nFigure 14.4\\nFortunately, VXLAN is anencapsulation technology and, therefore, transparent to exist-\\ning routers and network infrastructure. This means the routers and other infrastructure\\nin the underlay network see the VXLAN/overlay traffic as regular IP/UDP packets and\\nhandle it without requiring changes.\\nTo create the overlay, Docker creates aVXLAN tunnelthrough the underlay networks,\\nand this tunnel is what allows the overlay traffic to flow freely without having to\\ninteract with the complexity of the underlay networks.',\n",
              " '14: Docker overlay networking 225\\nTerminology: We use the termsunderlay networksor underlay infrastructureto\\nrefer to the networks the overlay tunnels through.\\nEach end of the VXLAN tunnel is terminated by aVXLAN Tunnel Endpoint (VTEP), and\\nit’s this VTEP that encapsulates and de-encapsulates the traffic entering and exiting the\\ntunnel. See Figure 14.5.\\nFigure 14.5\\nThe image shows the layer 3 infrastructure as a cloud for two reasons:\\n• It can be a lot more complex than the two networks and a single router from the\\nprevious diagrams\\n• The VXLAN tunnel abstracts the complexity and makes it opaque\\nIn reality, the VXLAN tunnel traverses the underlay network. However, I don’t show\\nthis in the diagram to keep the diagram simple.\\nTraffic flow example\\nThe hands-on examples from earlier had two hosts connected via an IP network. You\\ndeployed an overlay network across both hosts, connected two containers to it, and did\\na ping test. Let’s explain some of the things that happened behind the scenes.\\nDocker created a newsandbox (network namespace) on each host with a new switch\\ncalled Br0. It also created a VTEP with one end connected to theBr0 virtual switch and',\n",
              " '14: Docker overlay networking 226\\nthe other end connected to the host’s network stack. The end in the host’s network stack\\ngot an IP address on the underlay network that the host is connected to and was bound\\nto UDP port4789. Finally, the two VTEPs on each host created a VXLAN tunnel as the\\nbackbone for the overlay network.\\nFigure 14.6 shows the configuration. Remember, the VXLAN tunnel goes through the\\nnetworks at the bottom of the diagram; I’ve just drawn it higher up for readability.\\nFigure 14.6\\nAt this point, you’ve created the VXLAN overlay, and you’re ready to connect contain-\\ners.\\nDocker now creates a virtual Ethernet adapter (veth) in each container and connects it\\nto the localBr0 virtual switch. The final topology looks like Figure 14.7, and although\\nit’s complex, you should now see how the containers communicate over the VXLAN\\noverlay despite their hosts being on two separate networks — the overlay is a virtual\\nnetwork tunneled through the underlay networks.',\n",
              " '14: Docker overlay networking 227\\nFigure 14.7\\nNow that you know how Docker creates overlay networks, let’s see how the two\\ncontainers communicate.\\nWarning! This section is very technical, and you don’t need to understand it\\nall for day-to-day operations.\\nFor this example, we’ll call the container on node1“C1” and the container on node2\\n“C2”. We’ll also assumeC1 wants to pingC2 like we did in the practical example earlier.\\nFigure 14.8 shows the full configuration with container names and IPs added.',\n",
              " '14: Docker overlay networking 228\\nFigure 14.8\\nC1 initiates a ping request to10.0.0.4 — the IP address ofC2.\\nC1 doesn’t have an entry for10.0.0.4 in its local MAC address table (ARP cache), so\\nit floods the packet on all interfaces, including the veth interface connected to theBr0\\nbridge. TheBr0 bridge knows it can forward traffic for10.0.0.4 to the connected\\nVTEP interface and sends a proxy ARP reply to the container. This results in the veth\\nlearning how to forward the packet by updating its own MAC table to send all future\\npackets for10.0.0.4 directly to the local VTEP. TheBr0 switch knew about theC2\\ncontainer because Docker propagates details of all new containers to every swarm node\\nvia the network’s built-in gossip protocol.\\nNext, the veth in theC1 container sends the ping to the VTEP interface which encapsu-\\nlates it for transmission through the VXLAN tunnel. The encapsulation adds a VXLAN\\nheader containing a VXLAN network ID (VNID) that maps traffic from VLANs to\\nVXLANs and vice versa — each VLAN gets mapped to its own VNID so that packets\\ncan be de-encapsulated on the receiving end and forwarded to the correct VLAN. This\\nmaintains network isolation.\\nThe encapsulation also wraps the frame in a UDP packet and adds the IP of the remote\\nVTEP on node2 in thedestination IP field. It also adds the UDP/4789 socket information.\\nThis encapsulation allows the packets to be routed across the underlay networks\\nwithout the underlays knowing anything about VXLAN.\\nWhen the packet arrives at node2, the host’s kernel sees it’s addressed to UDP port\\n4789 and knows it has a VTEP bound to this socket. This means it sends the packet to\\nthe VTEP, which reads the VNID, de-encapsulates it, and sends it to its own localBr0\\nswitch on the VLAN corresponding to the VNID. From there, it delivers it to theC2',\n",
              " '14: Docker overlay networking 229\\ncontainer.\\nAnd that, my friends, is how Docker uses VXLAN to build and operate overlay networks\\n— a whole load of mind-blowing complexity beautifully hidden behind a single Docker\\ncommand.\\nI’m hoping that’s enough to get you started and help you when talking to your network-\\ning team about the networking aspects of your Docker infrastructure. On the topic of\\ntalking to your networking team… don’t approach them thinking that you now know\\neverything about VXLAN. If you do, you’ll probably embarrass yourself. I’m speaking\\nfrom experience ;-)\\nOne final thing. Docker also supports layer 3 routingwithin an overlay network. For\\nexample, you can create a single overlay network with two subnets, and Docker will\\nhandle the routing. The following command will create a new overlay calledprod-net\\nwith two subnets. Docker will automatically create two virtual switches calledBr0 and\\nBr1 inside thesandbox and handle all the routing.\\n$ docker network create --subnet=10.1.1.0/24 --subnet=11.1.1.0/24 -d overlay prod-net\\nClean up\\nIf you followed along, you’ll have created an overlay network calleduber-net and\\ndeployed a service calledtest. Youmay also have created a swarm.\\nRun the following command to delete thetest service.\\n$ docker service rm test\\nDelete theuber-net network with the following command. You may have to wait a few\\nseconds while Docker deletes the service using it.\\n$ docker network rm uber-net\\nIf you no longer need the swarm, you can run adocker swarm leave -f command on\\nboth nodes. You should run it onnode2 first.\\nDocker overlay networking – The commands\\n• docker network create tells Docker to create a new network. You use the-d\\noverlay flag to use the overlay driver to create an overlay network. You can also\\npass the-o encrypted flag to tell Docker to encrypt network traffic. However,\\nperformance may drop in the region of 10%.',\n",
              " '14: Docker overlay networking 230\\n• docker network ls lists all the container networks visible to a Docker host.\\nDocker hosts running inswarm modeonly see overlay networks if they run\\ncontainers attached to the network. This keeps network-related management\\ntraffic to a minimum.\\n• docker network inspect shows detailed information about a particular container\\nnetwork. You can find out the scope, driver, IPv4 and IPv6 info, subnet configura-\\ntion, IP addresses of connected containers, VXLAN network ID, encryption state,\\nand more.\\n• docker network rm deletes a network.\\nChapter Summary\\nIn this chapter, you created a new Docker overlay network and learned about the\\ntechnologies Docker uses to build them.',\n",
              " '15: Volumes and persistent data\\nStateful applications that create and manage data are a big part of modern cloud-native\\napps. This chapter explains how Docker volumes help stateful applications manage their\\ndata.\\nI’ve split the chapter into the following parts:\\n• Volumes and persistent data – The TLDR\\n• Containers without volumes\\n• Containers with volumes\\n• The commands\\nVolumes and persistent data – The TLDR\\nThere are two main types of data — persistent and non-persistent.\\nPersistent datais the stuff you care about and need tokeep. It includes things like cus-\\ntomer records, financial data, research results, audit data, and even some types of logs.\\nNon-persistent datais the stuff you don’t care about and don’t need to keep. We call\\napplications that create and manage persistent datastateful apps, and applications that\\ndon’t create or manage persistent datastateless apps.\\nBoth are important, and Docker has solutions for both.\\nFor stateless apps, Docker creates every container with an area of non-persistent local\\nstorage that’s tied to the container lifecycle. This storage is suitable for scratch data\\nand temporary files, but you’ll lose it when you delete the container or the container\\nterminates.\\nDocker hasvolumes for stateful apps that create and manage important data. Volumes are\\nseparate objects that you mount into containers, and they have their own lifecycles. This\\nmeans you don’t lose the volumes or the data on them when you delete containers. You\\ncan even mount volumes into different containers.\\nThat’s the TLDR. Let’s take a closer look.',\n",
              " '15: Volumes and persistent data 232\\nContainers without volumes\\nIn the early days of Docker, containers were only good for stateless applications that\\ndidn’t generate important data. However, despite beingstateless, many of these apps still\\nneeded a place to write temporary scratch data. So, as shown in Figure 15.1, Docker\\ncreates containers by stacking read-only image layers and placing a thin layer of local\\nstorage on top. The same technology allows multiple containers to share the same read-\\nonly image layers.\\nFigure 15.1 Ephemeral container storage\\nThis thin layer of local storage is integral to the read-write nature of containers. For\\nexample, if an application needs to update existing files or add new files, it makes\\nthe changes in the local storage layer, and Docker merges them into the view of the\\ncontainer. However, the local storage is coupled to the container’s lifecycle, meaning it\\ngets created when you create the container, and deleted when you delete it. This means\\nit’s not a good place for data that you need to keep (persist).\\nDocker keeps the local storage layer on the Docker host’s filesystem, and you’ll hear it\\ncalled various names such asthe thin writeable layer, ephemeral storage, read-write storage,\\nand graphdriver storage. It’s usually located in the following locations on your Docker\\nhosts:\\n• Linux containers:/var/lib/docker/<storage-driver>/...\\n• Windows containers:C:\\\\ProgramData\\\\Docker\\\\windowsfilter\\\\...\\nEven though the local storage layer allows you to update live containers, you should\\nnever do this. Instead, you should treat containers asimmutable objectsand never change\\nthem once deployed. For example, if you need to fix or change the configuration of a live\\ncontainer, you should create and test a new container with the changes and then replace\\nthe live container with the new one.\\nTo be clear, applications like databases can change the data they manage. But users\\nand configuration tools should never change the container’sconfiguration, such as its\\nnetwork or application configuration. You should always make changes like these in a\\nnew container and then replace the old container with the new one.',\n",
              " '15: Volumes and persistent data 233\\nIf your containers don’t create persistent data, this thin writable layer of local storage\\nwill be fine, and you’ll be good to go. However, if your containers create persistent data,\\nyou need to read the next section.\\nContainers with volumes\\nThere are three main reasons you should usevolumes to handle persistent data in\\ncontainers:\\n• Volumes are independent objects that are not tied to the lifecycle of a container\\n• You can map volumes to specialized external storage systems\\n• Multiple containers on different Docker hosts can use volumes to access and share\\nthe same data\\nAt a high level, you create a volume, then create a container, and finally mount the\\nvolume into the container. When you mount it into the volume, you mount it into a\\ndirectory in the container’s filesystem, and anything you write to that directory gets\\nstored in the volume. If you delete the container, the volume and data will still exist.\\nYou’ll even be able to mount the surviving volume into another container.\\nFigure 15.2 shows a Docker volume outside the container as a separate object. The\\nvolume is mounted into the container’s filesystem at/data, and anything you write to\\nthat directory will be stored on the volume and exist after you delete the container.\\nFigure 15.2 High-level view of volumes and containers\\nThe image also shows that you can map the volume to an external storage system\\nor a directory on the Docker host. External storage systems can be cloud services or\\ndedicated storage appliances, but either way, the volume’s lifecycle is decoupled from\\nthe container. All of the container’s other directories use the thin writable layer in the\\nlocal storage area on the Docker host.',\n",
              " '15: Volumes and persistent data 234\\nCreating and managing Docker volumes\\nVolumes are first-class objects in Docker. This means there’s adocker volume sub-\\ncommand, and avolume resource in the API.\\nRun the following command to create a new volume calledmyvol.\\n$ docker volume create myvol\\nmyvol\\nBy default, Docker creates new volumes with the built-inlocal driver. And, as the name\\nof the driver suggests, these volumes are only available to containers on the same node\\nas the volume. You can use the-d flag to specify a different driver, but you’ll need to\\ninstall the driver first.\\nThird-party drivers20 provide advanced features and access to external storage systems\\nsuch as cloud storage services and on-premises storage systems such as SAN and NAS.\\nFigure 15.3 shows a Docker host connected to an external storage system via a plugin\\n(driver).\\nFigure 15.3 Plugging external storage into Docker\\nOnce you’ve created the volume, you can see it with thedocker volume ls command\\nand inspect it with thedocker volume inspect command.\\n20https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins',\n",
              " '15: Volumes and persistent data 235\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal myvol\\n$ docker volume inspect myvol\\n[\\n{\\n\"CreatedAt\": \"2024-05-15T12:23:14Z\",\\n\"Driver\": \"local\",\\n\"Labels\": null,\\n\"Mountpoint\": \"/var/lib/docker/volumes/myvol/_data\",\\n\"Name\": \"myvol\",\\n\"Options\": null,\\n\"Scope\": \"local\"\\n}\\n]\\nNotice that theDriver and Scope fields are both set tolocal. This means you created\\nthe volume with thelocal driver, and it’s only available to containers on this Docker\\nhost. Mountpoint tells you where the volume exists in the Docker host’s filesystem.\\nBy default, Docker gives every volume created with thelocal driver its own directory\\non the host under/var/lib/docker/volumes. This means anyone with access to the\\nDocker host can bypass the container and access the volume’s contents directly in the\\nhost’s filesystem. You saw this in the Docker Compose chapter when we copied a file\\ndirectly into a volume’s directory on the Docker host, and the file immediately appeared\\nin the volume inside the container. However, that’s not a recommended practice.\\nNow that you’ve created a volume, you can create containers to use it. However, before\\nyou do that, there are two ways to delete Docker volumes:\\n• docker volume prune\\n• docker volume rm\\nThe docker volume prune command deletesall volumesnot mounted into a container\\nor service replica, so use it with caution!\\nThe docker volume rm command is more precise and lets you specify which volumes to\\ndelete.\\nNeither command will delete a volume in use by a container or service replica.\\nThe myvol volume you created isn’t used by a container, so you can delete it with either\\ncommand. Be careful if you use theprune command, as it may also delete other volumes.',\n",
              " '15: Volumes and persistent data 236\\n$ docker volume prune --all\\nWARNING! This will remove all local volumes not used by at least one container.\\nAre you sure you want to continue? [y/N] y\\nDeleted Volumes:\\nmyvol\\nTotal reclaimed space: 0B\\nCongratulations. You’ve created, inspected, and deleted a Docker volume, and none\\nof the actions involved a container. This proves that volumes are decoupled from\\ncontainers.\\nAt this point, you know all the commands to create, list, inspect, and delete Docker\\nvolumes. You’ve even seen how to deploy them via Compose files in the Compose and\\nSwarm stacks chapters. However, you can also deploy volumes via Dockerfiles by using\\nthe VOLUME instruction. The format isVOLUME <container-mount-point>. Interestingly,\\nyou cannot specify a host directory when you define volumes in a Dockerfile. This is\\nbecause host directories can differ depending on your host OS, and you could easily\\nbreak your builds if you specified a directory that doesn’t exist on a host. As a result,\\ndefining a volume in a Dockerfile requires you to specify host directories at deployment\\ntime.\\nUsing volumes with containers\\nLet’s see how to use volumes with containers.\\nRun the following command to create a new standalone container calledvoltainer that\\nmounts a volume calledbizvol.\\n$ docker run -it --name voltainer \\\\\\n--mount source=bizvol,target=/vol \\\\\\nalpine\\nThe command specified the--mount flag, telling Docker to mount a volume called\\nbizvol into the container at/vol. The command completed successfully even though\\nyou didn’t have a volume calledbizvol. This raises an important point:\\n• If you specify a volume that already exists, Docker will use it\\n• If you specify a volume that does not exist, Docker will create it\\nIn our case,bizvol didn’t exist, so Docker created it and mounted it into the container.\\nType Ctrl PQ to return to your local shell, and then list volumes to make sure Docker\\ncreated it.',\n",
              " '15: Volumes and persistent data 237\\n# <Ctrl-PQ>\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal bizvol\\nEven though volumes are decoupled from containers, Docker won’t let you delete this\\none because it’s in use by thevoltainer container.\\nTry to delete it.\\n$ docker volume rm bizvol\\nError response from daemon: remove bizvol: volume is in use - [b44d3f82...dd2029ca]\\nAs expected, you can’t delete it.\\nThe volume is brand new, so it doesn’t have any data. Let’sexec onto the container and\\nwrite some data to it.\\n$ docker exec -it voltainer sh\\n# echo \"I promise to write a book review on Amazon\" > /vol/file1\\nThe command writes some text to a file calledfile1 in the/vol directory where the\\nvolume is mounted.\\nRun a few commands to make sure the file and data exist.\\n# ls -l /vol\\ntotal 4\\n-rw-r--r-- 1 root root 50 May 23 08:49 file1\\n# cat /vol/file1\\nI promise to write a book review on Amazon\\nType exit to return to your Docker host’s shell, and then delete the container with the\\nfollowing commands.\\n# exit\\n$ docker rm voltainer -f\\nvoltainer\\nCheck that Docker deleted the container but kept the volume.',\n",
              " '15: Volumes and persistent data 238\\n$ docker ps -a\\nCONTAINER ID IMAGE COMMAND CREATED STATUS\\n$ docker volume ls\\nDRIVER VOLUME NAME\\nlocal bizvol\\nAs the volume still exists, you can view its contents in the Docker host’s local filesystem.\\nRemember, though, that it’s not recommended to access volumes directly via the host’s\\nfilesystem. We’re just showing you how to do it for demonstration and educational\\nreasons.\\nRun the following commands from your Docker host terminal. They’ll show the\\ncontents of the volume’s directory on your Docker host. The first command will show\\nthat the file still exists, and the second will show its contents.\\nThis step won’t work on Docker Desktop, as Docker Desktop runs inside a VM. You\\nmay have to prefix the commands withsudo.\\n$ ls -l /var/lib/docker/volumes/bizvol/_data/\\ntotal 4\\n-rw-r--r-- 1 root root 50 Jan 12 14:25 file1\\n$ cat /var/lib/docker/volumes/bizvol/_data/file1\\nI promise to write a book review on Amazon\\nGreat, the volume and the data still exist.\\nLet’s see if you can mount the existingbizvol volume into a new service or container.\\nRun the following command to create a new container callednewctr that mounts bizvol\\nat /vol.\\n$ docker run -it \\\\\\n--name newctr \\\\\\n--mount source=bizvol,target=/vol \\\\\\nalpine sh\\nYour terminal is now attached to thenewctr container. Check to see if the volume and\\ndata are mounted as expected.\\n# cat /vol/file1\\nI promise to write a book review on Amazon\\nCongratulations. You’ve created a volume, written some data to it, deleted the original\\ncontainer, mounted it in a second container, and verified the data still exists.\\nType exit to leave the container and jump over to Amazon to leave the book review you\\npromised to write.\\nIf you left a review, thanks! If you didn’t, I’ll cry, but I’ll live ;-)',\n",
              " '15: Volumes and persistent data 239\\nSharing storage across cluster nodes\\nIntegrating Docker withexternal storage systemslets you present shared storage to\\nmultiple nodes so that the containers running on different nodes can share the same\\nvolumes. These external systems can be cloud storage services or enterprise storage\\nsystems in your on-premises data centers. For example, you can present a single storage\\nLUN or NFS share (shared volume) to multiple Docker hosts so that any container on\\nthose hosts can access and share the volume. Figure 15.4 shows an external storage\\nsystem presenting a shared volume to two Docker nodes. The Docker nodes use the\\nappropriate driver for the external system to make the shared volume available to either\\nor both containers.\\nFigure 15.4\\nBuilding a shared setup like this requires a lot of things. You need access to specialized\\nstorage systems and knowledge of how they work. You also need a volume driver/plugin\\nthat works with the external storage system. Finally, you need to know how your\\napplications read and write to the shared storage to avoid potential data corruption.\\nPotential data corruption\\nData corruption is a major concern for any shared storage configuration.\\nAssume the following example based on Figure 15.4.\\nThe application running inctr1 writes an update to the shared volume. However,\\ninstead of directly committing the update, it keeps it in a local cache for faster recall. At\\nthis point, the application inctr1 thinks it’s written data to the volume. However, before\\nctr1 flushes its cache and commits the data to the volume, the app inctr2 updates the\\nsame data with a different value and commits it directly to the volume. At this point,\\nboth applicationsthink they’ve updated the data in the volume, but in reality, only',\n",
              " '15: Volumes and persistent data 240\\nthe application inctr2 has. A few seconds later,ctr1 flushes the data to the volume\\nand overwrites the changes made by the application inctr2. However, neither of the\\napplications is aware of the changes the other has made.\\nThis is why you need to design applications that share data to coordinate updates to\\nshared volumes.\\nClean up\\nIf you’ve been following along, you’ll have a container and a volume.\\nRun the following command to delete the container.\\n$ docker rm\\nNow, run this command to delete the volume.\\n$ docker volume rm bizvol\\nVolumes and persistent data – The Commands\\n• docker volume create creates new volumes. By default, it creates them with the\\nlocal driver, but you can use the-d flag to specify a different driver.\\n• docker volume ls lists all volumes on your Docker host.\\n• docker volume inspect shows you detailed volume information. You can use this\\ncommand to see where a volume exists in the Docker host’s filesystem.\\n• docker volume prune deletes all volumes not in use by a container or service\\nreplica. Use with caution!\\n• docker volume rm deletes specific volumes that are not in use.\\nChapter Summary\\nThere are two main types of data:persistent and non-persistent.\\nPersistent data is data you need to keep, and non-persistent data is data you don’t need\\nto keep.\\nBy default, all containers get a layer of writable non-persistent storage that lives and dies\\nwith the container. We sometimes call thislocal storage, and it’s ideal for non-persistent',\n",
              " '15: Volumes and persistent data 241\\ndata. However, if your apps create data you need to keep, you should store the data in a\\nDocker volume.\\nDocker volumes are first-class objects in the Docker API, and you manage them\\nindependently of containers using their owndocker volume sub-command. This means\\ndeleting containers doesn’t delete the data in their volumes.\\nA few third-party plugins exist that provide Docker with access to specialized external\\nstorage systems.\\nVolumes are the recommended way to work with persistent data in Docker environ-\\nments.',\n",
              " '16: Docker security\\nIf security is hard, we’re less likely to implement it. Fortunately, most of the security\\nin Docker is easy and pre-configured with sensible defaults. This means you get a\\nmoderately secureexperience with zero effort. The defaults are not perfect, but they’re a\\ngood starting point.\\nDocker supports all major Linux security technologies and adds some of its own. As\\nsuch, I’ve divided the chapter so we cover the Linux security technologies first and\\nfinish the chapter covering the Docker technologies:\\n• Docker security – The TLDR\\n• Linux security technologies\\n– Kernel namespaces\\n– Control Groups\\n– Capabilities\\n– Mandatory Access Control\\n– seccomp\\n• Docker security technologies\\n– Swarm security\\n– Docker Scout and vulnerability scanning\\n– Docker Content Trust\\n– Docker secrets\\nThe chapter focuses heavily on Linux, but the sections relating to Docker security\\ntechnologies apply to Linux and Windows containers.\\nDocker security – The TLDR\\nGood security is about layers and defence in depth, and more layers is always better.\\nFortunately, Docker offers a lot of security layers, including the ones shown in Figure\\n16.1.',\n",
              " '16: Docker security 243\\nFigure 16.1\\nAs you can see, Docker leverages the common Linux security and workload isolation\\ntechnologies, includingnamespaces, control groups, capabilities, mandatory access control\\n(MAC), and seccomp. It ships with sensible defaults for each one, but you can customize\\nthem to your specific requirements.\\nDocker also has its own security technologies, includingDocker Scoutand Docker\\nContent Trust.\\nDocker Scout offers class-leadingvulnerability scanningthat scans your images, provides\\ndetailed reports on known vulnerabilities, and recommends solutions. Docker Content\\nTrust (DCT) lets you cryptographically sign and verify images.\\nIf you use Docker Swarm, you’ll also get all of the following that Docker automatically\\nconfigures: cryptographic node IDs, mutual authentication (TLS), automatic CA\\nconfiguration and certificate rotation, secure cluster join tokens, an encrypted cluster\\nstore, encrypted networks, and more.\\nOther security-related technologies also exist, but the important thing to know is that\\nDocker works with the major Linux security technologies and adds a few of its own.\\nSometimes, the Linux security technologies can be complex and challenging to work\\nwith, but the native Docker ones are always easy.\\nKernel Namespaces\\nKernel namespaces,usually shortened tonamespaces, are the main technology for building\\ncontainers.\\nLet’s quickly compare namespaces and containers with hypervisors and virtual ma-\\nchines (VM).',\n",
              " '16: Docker security 244\\nNamespaces virtualize operating system constructssuch as process trees and filesystems,\\nwhereas hypervisors virtualize physical resourcessuch as CPUs and disks. In the VM model,\\nhypervisors create virtual machines by grouping virtual CPUs, virtual disks, and virtual\\nnetwork cards so that every VM looks, smells, and feels like a physical machine. In the\\ncontainer model,namespaces create virtual operating systems (containers) by grouping\\nvirtual process trees, virtual filesystems, and virtual network interfaces so that every\\ncontainer looks, smells, and feels exactly like a regular OS.\\nAt a very high level, namespaces provide lightweight isolation but do not provide a\\nstrong security boundary. Compared with VMs, containers are more efficient, but\\nvirtual machines are more secure.\\nDon’t worry, though. Platforms like Docker implement additional security technologies,\\nsuch as cgroups, capabilities, and seccomp, to improve container security.\\nNamespaces are a tried and tested technology that’s existed in the Linux kernel for\\na very long time. However, they were complex and hard to work with until Docker\\ncame along and hid all the complexity behind the simpledocker run command and a\\ndeveloper-friendly API.\\nAt the time of writing, every Docker container gets its own instance of the following\\nnamespaces:\\n• Process ID (pid)\\n• Network (net)\\n• Filesystem/mount (mnt)\\n• Inter-process Communication (ipc)\\n• User (user)\\n• UTS (uts)\\nFigure 16.2 shows a single Docker host running two containers. The host OS has its\\nown collection of namespaces we call theroot namespaces, and each container has its own\\ncollection of equivalent isolated namespaces. Applications in containers think they’re\\nrunning on their own host and are unaware of theroot namespacesor namespaces in\\nother containers.',\n",
              " '16: Docker security 245\\nFigure 16.2\\nLet’s briefly look at how Docker uses each namespace:\\n• Process ID namespace:Docker uses thepid namespace to give each container\\nits own isolated process tree. This means every container gets its own PID 1 and\\ncannot see or access processes running in other containers. Nor can any container\\nsee or access processes running on the host.\\n• Network namespace:Docker uses thenet namespace to provide each container\\nwith an isolated network stack. This stack includes interfaces, IP addresses,\\nport ranges, and routing tables. For example, every container gets its owneth0\\ninterface with its own unique IP and range of ports.\\n• Mount namespace:Every container has its ownmnt namespace with its own\\nunique isolated root (/) filesystem. This means every container can have its own\\n/etc, /var, /dev, and other important filesystem constructs. Processes inside a\\ncontainer cannot access the host’s filesystem or filesystems in other containers.\\n• Inter-process Communication namespace:Docker uses theipc namespace\\nfor shared memory access within a container. It also isolates the container from\\nshared memory on the host and other containers.\\n• User namespace:Docker gives each container its own users that are only valid\\ninside the container. It also lets you map those users to different users on the\\nDocker host. For example, you can map a container’sroot user to a non-root user\\non the host.\\n• UTS namespace:Docker uses theuts namespace to provide each container with\\nits own hostname.\\nRemember, a container is a collection of namespaces that Docker organizes to look like\\na regular OS. These namespaces provide isolation, but they are not a strong enough',\n",
              " '16: Docker security 246\\nsecurity boundary on their own. This is why Docker augments container security with\\nthe technologies we’re about to discuss.\\nControl Groups\\nIf namespaces are aboutisolation, control groups (cgroups) are aboutlimits.\\nThink of containers as similar to rooms in a hotel. While each room might appear to be\\nisolated, they actually share a lot of things such as water supply, electricity supply, air\\nconditioning, swimming pool, gym, elevators, breakfast bar, and more. Containers are\\nsimilar — even though they’re isolated, they share a lot of common resources such as the\\nhost’s CPU, RAM, network I/O, and disk I/O.\\nDocker usescgroups to limit a container’s use of these shared resources and prevent any\\ncontainer from consuming them all and causing a denial of service (DoS) attack.\\nCapabilities\\nThe Linux root user is extremely powerful, and you shouldn’t use it to run apps and\\ncontainers.\\nHowever, it’s not as simple as running them as non-root users, as most non-root users\\nare so powerless that they are practically useless. What’s needed is a way to run apps and\\ncontainers with the exact set of permissions they need — nothing more, nothing less.\\nThis is wherecapabilities come to the rescue.\\nUnder the hood, the Linuxroot user is a combination of a long list ofcapabilities. Some\\nof these capabilities include:\\n• CAP_CHOWN: lets you change file ownership\\n• CAP_NET_BIND_SERVICE: lets you bind a socket to low-numbered network\\nports\\n• CAP_SETUID: lets you elevate the privilege level of a process\\n• CAP_SYS_BOOT: lets you reboot the system.\\nThe list goes on and is long.\\nDocker leverages capabilities so that you can run containers asroot but strip out\\nall the capabilities you don’t need. For example, suppose the only capability your\\ncontainer needs is the ability to bind to low-numbered network ports. In that case,',\n",
              " '16: Docker security 247\\nDocker can start the container as root,drop all root capabilities,and then add back the\\nCAP_NET_BIND_SERVICE capability.\\nThis is a good example of implementing the principle ofleast privilegeas you end up\\nwith a container that only has the capabilities it needs. Docker also sets restrictions to\\nprevent containers from re-adding dropped capabilities.\\nDocker ships with sensible out-of-the-box capabilities, but you should configure your\\nown for your production apps and containers. However, configuring your own requires\\nextensive effort and testing.\\nMandatory Access Control systems\\nDocker works with major Linux MAC technologies such as AppArmor and SELinux.\\nDepending on your Linux distribution, Docker applies default AppArmor or SELinux\\nprofiles to all new containers, and according to the Docker documentation, the default\\nprofiles aremoderately protective while providing wide application compatibility.\\nYou can tell Docker to start containers without these policies, and you can configure\\nyour own. However, as withcapabilities, configuring your own policies is very powerful\\nbut requiresa lotof effort and testing.\\nseccomp\\nDocker uses seccomp to limit which syscalls a container can make to the host’s kernel.\\nSyscalls are how applications ask the Linux kernel to perform tasks. At the time of writ-\\ning, Linux has over 300 syscalls and the default Docker profile disables approximately\\n40-50.\\nAs per the Docker security philosophy, all new containers get a default seccomp profile\\nconfigured withsensible defaultsdesigned to providemoderate security without impacting\\napplication compatibility.\\nAs always, you can customize your own seccomp profiles or tell Docker to start\\ncontainers without one. Unfortunately, the Linux syscall table is long, and configuring\\ncustom seccomp policies may be prohibitively complex for some users.\\nFinal thoughts on the Linux security technologies\\nDocker supports most of the important Linux security technologies and ships with\\nsensible defaults that add security without being too restrictive. Figure 16.3 shows how\\nDocker uses them to build adefence in depthsecurity posture with multiple layers.',\n",
              " '16: Docker security 248\\nFigure 16.3 - Linux security defense in depth\\nSome of these technologies require knowledge of the Linux kernel and can be complex\\nto customize. Fortunately, many platforms, including Docker, ship with defaults that are\\na good place to start.\\nDocker security technologies\\nLet’s switch our focus to some of the security technologies Docker offers.\\nSwarm security\\nDocker Swarm lets you cluster multiple Docker hosts and manage applications declar-\\natively. Every Swarm comprisesmanager nodesand worker nodesthat can be Linux or\\nWindows. Managers host the control plane and are responsible for configuring the\\ncluster and dispatching work tasks. Workers run application containers.\\nFortunately, swarm modeincludes many security features that Docker automatically\\nconfigures with sensible defaults. These include:\\n• Cryptographic node IDs\\n• TLS for mutual authentication',\n",
              " '16: Docker security 249\\n• Secure join tokens\\n• CA configuration with automatic certificate rotation\\n• Encrypted cluster store\\n• Encrypted networks\\nLet’s walk through building a secure swarm and configuring some of the security\\naspects.\\nIf you’re following along, you’ll need three Docker hosts that can ping each other by\\nname. The examples use three hosts calledmgr1, mgr2, andwrk1.\\nConfigure a secure Swarm\\nRun the following command from the node you want to be the first manager. We’ll run\\nthe example frommgr1.\\n$ docker swarm init\\nSwarm initialized: current node (7xam...662z) is now a manager.\\nThat’s it! You’ve configured a secure swarm with a cryptographic cluster ID, an en-\\ncrypted cluster store, a certificate authority with a 90-day certificate rotation policy, a\\nset of secure join tokens to use when adding new managers and workers, and configured\\nthe current manager with a client certificate for mutual TLS — all with a single com-\\nmand!\\nFigure 16.4 shows the current swarm configuration. Some of the details may be\\ndifferent in your lab.\\nFigure 16.4\\nLet’s joinmgr2 as an additional manager.\\nJoining new managers is a two-step process:',\n",
              " '16: Docker security 250\\n• Extract the secure join token\\n• Execute adocker swarm join command with the join token on the node you’re\\nadding\\nRun the following command frommgr1 to extract the manager join token.\\n$ docker swarm join-token manager\\nTo add a manager to this swarm, run the following command:\\ndocker swarm join --token \\\\\\nSWMTKN-1-1dmtwu...r17stb-2axi5...8p7glz \\\\\\n172.31.5.251:2377\\nThe output gives you the full command and join token to run onmgr2. The join token\\nand IP address will be different in your lab.\\nThe format of the joincommand is:\\n• docker swarm join --token <manager-join-token> <ip-of-existing-\\nmanager>:<swarm-port>\\nThe format of thetoken is:\\n• SWMTKN-1-<hash-of-cluster-certificate>-<manager-join-token>\\nCopy the command and run it onmgr2:\\n$ docker swarm join --token SWMTKN-1-1dmtwu...r17stb-2axi5...8p7glz 172.31.5.251:2377\\nThis node joined a swarm as a manager.\\nList the nodes in your swarm.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\n7xamk...ge662z mgr1 Ready Active Leader\\ni0ue4...zcjm7f * mgr2 Ready Active Reachable\\nYou now have a two-node swarm withmgr1 and mgr2 as managers. Both have access to\\nthe cluster store and are configured with client certificates for mutual TLS.\\nIn the real world, you’ll always run three or five managers for high availability.\\nFigure 16.5 shows the updated swarm with both managers.',\n",
              " '16: Docker security 251\\nFigure 16.5\\nAdding worker nodes is a similar two-step process — extract the join token and run the\\ncommand on the node.\\nRun the following command on either of the managers to expose the worker join\\ncommand and token.\\n$ docker swarm join-token worker\\nTo add a worker to this swarm, run the following command:\\ndocker swarm join --token \\\\\\nSWMTKN-1-1dmtw...17stb-ehp8g...w738q \\\\\\n172.31.5.251:2377\\nCopy the command and run it onwrk1:\\n$ docker swarm join --token SWMTKN-1-1dmtw...17stb-ehp8g...w738q 172.31.5.251:2377\\nThis node joined a swarm as a worker.\\nRun anotherdocker node ls from either of your managers.\\n$ docker node ls\\nID HOSTNAME STATUS AVAILABILITY MANAGER STATUS\\n7xamk...ge662z * mgr1 Ready Active Leader\\nailrd...ofzv1u wrk1 Ready Active\\ni0ue4...zcjm7f mgr2 Ready Active Reachable\\nYour swarm has two managers and a worker. The managers are configured for high\\navailability (HA) and the cluster store is replicated to both. The worker node is part of\\nthe swarm but cannot access the cluster store. Figure 16.6 shows the final configuration.',\n",
              " '16: Docker security 252\\nFigure 16.6\\nNow that you’ve built a secure Swarm, let’s examine some of the security aspects.\\nSwarm join tokens\\nThe only requirement for joining managers and workers is possession of the secure join\\ntoken. This means you should keep them safe and never post them on public repos or\\neven internal repos that are not restricted.\\nEvery swarm maintains two distinct join tokens:\\n• Manager token\\n• Worker token\\nEvery join token has four distinct fields separated by dashes (-):\\n• PREFIX - VERSION - SWARM ID - TOKEN\\nThe prefix is alwaysSWMTKN and allows you to pattern-match against it to prevent people\\nfrom accidentally posting it publicly. TheVERSION field indicates the version of the\\nswarm. TheSwarm ID field is a hash of the swarm’s certificate. TheTOKEN field is the\\nworker or manager token.\\nAs you can see in the following table, the manager and worker tokens for any given\\nswarm are identical except for the finalTOKEN field.\\nRole Prefix Version Swarm ID Token\\nManager SWMTKN 1 1dmtwusdc…r17stb 2axi53zjbs45lqxykaw8p7glz\\nWorker SWMTKN 1 1dmtwusdc…r17stb ehp8gltji64jbl45zl6hw738q',\n",
              " '16: Docker security 253\\nIf you suspect either of your join tokens are compromised, you can revoke them and\\nissue new ones with a single command. The following example revokes the existing\\nmanager token and issues a new one.\\n$ docker swarm join-token --rotate manager\\nSuccessfully rotated manager join token.\\nExisting managers are unaffected, but you can only add new ones with the new token.\\nAs expected, the last field is the only difference between the old and new tokens.\\nDocker keeps a copy of join tokens in the encrypted cluster store.\\nTLS and mutual authentication\\nDocker issues every manager and worker with a client certificate that they use for\\nmutual authentication. It identifies the node, the swarm it’s a member of, and whether\\nit’s a manager or worker.\\nYou can inspect a node’s client certificate on Linux with the following command.\\n$ sudo openssl x509 \\\\\\n-in /var/lib/docker/swarm/certificates/swarm-node.crt \\\\\\n-text\\nCertificate:\\nData:\\nVersion: 3 (0x2)\\nSerial Number:\\n7c:ec:1c:8f:f0:97:86:a9:1e:2f:4b:a9:0e:7f:ae:6b:7b:b7:e3:d3\\nSignature Algorithm: ecdsa-with-SHA256\\nIssuer: CN = swarm-ca\\nValidity\\nNot Before: May 23 08:23:00 2024 GMT\\nNot After : Aug 21 09:23:00 2024 GMT\\nSubject: O = tcz3w1t7yu0s4wacovn1rtgp4, OU = swarm-manager,\\nCN = 2gxz2h1f0rnmc3atm35qcd1zw\\nSubject Public Key Info:\\n<SNIP>\\nAs shown in Figure 16.7, theSubject field uses the standardO, OU, andCN fields to\\nspecify the Swarm ID, the node’s role, and the node ID:\\n• The Organization (O) field stores the Swarm ID\\n• The Organizational Unit (OU) field stores the node’s role in the swarm',\n",
              " '16: Docker security 254\\n• The Canonical Name (CN) field stores the node’s crypto ID.\\nYou can also see the certificate rotation period in theValidity section.\\nFigure 16.7\\nYou can match these values to the corresponding values from adocker info command.\\n$ docker info\\n<SNIP>\\nSwarm: active\\nNodeID: 2gxz2h1f0rnmc3atm35qcd1zw <<---- Relates to the CN field\\nIs Manager: true <<---- Relates to the OU field\\nClusterID: tcz3w1t7yu0s4wacovn1rtgp4 <<---- Relates to the O field\\n<SNIP>\\nCA Configuration:\\nExpiry Duration: 3 months <<---- Relates to the validity block\\nForce Rotate: 0\\nRoot Rotation In Progress: false\\n<SNIP>\\nSwarm CA configuration\\nYou can use thedocker swarm update command to configure the certificate rotation\\nperiod. The following example changes it to 30 days.\\n$ docker swarm update --cert-expiry 720h\\nSwarm allows nodes to renew certificates early so that all nodes don’t update at exactly\\nthe same time.',\n",
              " '16: Docker security 255\\nYou can configure a new swarm to use an external CA by passing the--external-ca\\nflag todocker swarm init command, and you can use thedocker swarm ca command\\nto manage other CA-related settings.\\n$ docker swarm ca --help\\nUsage: docker swarm ca [OPTIONS]\\nDisplay and rotate the root CA\\nOptions:\\n--ca-cert pem-file Path to the PEM-formatted root CA certificate to use\\nfor the new cluster\\n--ca-key pem-file Path to the PEM-formatted root CA key to use for the\\nnew cluster\\n--cert-expiry duration Validity period for node certificates (ns|us|ms|s|m|h)\\n(default 2160h0m0s)\\n-d, --detach Exit immediately instead of waiting for the root rotation\\nto converge\\n--external-ca external-ca Specifications of one or more certificate signing endpoints\\n-q, --quiet Suppress progress output\\n--rotate Rotate the swarm CA - if no certificate or key are\\nprovided, new ones will be generated\\nThe cluster store\\nThe cluster store is where Docker keeps the configuration and state of a swarm. It’s also\\ncritical to other Docker technologies, such as overlay networks and secrets. This is why\\noverlay networks and many other advanced security features only work in swarm mode.\\nThe cluster store is based on the popularetcd distributed database and is automatically\\nencrypted and replicated to all managers.\\nDocker handles day-to-day maintenance, but you should implement strong backup and\\nrecovery procedures for production clusters.\\nThat’s enough about swarm mode security for now. Let’s look at some Docker security\\ntechnologies that don’t require swarm mode.\\nDocker Scout and vulnerability scanning\\nEvery container runs multiple software packages that are susceptible to bugs and\\nvulnerabilities that malicious actors can exploit.\\nImage scanning analyzes your images and produces a detailed list of all the software\\npackages it uses. We call this list asoftware bill of materials (SBOM),and the image',\n",
              " '16: Docker security 256\\nscanning system compares the SBOM against databases of known vulnerabilities and\\nprovides a report of vulnerabilities in your software. Most vulnerability scanners will\\nrank the vulnerabilities and provide advice on fixes.\\nVulnerability scanning is now an integral part of most software supply chains.\\nDocker Scoutis Docker’s native scanning platform and works with Docker Hub,\\nDocker Desktop, the Docker CLI, and even has its own Docker Scout Dashboard.\\nHowever, it’s a subscription-based service.\\nOther scanning platforms are available, but most of these also require some form of\\nsubscription.\\nIf you’re using Docker Desktop, you can run the following command to see an example\\nof Docker Scout.\\n$ docker scout quickview nigelpoulton/tu-demo:latest\\n✓ Provenance obtained from attestation\\n✓ Pulled\\n✓ Image stored for indexing\\n✓ Indexed 66 packages\\nTarget │ nigelpoulton/tu-demo:latest │ 0C 4H 2M 0L\\ndigest │ b4210d0aa52f │\\nBase image │ python:3-alpine │ 0C 2H 1M 0L\\nUpdated base image │ python:3.11-alpine │ 0C 1H 1M 0L\\n│ │\\nThe output shows zero critical vulnerabilities (0C), four high (4H), two medium (2M),\\nand zero low (0L).\\nYou can also run adocker scout cves command to get more detailed information,\\nincluding remediation advice.\\n$ docker scout cves nigelpoulton/tu-demo:latest\\n✓ SBOM of image already cached, 66 packages indexed\\n\\uffff Detected 6 vulnerable packages with a total of 8 vulnerabilities\\n## Overview\\n│ Analyzed Image\\n────────────────────┼────────────────────────────────\\nTarget │ nigelpoulton/tu-demo:latest\\ndigest │ b4210d0aa52f\\nplatform │ linux/arm64\\nvulnerabilities │ 0C 4H 2M 0L\\nsize │ 26 MB\\npackages │ 66',\n",
              " '16: Docker security 257\\n## Packages and Vulnerabilities\\n0C 1H 1M 0L expat 2.5.0-r2\\npkg:apk/alpine/expat@2.5.0-r2?os_name=alpine&os_version=3.19\\n\\uffff HIGH CVE-2023-52425\\nhttps://scout.docker.com/v/CVE-2023-52425\\nAffected range : <2.6.0-r0\\nFixed version : 2.6.0-r0\\n\\uffff MEDIUM CVE-2023-52426\\nhttps://scout.docker.com/v/CVE-2023-52426\\nAffected range : <2.6.0-r0\\nFixed version : 2.6.0-r0\\n<Snip>\\nI’ve snipped the output, so it only shows some of the vulnerabilities. However, even\\nfrom the snipped output in the book, you can see:\\n• Scout has scanned 66 packages and detected several vulnerabilities\\n• We’re using version2.5.0-r2 of theexpat package which has one high (1H) and\\none medium (1M) vulnerability\\n• The high vulnerability is listed asCVE-2023-52425 and the medium asCVE-2023-\\n52426\\n• The report includes links to Scout reports containing more info on each vulnera-\\nbility\\n• Scout recommends updating toexpat version 2.6.0-r0 which contains fixes for\\nboth\\nFigure 16.8 shows what it looks like in in Docker Desktop, and you get similar integra-\\ntions and views in Docker Hub.',\n",
              " '16: Docker security 258\\nFigure 16.8 - Docker Scout integration with Docker Desktop\\nIf you subscribe to Docker Scout, you can use thescout.docker.com portal to configure\\npolicies and integrations with Docker Hub and other registries.\\nAs good as vulnerability scanning is, it only scans images and doesn’t detect security\\nproblems with networks, nodes, or orchestrators. Also, not all image scanners are equal.\\nFor example, the best ones perform deep binary-level scans, whereas others may just\\nlook at package names and do not inspect content closely.\\nIn summary, scanning tools are great for inspecting your images and detecting known\\nvulnerabilities. Beware though, with great knowledge comes great responsibility — once\\nyou’re aware of vulnerabilities, you’re responsible for mitigating or fixing them.\\nSigning and verifying images with Docker Content\\nTrust\\nDocker Content Trust (DCT) makes it simple for you to verify the integrity and pub-\\nlisher of images and is especially important when you’re pulling images over untrusted\\nnetworks such as the internet.',\n",
              " '16: Docker security 259\\nAt a high level, DCT lets you sign your images when you push them to registries like\\nDocker Hub. It also lets you verify the images you pull and run as containers.\\nFigure 16.9 shows the high-level process.\\nFigure 16.9 - Docker Content Trust image signing and verification\\nYou can also use DCT to providecontext, such as whether or not a developer has signed\\nan image for use in a particular environment such asprod or dev, or whether an image\\nhas been superseded by a newer version and is therefore stale.\\nThe following steps walk you through configuring Docker Content Trust, signing and\\npushing an image, and then pulling the signed image.\\nIf you plan on following along, you’ll need a cryptographic key pair. If you don’t already\\nhave one, you can run the followingdocker trust command to generate one. The\\ncommand generates a new key pair callednigel and loads it to the local trust store ready\\nfor use. It will prompt you to enter a passphrase; don’t forget it :-)\\n$ docker trust key generate nigel\\nGenerating key for nigel...\\nEnter passphrase for new nigel key with ID 1f78609:\\nRepeat passphrase for new nigel key with ID 1f78609:\\nSuccessfully generated and loaded private key.... key available: /Users/nigelpoulton/nigel.pub\\nIf you already have a key pair, you can import and load it withdocker trust key load\\nkey.pem --name nigel.\\nThe next step is associating your key pair with the image repository to which you’ll push\\nsigned images. This example associates thenigel.pub key with thenigelpoulton/ddd-\\ntrust repo on Docker Hub. Your key file and repo will be different, and the repository\\ndoesn’t have to exist before you run the command.',\n",
              " '16: Docker security 260\\n$ docker trust signer add --key nigel.pub nigel nigelpoulton/ddd-trust\\nAdding signer \"nigel\" to nigelpoulton/dct...\\nInitializing signed repository for nigelpoulton/dct...\\nEnter passphrase for root key with ID aee3314:\\nEnter passphrase for new repository key with ID 1a18dd1:\\nRepeat passphrase for new repository key with ID 1a18dd1:\\nSuccessfully initialized \"nigelpoulton/dct\"\\nSuccessfully added signer: nigel to nigelpoulton/dct\\nNow that you’ve loaded the key pair and associated it with a repository, the final step is\\nto sign an image and push it to the repo.\\nThe following command signs a local image callednigelpoulton/ddd-trust:signed\\nand pushes it to Docker Hub. Your image will have a different name and you’ll push it to\\na different repo.\\n$ docker trust sign nigelpoulton/ddd-trust:signed\\nSigning and pushing trust data for local image nigelpoulton/ddd-trust:signed may...\\nThe push refers to repository [docker.io/nigelpoulton/ddd-trust]\\n6495b414566f: Mounted from nigelpoulton/ddd-book\\n798676f7ef8b: Mounted from nigelpoulton/ddd-book\\nbca4290a9639: Mounted from nigelpoulton/ddd-book\\n28ad2149d870: Mounted from nigelpoulton/ddd-book\\n4f4fb700ef54: Mounted from nigelpoulton/ddd-book\\n5e1fc7f5df34: Mounted from nigelpoulton/ddd-book\\nsigned: digest: sha256:b65f9a1aa4e670bbafd0fbb91281ea95f9cdc5728aa546579e248dfbc0ea4bde\\nSigning and pushing trust metadata\\nEnter passphrase for nigel key with ID 92330ea:\\nSuccessfully signed docker.io/nigelpoulton/ddd-trust:signed\\nThe push operation creates the repo on Docker Hub and then signs and pushes the\\nimage. You can view the repo on Docker Hub, and you can run the following command\\nto inspect its signing data.\\n$ docker trust inspect nigelpoulton/ddd-trust:signed --pretty\\nSignatures for nigelpoulton/ddd-trust:signed\\nSIGNED TAG DIGEST SIGNERS\\nsigned 30e6d35703c578ee...4fcbbcbb0f281 nigel\\nList of signers and their keys for nigelpoulton/ddd-trust:signed\\nSIGNER KEYS\\nnigel 4d6f1bf55702\\nAdministrative keys for nigelpoulton/ddd-trust:signed\\nRepository Key: 5e72e54afafb8444f...6b2744b32010ad22\\nRoot Key: 40418fc47544ca630...69a2cb89028c22092',\n",
              " '16: Docker security 261\\nYou can export theDOCKER_CONTENT_TRUST variable with a value of1 to force a Docker\\nhost to sign and verify all images.\\n$ export DOCKER_CONTENT_TRUST=1\\nOnce enabled, you won’t be able to pull and work with unsigned images.\\nTest it by trying to pull an unsigned image.\\n$ docker pull nigelpoulton/ddd-book:web0.2\\nError: remote trust data does not exist for docker.io/nigelpoulton/ddd-book: notary.docker.io\\ndoes not have trust data for docker.io/nigelpoulton/ddd-book\\nYou can no longer pull images without trust data!\\nDelete the local copy of the image you just signed and pushed so that you can try pulling\\nit from Docker Hub. Your image name will be different.\\n$ docker rmi nigelpoulton/ddd-trust:signed\\nUntagged: nigelpoulton/ddd-trust:signed@sha256...\\n<Snip>\\nNow, try pulling the image.\\n$ docker pull nigelpoulton/ddd-trust:signed\\nPull (1 of 1): nigelpoulton/ddd-trust:signed@sha256:30e6...\\ndocker.io/nigelpoulton/ddd-trust@sha256:30e6... Pulling from nigelpoulton/ddd-trust\\n08409d417260: Pull complete\\nDigest: sha256:30e6d35703c578ee703230b9dc87ada2ba958c1928615ac8a674fcbbcbb0f281\\nStatus: Downloaded newer image for nigelpoulton/ddd-trust@sha256:30e6...\\nTagging nigelpoulton/ddd-trust@sha256:30e6d... as nigelpoulton/ddd-trust:signed\\ndocker.io/nigelpoulton/ddd-trust:signed\\nThe pull worked because the image has valid trust data.\\nIn summary, Docker Content Trust is an important technology that helps you verify the\\nintegrity of the images you pull and run. It’s simple to configure in its basic form, but\\nmore advanced features, such ascontext, can be more complex.\\nDocker Secrets\\nMost applications leverage sensitive data such as passwords, certificates, and SSH keys.\\nFortunately, Docker lets you wrap them insidesecrets to keep them secure.',\n",
              " '16: Docker security 262\\nNote: Secrets only work in swarm mode as they leverage the cluster store.\\nBehind the scenes, Docker encrypts secrets when they’reat restin the cluster store and\\nwhile they’rein flighton the network. It also usesin-memory filesystemsto mount secrets\\ninto containers and operates a least-privilege model, where secrets are only available\\nto services that have been explicitly granted access. There’s even adocker secret\\ncommand.\\nFigure 16.10 shows the high-level workflow of creating a secret and deploying it to\\nservice replicas:\\nFigure 16.10 - Secret workflow\\nLet’s go through the five steps in the diagram. I’ve used a key symbol to show the secret,\\nand it’s only available to the dark containers.\\n1. You create the secret\\n2. Docker stores it in the encrypted cluster store\\n3. You create a service (the dark containers) and grant it access to the secret\\n4. Docker encrypts the secret when sending it over the network to service replicas\\n5. Docker mounts the secret into service replicas as an unencrypted file in an in-\\nmemory filesystem\\nThe light-colored containers are part of a different service and cannot access the secret.\\nAs soon as replicas using the secret terminate, Docker destroys the in-memory filesys-\\ntem and flushes the secret from the node.\\nDocker mounts secrets in their unencrypted form so that applications can use them\\nwithout needing keys to decrypt them.',\n",
              " '16: Docker security 263\\nYou can create and manage secrets with thedocker secret command and attach them\\nto services by passing the--secret flag to thedocker service create command.\\nClean up\\nIf you’ve followed along, you’ve created a swarm, added a signer, created a new repo\\non Docker Hub, and exported an environment variable to sign and verify images\\nautomatically.\\nRun the following command to disable Docker Content Trust. You’ll need to run it on\\nevery node where you enabled Docker Content Trust.\\n$ unset DOCKER_CONTENT_TRUST\\nRemove the signer from the repository you created. Your signer and repository will have\\ndifferent names.\\n$ docker trust signer remove nigel nigelpoulton/ddd-trust\\nRemoving signer \"nigel\" from nigelpoulton/ddd-trust...\\nall signed tags are currently revoked, use docker trust sign to fix\\nYou may also want to delete the repositories you created on Docker Hub and delete the\\nlocal key files on your system (usually a .pub file in your home directory)\\nDelete the swarm by running the following command on all swarm nodes. You should\\nrun it on the swarm managers last.\\n$ docker swarm leave -f\\nChapter Summary\\nYou can configure Docker to be extremely secure. It supports all of the major Linux\\nsecurity technologies such as kernel namespaces, cgroups, capabilities, MAC, and\\nseccomp. It ships with sensible defaults for all of these, but you can customize and even\\ndisable them.\\nIn addition to the Linux security technologies, Docker includes an extensive set of\\nits own security technologies. Swarms are built on TLS and are secure out of the box.\\nDocker Scout performs binary-level image scans and provides detailed reports of\\nknown vulnerabilities and suggested fixes. Docker Content Trust lets you sign and\\nverify images, and Docker secrets allow you to share sensitive data with swarm services.',\n",
              " 'What next\\nThank you so much for reading my book. You’re on your way to mastering containers!\\nAbout the front cover\\nI love this book’s cover, and I’m grateful to the hundreds of people who voted for the\\ndesign.\\nThe YAML code on the left represents the book’s technical nature. The Docker whale\\nrepresents the main topic. The vertical symbols on the right are container-related icons\\ndone in the style ofdigital rainfrom the Matrix movies. There’s also a hidden message\\nwritten in Klingon.\\nGet involved with the community\\nThere’s a vibrant container community full of helpful people. Get involved with Docker\\ngroups and chats on the internet, and look up your local Docker or cloud-native meetup\\n(search for “Docker meetup near me”).\\nKubernetes\\nNow that you know a thing or two about Docker, a great next step is Kubernetes. It’s a\\nlot like Swarm but has a larger scope and a more active community.\\nIf you liked this book, you’ll love my books on Kubernetes.\\nConnect with me\\nI’d love to connect with you and talk about Kubernetes and other cool tech.\\nYou can reach me at all of the following:\\n• Twitter: twitter.com/nigelpoulton\\n• LinkedIn: linkedin.com/in/nigelpoulton\\n• Mastodon: @nigelpoulton@hachyderm.io\\n• Web: nigelpoulton.com',\n",
              " 'What next 265\\n• YouTube: youtube.com/nigelpoulton\\n• Delete me\\nFeedback and reviews\\nBooks live and die by Amazon reviews and stars.\\nI’ve spent more than a year of my life writing this book, and I work tirelessly keeping\\nit up-to-date. Soooo… please take a moment to leave a kind review on Amazon or\\nGoodreads.\\nAlso, ping me atddd@nigelpoulton.com if you want to suggest content or fixes for\\nfuture editions.',\n",
              " 'Terminology\\nThis glossary defines some of the most common Docker and container-related terms\\nused in the book.\\nIf you think I’ve missed anything important, ping me atddd@nigelpoulton.com.\\nTerm Definition (according to Nigel)\\nAPI Application Programming Interface. In the\\ncase of Docker, all resources are defined in\\nthe Docker API, which is RESTful and\\nexposed via theDocker Daemon.\\nBase image The first layer of all container images.\\nCreated by the DockerfileFROM instruction\\nand usually contains a minimal set of OS\\nconstructs required by an application.\\nBuild The process of building a new container\\nimage. Dockerbuilds images by stepping\\nthrough a set of instructions defined in a\\nDockerfile.\\nBuild Cloud A subscription service that performs fast and\\nefficient image builds in Docker’s cloud\\ninfrastructure. It allows you to share a\\ncommon build-cache among teams for very\\nfast builds.\\nBuildKit Docker’s build engine that implements\\nadvanced build features such as advanced\\ncaching, multi-stage builds, and\\nmulti-architecture builds.\\nBuildx Docker’s latest and greatest build client that\\nsupports all the latest features of BuildKit,\\nsuch as multi-stage builds and\\nmulti-architecture images. Buildx has been\\nDocker’s default build client since Docker\\nEngine v23.0 and Docker Desktop v4.19.\\nCapability Linux kernel technology used by Docker to\\ncreate user accounts with the precise set of\\nsystem access they need.',\n",
              " 'Terminology 267\\nTerm Definition (according to Nigel)\\nCloud native A loaded term that means different things to\\ndifferent people. Cloud native is a way of\\ndesigning, building, and working with\\nmodern applications and infrastructure. I\\nconsider an application to becloud nativeif it\\ncan self-heal, scale on-demand, perform\\nrolling updates, and versioned rollbacks.\\nCluster store Docker Swarm’s distributed database that\\nholds the state of the cluster and apps. Based\\non theetcd distributed database, it is\\nautomatically encrypted and automatically\\ndistributed across all swarm managers for\\nhigh availability.\\nCompose An open specification for defining, deploying,\\nand managing multi-container microservices\\napps. Docker implements the Compose spec\\nand provides thedocker compose command\\nto make it easy to work with Compose apps.\\nContainer A container is a collection of kernel\\nnamespaces organized to look, smell, and feel\\nlike a regular operating system. Each\\ncontainer runs a single application, and\\ncontainers are smaller, faster, and more\\nportable than virtual machines. We\\nsometimes call themDocker containersor OCI\\ncontainers\\nContainer Network Model Pluggable interface enabling different\\nnetwork topologies and architectures. Third\\nparties provide CNM plugins for overlay\\nnetworks and BGP networks, as well as\\nvarious implementations of each.',\n",
              " 'Terminology 268\\nTerm Definition (according to Nigel)\\nContainer runtime Software running on every Docker node\\nresponsible for pulling container images,\\nstarting containers, stopping containers, and\\nother low-level container operations. Docker\\nuses two runtimes that work together:\\ncontainerd is Docker’s high-level runtime\\nthat manages lifecycle events such as starting\\nand stopping containers, whereasrunc is\\nDocker’s low-level runtime that interfaces\\nwith kernel constructs such as namespaces\\nand cgroups.\\ncontainerd Industry-standard container runtime used by\\nDocker and most Kubernetes clusters.\\nDonated to the CNCF by Docker, Inc.\\nPronounced “container dee”.\\nContainerize The process of packaging an application and\\nall dependencies into a container image.\\nControl Groups (cgroups) Linux kernel feature that Docker uses to limit\\nthe amount of host CPU, RAM, disk, and\\nnetwork resources a container uses.\\nDesired state How your cluster and applications should be.\\nFor example, thedesired stateof an application\\nmicroservice might be five replicas of xyz\\ncontainer listening on port 8080/tcp. Vital to\\nreconciliation.\\nDocker Platform that makes it easy to work with\\ncontainerized apps. It allows you to build\\nimages, as well as run and manage standalone\\ncontainers and multi-container apps.\\nDocker Debug Docker CLI plugin that lets you easily debug\\nslim images and containers that don’t ship\\nwith any debugging tools.\\nDocker Desktop Desktop application for Linux, Mac, and\\nWindows that makes working with Docker\\neasy. It has a slick UI and many advanced\\nfeatures like image management, vulnerability\\nscanning, and Wasm support.',\n",
              " 'Terminology 269\\nTerm Definition (according to Nigel)\\nDocker Hub High-performance OCI-compliant image\\nregistry. Docker Hub has over 57PB of\\nstorage and handles an average of 30K\\nrequests per second.\\nDocker, Inc. US-based technology company making it easy\\nfor developers to build, ship, and run\\ncontainerized applications. The company\\nbehind the Docker platform.\\nDocker init A new Docker CLI plugin that creates\\nhigh-fidelity Dockerfiles and makes it easy to\\nscaffold Compose apps.\\nDocker Scout Docker’s native vulnerability scanning service.\\nScout is a subscription service that integrates\\nwith the Docker CLI, Docker Desktop,\\nDocker Hub, and other image registries.\\nDockerfile Plain text file with instructions telling Docker\\nhow to build an application into a container\\nimage.\\netcd The open-source distributed database used by\\nDocker Swarm.\\nImage Archive containing application code, all\\ndependencies, and the metadata required to\\nstart a single application as a container. We\\nsometimes call themOCI images, container\\nimages, orDocker images.\\nIngress network Hidden network on all Docker Swarm\\nclusters used to publish services to external\\nclients.\\nKernel namespace Feature of the Linux kernel used by Docker to\\nisolate containers from processes running on\\nthe host and in other containers.\\nLayer Image layers contain modifications to the\\nbase image or the layer below them. Docker\\nbuilds images by stacking layers, each\\ncontaining changes to the layer below it. A\\nsimple example is a base layer that has basic\\nOS constructs, followed by a layer with the\\napplication. The two combined layers create\\nthe image with the OS and app.',\n",
              " 'Terminology 270\\nTerm Definition (according to Nigel)\\nlibcontainer A Go library that uses namespaces, cgroups,\\nand capabilities to build containers. Docker\\nuses libcontainer via therunc low-level\\nruntime that is a CLI wrapper around\\nlibcontainer.\\nlibnetwork The Go library used by Docker to create and\\nmanage container networks.\\nMicroservices Design pattern for modern applications.\\nIndividual application features are developed\\nas their own small applications\\n(microservices/containers) and communicate\\nvia APIs. They work together to form a useful\\napplication.\\nMulti-architecture builds (sometimes called\\nmulti-platform builds)\\nAllows you to build images for multiple\\narchitectures and platforms with a single\\ndocker build command. For example, you\\ncan run a singledocker build command on\\nan AMD-based Windows system to build an\\nAMD imageand an ARM image.\\nMulti-state build Allows you to create very small images (slim\\nimages). You build your images in stages and\\nonly carry forward the necessary artifacts for\\neach next stage. Each build stage is\\nrepresented by its ownFROM instruction in\\nyour Dockerfile, and later build stages use the\\nCOPY --from instruction to use artifacts from\\nprevious stages and leave everything else\\nbehind.\\nObserved state Also known ascurrent stateor actual state. The\\nmost up-to-date view of the cluster and\\nrunning applications. Docker Swarm is\\nalways working to makeobserved statematch\\ndesired state.\\nOpen Container Initiative (OCI) Lightweight governance body responsible for\\ncreating and maintaining standards for\\nlow-level container technologies such as\\nimages, runtimes, and registries. Docker\\ncreates OCI-compliant images, implements\\nan OCI-compliant runtime, and Docker Hub\\nis an OCI-compliant registry.',\n",
              " 'Terminology 271\\nTerm Definition (according to Nigel)\\nOrchestrator Software that deploys and manages apps.\\nDocker Swarm and Kubernetes are examples\\nof orchestrators that manage microservices\\napps, keep them healthy, scale them up and\\ndown, and more…\\nOverlay network A large flat layer-2 network that spans\\nmultiple swarm nodes. All containers on the\\nsame overlay network can communicate with\\neach other even if they’re on different Docker\\nhosts that are on different networks. The\\nbuilt-in overlay driver creates overlay\\nnetworks using advanced VXLAN\\ntechnologies. They are only supported by\\nDocker Swarm.\\nPush Upload an image to a registry.\\nPull Download an image from a registry.\\nReconciliation The process of watching the state of an\\napplication and ensuring observed state\\nmatches desired state. Docker Swarm runs\\nreconciliation loops, ensuring applications\\nrun how you want them to.\\nRegistry Central place for storing and retrieving\\nimages. We sometimes call themOCI registries,\\ncontainer registries,or Docker registries.\\nRepository An area of a registry where you store related\\ncontainer images. You can set access controls\\nper repository.\\nSeccomp Secure computing Linux kernel feature used\\nby Docker to restrict the syscalls available to a\\ncontainer.\\nSecret The way Docker Swarm lets you inject\\nsensitive data into a container at run-time.\\nService Capital “S” is a Docker Swarm feature that\\naugments containers with self-healing,\\nscaling, rollouts, and rollbacks.',\n",
              " 'Terminology 272\\nTerm Definition (according to Nigel)\\nSpin Framework that makes it easy to build,\\ndeploy, and run Wasm apps. Docker Desktop\\nships with thespin runtime. Created by\\nFermyon Technologies, Inc.\\nSwarm (also known as Docker Swarm Docker’s native orchestration platform. A\\nlightweight and easy alternative to\\nKubernetes.\\nVolume Where containers store important data they\\nneed to keep. You can create and delete\\nvolumes independently from containers.\\nWasm See WebAssembly.\\nWebAssembly Also known as Wasm. New virtual machine\\narchitecture that is smaller, faster, more\\nportable, and more secure than traditional\\ncontainers. Wasm apps run anywhere with a\\nWasm runtime.\\nYAML Yet Another Markup Language. You write\\nCompose files in YAML. It’s a superset of\\nJSON.',\n",
              " 'Index\\nplaceholder just to create index']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter  # Import CharacterTextSplitter\n",
        "\n",
        "# Initialize the CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "# Split the text into chunks\n",
        "chunks = text_splitter.create_documents(pages)\n",
        "\n",
        "# Display results\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(f\"Type of first chunk: {type(chunks[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Uu54OqL54Q",
        "outputId": "a50e9e8a-edd1-4309-ffeb-76843911735a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 279\n",
            "Type of first chunk: <class 'langchain_core.documents.base.Document'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks [7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy4aIvHWMfif",
        "outputId": "c27d4e85-2948-4903-d67b-bc669f72c2bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='0: About the book 2\\nChapter breakdown\\n• Chapter 1:Summarises the history and potential future of Docker and containers\\n• Chapter 2:Explains the most important container-related standards and projects\\n• Chapter 3:Shows you a few ways to get Docker\\n• Chapter 4:Walks you through a very simple hands-on container workflow\\n• Chapter 5:Explains the architecture of the Docker Engine\\n• Chapter 6:Dives deep into images and image management\\n• Chapter 7:Dives deep into containers and container management\\n• Chapter 8:Walks you through the process of containerizing an app\\n• Chapter 9:Shows you how to build, deploy, and manage multi-container apps\\nwith Compose\\n• Chapter 10:Walks you through building a secure swarm\\n• Chapter 11:Deploys and manages a multi-container app on a secure swarm\\n• Chapter 12:Walks you through building and containerizing a WebAssembly app\\n• Chapter 13:Dives into Docker networking\\n• Chapter 14:Builds and tests Docker overlay networks\\n• Chapter 15:Introduces you to persistent and non-persistent data in Docker\\n• Chapter 16:Covers all the major Linux and Docker security technologies\\nEditions and updates\\nDocker and the cloud-native ecosystem are evolving fast, and a 2-3-year-old book on\\nDocker isn’t valuable. As a result, I’m committed to updating the book every year.\\nIf that sounds excessive, welcome to the new normal.\\nThe book is available in hardback, paperback, and e-book on all good book publishing\\nplatforms.\\nWhen you purchase the Kindle edition, you’re entitled to all future updates. However,\\nKindle doesn’t always download the latest edition.\\nA potential solution is to go tohttp://amzn.to/2l53jdg and chooseQuick Solutions.\\nThen selectDigital Purchases, search for your Docker Deep Dive Kindle edition\\npurchase, and selectContent and Devices. Your purchase should appear in the list with\\na button that saysUpdate Available. Click that button. Delete your old version on your\\nKindle and download the new one.\\nIf this doesn’t work, your only option is to contact Kindle Support.')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key, model=\"models/embedding-004\")\n",
        ""
      ],
      "metadata": {
        "id": "md-e9r9-MgRr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key,  model=\"models/text-embedding-004\",  chunk_size=100,task_type=\"retrieval_document\")\n"
      ],
      "metadata": {
        "id": "zApJ7ZnZMp2w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(type(retriever))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anKqbP8BMtzv",
        "outputId": "0e9d0f71-4f03-4196-d2cc-5cf18d651431"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
      ],
      "metadata": {
        "id": "eUal9aO0Mv3r"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    # System Message Prompt Template\n",
        "    SystemMessage(content=\"\"\"You are a Helpful AI Bot.\n",
        "                  Given a context and question from user,\n",
        "                  you should answer based on the given context.\"\"\"),\n",
        "    # Human Message Prompt Template\n",
        "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "    Answer: \"\"\")\n",
        "])"
      ],
      "metadata": {
        "id": "Wia3mzRsMyOk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "SwlpfcPiM0bi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | chat_template\n",
        "    | chat_model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "1DJR3CmqM2Wn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"Please summarize Leave No Context Behind:\n",
        "                            Efficient Infinite Context Transformers with Infini-attention\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "vfjuggypM57p",
        "outputId": "73e7ba10-b751-4aa9-f0a8-bdedbc847fea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\" introduces a new transformer architecture designed to handle arbitrarily long sequences.  It achieves this through a novel attention mechanism called \"Infini-attention,\" which avoids the quadratic complexity typically associated with long sequences in standard transformers.  Instead of computing attention over the entire sequence, Infini-attention uses a more efficient approach that allows the model to effectively attend to all relevant context, regardless of length, making it suitable for tasks requiring very long-range dependencies.  The key is improved efficiency without sacrificing performance on long sequences.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = retriever.invoke(\"\"\"Please summarize Leave No Context Behind:\n",
        "                            Efficient Infinite Context Transformers with Infini-attention\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UccFbhAM7yK",
        "outputId": "606d2950-8263-4852-ef73-31abde5ab47e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"Please summarize Chapter 1:\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uBX4lVhKM_SW",
        "outputId": "35a4cdab-5e16-4cf5-ada4-f860ca020652"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Please provide the text of Chapter 1.  I need the chapter's content to summarize it.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"Who are you?\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wQp-aLIgNBK0",
        "outputId": "edab6a84-2a6e-40eb-86cb-0f495f5b5820"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am a large language model, trained by Google.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"What can you help me?\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "q-zzjMpYNrEP",
        "outputId": "6c2fe371-64eb-46b5-8a89-a3a5bc565caa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I can help you answer questions based on the provided context.  If no context is given, I can try my best to answer your question using my general knowledge.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"Is pine cone setup in this colab?\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8sUvAX0cPFr5",
        "outputId": "786095b1-4930-4251-e103-25745ff7d872"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided context is empty.  Therefore, I cannot answer whether a pine cone setup is in the colab.  More context is needed.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"What is Gemini Flash optimized for?\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XimkilKPPzLV",
        "outputId": "dcfd4a2d-a91d-4c9b-f066-c7566be15fa0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided context is empty.  Therefore, I cannot answer the question about what Gemini Flash is optimized for.  I need context about Gemini Flash to provide an answer.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"\"\"What is the capital of USA?\"\"\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "m9exFNlWQ7K2",
        "outputId": "491eeb23-26ab-4f3c-8872-5f14ea3c75e0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided context does not contain the answer to the question \"What is the capital of the USA?\".  Therefore, I cannot answer the question based on the given context.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXEOxWplRxP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}